{"pass": false, "messages": [{"role": "user", "content": "I am a doctoral student in computer vision and will graduate next year. My career plan is to become a teacher in a university, so after graduating with my doctorate, I plan to find a professor who is engaged in the same research field to conduct postdoctoral work. More personal information about me can be obtained from personal_info.md \u200b\u200bunder the workspace. To ensure that this professor is active in the academic community, please help me find the three people who have published the most articles at CVPR 2025 and who meet my requirements. Please put the names of these three researchers in the `top3_match_researchers.txt` under the workspace, each in a spearate line.\n"}, {"role": "assistant", "content": "I'll help you find the top 3 researchers who published the most articles at CVPR 2025 and match your requirements. Let me start by reading your personal information and then searching for CVPR 2025 publication data.", "tool_calls": [{"id": "toolu_01S6RQ52S2BtixZoSuCfvaqg", "type": "function", "function": {"name": "filesystem-read_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/personal_info.md\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"# About Me\\n\\n## Research Areas\\n**Main Research Area:** Visual Generative Models\\n\\n**Specific Fields:** Application and innovation of diffusion models, flow matching and related technologies in tasks such as image generation, video generation, and cross-modal generation.\\n\\n## Research Interests\\n\\nMy research focuses on advancing the frontier of visual content generation through deep generative models. I am particularly interested in:\\n\\n- **Diffusion Models & Flow Matching**: Developing efficient sampling strategies and improving the quality-diversity trade-off in generative processes\\n- **Video Generation**: Exploring temporal consistency and motion dynamics in video synthesis using state-of-the-art generative frameworks\\n- **Cross-modal Generation**: Bridging different modalities (text-to-image, image-to-video, audio-visual synthesis) through unified generative architectures\\n- **Controllable Generation**: Investigating methods for fine-grained control over generated content while maintaining high fidelity\\n- **Efficient Inference**: Optimizing generative models for real-world deployment with reduced computational costs\\n\\n## News\\n\\n\ud83d\udd0d **I am actively seeking a postdoctoral position at a university in Hong Kong.** If you are interested in collaboration or have opportunities available, please feel free to contact me.\\n\\n\ud83d\udce7 Contact: [tadaawc@adp.com]\\n\\n## Recent Publications\\n\\nPlease refer to my google scholar page for this part!\\n\\n## Education\\n\\n- Ph.D. in CSE, ABU, 2026 (expected)\\n- M.S. in CSE, ABU, 2021\\n- B.S. in EE, BCU, 2019\\n\\n## Selected Projects\\n\\n### Project 1: Diffusion Models for High-Quality Image Synthesis\\nDeveloped an advanced diffusion-based framework that significantly improves the fidelity and resolution of generated images. Introduced novel architectural enhancements and sampling techniques to achieve state-of-the-art results on benchmark datasets.\\n\\n### Project 2: Consistent Video Generation via Flow Matching\\nProposed a flow matching approach for temporal-consistent video generation, enabling the synthesis of realistic motion and persistent object appearances across frames. The method achieves robust performance in both unconditional and conditional video generation tasks.\\n\\n### Project 3: Cross-Modal Generative Framework\\nDesigned a unified generative model capable of handling multiple modalities, such as text-to-image and image-to-video generation. The framework supports smooth cross-modal translation and demonstrates strong results on several public datasets.\\n\\n---\\n\\n*Feel free to reach out for research collaboration, academic discussions, or potential opportunities!*\",\"annotations\":null}", "tool_call_id": "toolu_01S6RQ52S2BtixZoSuCfvaqg"}, {"role": "assistant", "content": "Now let me search for CVPR 2025 publication data and find the most prolific authors in your research area.", "tool_calls": [{"id": "toolu_01ESqzbzgsX5MYVgz3nzkT8S", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"CVPR 2025 accepted papers list\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: Kai Chen \u00b7 Yunhao Gou \u00b7 Runhui Huang \u00b7 Zhili Liu \u00b7 Daxin Tan \u00b7 Jing Xu \u00b7 Chunwei Wang \u00b7 Yi Zhu \u00b7 yihan zeng \u00b7 Kuo Yang \u00b7 Dingdong WANG \u00b7 Kun Xiang \u00b7 Haoyuan ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025\nSnippet: These CVPR 2025 papers are the Open Access versions, provided by the Computer Vision Foundation. Except for the watermark, they are identical to the accepted ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Paper List - Paper Copilot\nLink: https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2025-paper-list/\nSnippet: CVPR 2025 Accepted Paper List ; 1, 2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification.\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Papers\nLink: https://cvpr.thecvf.com/virtual/2025/papers.html\nSnippet: Select Year: (2025). 2026 \u00b7 2025 \u00b7 2024 \u00b7 2023 \u00b7 Home \u00b7 Schedule \u00b7 Workshops \u00b7 Tutorials \u00b7 Keynotes; Awards. Best Papers \u00b7 Highlights \u00b7 Award Candidates.\nSitelinks: No sitelinks\n\nTitle: SkalskiP/top-cvpr-2025-papers - GitHub\nLink: https://github.com/SkalskiP/top-cvpr-2025-papers\nSnippet: In 2025 alone, 13,008 papers were submitted, and 2,878 were accepted. I created this repository to help you search for cr\u00e8me de la cr\u00e8me of ...\nSitelinks: No sitelinks\n\nTitle: All Papers - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=all\nSnippet: These CVPR 2025 papers are the Open Access versions, provided by the Computer Vision Foundation. Except for the watermark, they are identical to the accepted ...\nSitelinks: No sitelinks\n\nTitle: Paper Digest: CVPR 2025 Papers & Highlights\nLink: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\nSnippet: Note: CVPR-2025 accepts more than 2,800 papers, this page only includes 500 of them selected by our daily paper digest algorithm. Interested ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Workshop List - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/Conferences/2025/workshop-list\nSnippet: CVPR 2025 Accepted Workshops \u00b7 3D Capture and Reconstruction \u00b7 3D Scene Understanding \u00b7 Accessibility \u00b7 Analysis of Foundation Models \u00b7 Autonomous Driving.\nSitelinks: No sitelinks\n\nTitle: CVPR.2025 - Oral | Cool Papers - Immersive Paper Discovery\nLink: https://papers.cool/venue/CVPR.2025?group=Oral\nSnippet: The list of accepted papers for CVPR.2025 - Oral, including titles, authors, and abstracts, with support for paper interpretation based on Kimi AI.\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers - MMLab@NTU\nLink: https://www.mmlab-ntu.com/conference/cvpr2025/index.html\nSnippet: Presentation. Schedule. The team has a total of 20 papers (including 2 orals and 3 highlights) accepted to CVPR 2025.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01ESqzbzgsX5MYVgz3nzkT8S"}, {"role": "assistant", "content": "Let me access the official CVPR 2025 accepted papers page to get the full list.", "tool_calls": [{"id": "toolu_01AFkd8sSagDxb72ibGbu3sT", "type": "function", "function": {"name": "playwright_with_chunk-browser_navigate", "arguments": "{\"url\": \"https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Navigate to https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\nawait page.goto('https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers');\\n```\\n\\n### Page state\\n- Page URL: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\n- Page Title: CVPR 2025 Accepted Papers\\n- Page Snapshot (Span 1 of 502):\\n```yaml\\n- generic [ref=e1]:\\n  - dialog \\\"Cookie Consent Banner\\\" [ref=e3]:\\n    - generic [ref=e4]:\\n      - generic [ref=e5]: This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising.\\n      - link \\\"Privacy Policy\\\" [active] [ref=e6] [cursor=pointer]:\\n        - /url: https://www.ieee.org/security-privacy.html\\n    - generic [ref=e7]:\\n      - button \\\"Accept\\\" [ref=e8] [cursor=pointer]\\n      - button \\\"Deny\\\" [ref=e9] [cursor=pointer]\\n      - button \\\"Manage Preferences\\\" [ref=e10] [cursor=pointer]\\n  - banner [ref=e12]:\\n    - link \\\"Skip to yearly menu bar\\\" [ref=e13] [cursor=pointer]:\\n      - /url: \\\"#child-menu\\\"\\n    - link \\\"Skip to main content\\\" [ref=e14] [cursor=pointer]:\\n      - /url: \\\"#main\\\"\\n    - generic \\\"Main Navigation\\\" [ref=e15]:\\n      - heading \\\"Main Navigation\\\" [level=2] [ref=e16]\\n      - generic [ref=e17]:\\n        - link \\\"Home\\\" [ref=e19] [cursor=pointer]:\\n          - /url: /\\n          - img \\\"conference_logo\\\" [ref=e20] [cursor=pointer]\\n        - generic [ref=e21]:\\n          - list [ref=e22]:\\n            - listitem [ref=e23]:\\n              - button \\\"CVPR\\\" [ref=e24] [cursor=pointer]\\n            - listitem [ref=e25]:\\n              - link \\\"Reset Password\\\" [ref=e26] [cursor=pointer]:\\n                - /url: /resetpassword\\n                - generic [ref=e27] [cursor=pointer]: Reset Password\\n            - listitem [ref=e28]:\\n              - link \\\"My Stuff\\\" [ref=e29] [cursor=pointer]:\\n                - /url: /MyStuff\\n                - generic [ref=e30] [cursor=pointer]: My Stuff\\n          - search \\\"Search\\\" [ref=e31]:\\n            - search [ref=e32]:\\n              - textbox \\\"Search\\\" [ref=e33]\\n              - button \\\"execute search\\\" [ref=e35] [cursor=pointer]:\\n                - generic [ref=e36] [cursor=pointer]: \uf002\\n          - link \\\"Login\\\" [ref=e37] [cursor=pointer]:\\n            - /url: \\\"/accounts/login?nextp=/virtual/2025/poster/34726 \\\"\\n            - generic [ref=e38] [cursor=pointer]: \uf2f6\\n            - text: Login\\n  - main [ref=e39]:\\n    - navigation [ref=e42]:\\n      - button \\\"Toggle navigation\\\" [ref=e44] [cursor=pointer]\\n    - generic [ref=e51]:\\n      - heading \\\"CVPR 2025 Accepted Papers\\\" [level=1] [ref=e52]\\n      - paragraph [ref=e53]:\\n        - generic [ref=e54]: This page is cached for 1 hour\\n        - text: . Changes to affiliation or name in your local\\n        - link \\\"profile\\\" [ref=e55] [cursor=pointer]:\\n          - /url: /EditProfile\\n        - text: may take up to 60 minutes to appear here.\\n      - table [ref=e57]:\\n        - rowgroup [ref=e58]:\\n          - row [ref=e59]:\\n            - cell [ref=e60]\\n            - cell [ref=e61]\\n            - cell [ref=e62]\\n          - row [ref=e63]:\\n            - cell [ref=e64]\\n            - cell [ref=e65]\\n            - cell [ref=e66]\\n          - 'row \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi ExHall D Poster #67\\\" [ref=e67]':\\n            - 'cell \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\\" [ref=e68]':\\n              - 'link \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation\\\" [ref=e69] [cursor=pointer]':\\n                - /url: https://pfnet-research.github.io/himor/\\n              - text: Poster Session 1\\n              - generic [ref=e72]: Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\n            - cell [ref=e73]\\n            - 'cell \\\"ExHall D Poster #67\\\" [ref=e74]'\\n          - 'row \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak ExHall D Poster #367\\\" [ref=e75]':\\n            - 'cell \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\\" [ref=e76]':\\n              - 'link \\\"GENIUS: A Generative Framework for Universal Multimodal Search\\\" [ref=e77] [cursor=pointer]':\\n                - /url: https://sung-yeon-kim.github.io/project_pages/GENIUS\\n              - text: Poster Session 4\\n              - generic [ref=e80]: Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\n            - cell [ref=e81]\\n            - 'cell \\\"ExHall D Poster #367\\\" [ref=e82]'\\n          - 'row \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu ExHall D Poster #400\\\" [ref=e83]':\\n            - cell \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\\" [ref=e84]:\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01AFkd8sSagDxb72ibGbu3sT"}, {"role": "assistant", "content": "Good! I can see the CVPR 2025 accepted papers page. Let me accept the cookie banner and then try to get the complete list of papers. The page appears to have 502 spans, which means there's a lot of data. Let me try to download or scrape the complete data.", "tool_calls": [{"id": "toolu_011rUvdt9W4AJqjzVimSPdmt", "type": "function", "function": {"name": "playwright_with_chunk-browser_click", "arguments": "{\"element\": \"Accept button in cookie consent banner\", \"ref\": \"e8\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Click Accept button in cookie consent banner\\nawait page.getByRole('button', { name: 'Accept' }).click();\\n```\\n\\n### New console messages\\n- [ERROR] Failed to load resource: the server responded with a status of 404 (Not Found) @ https://cvp...\\n\\n### Page state\\n- Page URL: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\n- Page Title: CVPR 2025 Accepted Papers\\n- Page Snapshot (Span 1 of 502):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - button \\\"Cookie Preferences\\\" [ref=e23449] [cursor=pointer]:\\n    - img \\\"Cookie Preferences\\\" [ref=e23450] [cursor=pointer]\\n  - banner [ref=e12]:\\n    - link \\\"Skip to yearly menu bar\\\" [ref=e13] [cursor=pointer]:\\n      - /url: \\\"#child-menu\\\"\\n    - link \\\"Skip to main content\\\" [ref=e14] [cursor=pointer]:\\n      - /url: \\\"#main\\\"\\n    - generic \\\"Main Navigation\\\" [ref=e15]:\\n      - heading \\\"Main Navigation\\\" [level=2] [ref=e16]\\n      - generic [ref=e17]:\\n        - link \\\"Home\\\" [ref=e19] [cursor=pointer]:\\n          - /url: /\\n          - img \\\"conference_logo\\\" [ref=e20] [cursor=pointer]\\n        - generic [ref=e21]:\\n          - list [ref=e22]:\\n            - listitem [ref=e23]:\\n              - button \\\"CVPR\\\" [ref=e24] [cursor=pointer]\\n            - listitem [ref=e25]:\\n              - link \\\"Reset Password\\\" [ref=e26] [cursor=pointer]:\\n                - /url: /resetpassword\\n                - generic [ref=e27] [cursor=pointer]: Reset Password\\n            - listitem [ref=e28]:\\n              - link \\\"My Stuff\\\" [ref=e29] [cursor=pointer]:\\n                - /url: /MyStuff\\n                - generic [ref=e30] [cursor=pointer]: My Stuff\\n          - search \\\"Search\\\" [ref=e31]:\\n            - search [ref=e32]:\\n              - textbox \\\"Search\\\" [ref=e33]\\n              - button \\\"execute search\\\" [ref=e35] [cursor=pointer]:\\n                - generic [ref=e36] [cursor=pointer]: \uf002\\n          - link \\\"Login\\\" [ref=e37] [cursor=pointer]:\\n            - /url: \\\"/accounts/login?nextp=/virtual/2025/poster/34726 \\\"\\n            - generic [ref=e38] [cursor=pointer]: \uf2f6\\n            - text: Login\\n  - main [ref=e39]:\\n    - navigation [ref=e42]:\\n      - button \\\"Toggle navigation\\\" [ref=e44] [cursor=pointer]\\n    - generic [ref=e51]:\\n      - heading \\\"CVPR 2025 Accepted Papers\\\" [level=1] [ref=e52]\\n      - paragraph [ref=e53]:\\n        - generic [ref=e54]: This page is cached for 1 hour\\n        - text: . Changes to affiliation or name in your local\\n        - link \\\"profile\\\" [ref=e55] [cursor=pointer]:\\n          - /url: /EditProfile\\n        - text: may take up to 60 minutes to appear here.\\n      - table [ref=e57]:\\n        - rowgroup [ref=e58]:\\n          - row [ref=e59]:\\n            - cell [ref=e60]\\n            - cell [ref=e61]\\n            - cell [ref=e62]\\n          - row [ref=e63]:\\n            - cell [ref=e64]\\n            - cell [ref=e65]\\n            - cell [ref=e66]\\n          - 'row \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi ExHall D Poster #67\\\" [ref=e67]':\\n            - 'cell \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\\" [ref=e68]':\\n              - 'link \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation\\\" [ref=e69] [cursor=pointer]':\\n                - /url: https://pfnet-research.github.io/himor/\\n              - text: Poster Session 1\\n              - generic [ref=e72]: Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\n            - cell [ref=e73]\\n            - 'cell \\\"ExHall D Poster #67\\\" [ref=e74]'\\n          - 'row \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak ExHall D Poster #367\\\" [ref=e75]':\\n            - 'cell \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\\" [ref=e76]':\\n              - 'link \\\"GENIUS: A Generative Framework for Universal Multimodal Search\\\" [ref=e77] [cursor=pointer]':\\n                - /url: https://sung-yeon-kim.github.io/project_pages/GENIUS\\n              - text: Poster Session 4\\n              - generic [ref=e80]: Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\n            - cell [ref=e81]\\n            - 'cell \\\"ExHall D Poster #367\\\" [ref=e82]'\\n          - 'row \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu ExHall D Poster #400\\\" [ref=e83]':\\n            - cell \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\\" [ref=e84]:\\n              - strong [ref=e85]: Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition\\n              - img \\\"Highlight\\\" [ref=e86]\\n              - text: Poster Session 2\\n              - generic [ref=e89]: Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\n            - cell [ref=e90]\\n            - 'cell \\\"ExHall D Poster #400\\\" [ref=e91]'\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_011rUvdt9W4AJqjzVimSPdmt"}, {"role": "assistant", "content": "The page has 502 spans which is too much to navigate manually. Let me try to use JavaScript to extract all the paper data at once.", "tool_calls": [{"id": "toolu_01N5AoeJ6kxWH3sHZPjevAKY", "type": "function", "function": {"name": "playwright_with_chunk-browser_evaluate", "arguments": "{\"function\": \"() => {\\n  const papers = [];\\n  const rows = document.querySelectorAll('table tr');\\n  \\n  for (let row of rows) {\\n    const cells = row.querySelectorAll('td');\\n    if (cells.length >= 1) {\\n      const authorCell = cells[0];\\n      const authorText = authorCell.textContent || '';\\n      \\n      // Extract authors (they appear after \\\"Poster Session X\\\" or similar)\\n      const match = authorText.match(/(?:Poster Session \\\\d+|Oral|Highlight[^\u00b7]*)(.*)/);\\n      if (match && match[1]) {\\n        const authors = match[1].split('\u00b7').map(a => a.trim()).filter(a => a.length > 0);\\n        if (authors.length > 0) {\\n          papers.push({\\n            text: authorText.substring(0, 200),\\n            authors: authors\\n          });\\n        }\\n      }\\n    }\\n  }\\n  \\n  return {\\n    totalPapers: papers.length,\\n    sample: papers.slice(0, 10)\\n  };\\n}\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\n{\\n  \\\"totalPapers\\\": 4,\\n  \\\"sample\\\": [\\n    {\\n      \\\"text\\\": \\\"\\\\n                    \\\\n                        OralXrays-9: Towards Hospital-Scale Panoramic X-ray Anomaly Detection via Personalized Multi-Object Query-Aware Mining\\\\n                    \\\\n              \\\",\\n      \\\"authors\\\": [\\n        \\\"Xrays-9: Towards Hospital-Scale Panoramic X-ray Anomaly Detection via Personalized Multi-Object Query-Aware Mining\\\"\\n      ]\\n    },\\n    {\\n      \\\"text\\\": \\\"\\\\n                    \\\\n                        Hierarchical Adaptive Filtering Network for Text Image Specular Highlight Removal\\\\n                    \\\\n                    \\\\n                    Poster Ses\\\",\\n      \\\"authors\\\": [\\n        \\\"Jingbo Hu\\\",\\n        \\\"Ling Zhang\\\",\\n        \\\"Gang Fu\\\",\\n        \\\"Chunxia Xiao\\\"\\n      ]\\n    },\\n    {\\n      \\\"text\\\": \\\"\\\\n                    \\\\n                        Learning to Highlight Audio by Watching Movies\\\\n                    \\\\n                    \\\\n                    Poster Session 5\\\\n                    \\\\n       \\\",\\n      \\\"authors\\\": [\\n        \\\"Ruohan Gao\\\",\\n        \\\"J. M. F. Tsang\\\",\\n        \\\"Jan Kurcius\\\",\\n        \\\"Cagdas Bilen\\\",\\n        \\\"Chenliang Xu\\\",\\n        \\\"Anurag Kumar\\\",\\n        \\\"Sanjeel Parekh\\\"\\n      ]\\n    },\\n    {\\n      \\\"text\\\": \\\"\\\\n                    \\\\n                        No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather\\\\n                    \\\\n                   \\\",\\n      \\\"authors\\\": [\\n        \\\"HwiJeong Lee\\\",\\n        \\\"Inha Kang\\\",\\n        \\\"Hyunjung Shim\\\"\\n      ]\\n    }\\n  ]\\n}\\n\\n### Ran Playwright code\\n```js\\nawait page.evaluate('() => {\\\\n  const papers = [];\\\\n  const rows = document.querySelectorAll(\\\\'table tr\\\\');\\\\n  \\\\n  for (let row of rows) {\\\\n    const cells = row.querySelectorAll(\\\\'td\\\\');\\\\n    if (cells.length >= 1) {\\\\n      const authorCell = cells[0];\\\\n      const authorText = authorCell.textContent || \\\\'\\\\';\\\\n      \\\\n      // Extract authors (they appear after \\\"Poster Session X\\\" or similar)\\\\n      const match = authorText.match(/(?:Poster Session \\\\\\\\d+|Oral|Highlight[^\u00b7]*)(.*)/);\\\\n      if (match && match[1]) {\\\\n        const authors = match[1].split(\\\\'\u00b7\\\\').map(a => a.trim()).filter(a => a.length > 0);\\\\n        if (authors.length > 0) {\\\\n          papers.push({\\\\n            text: authorText.substring(0, 200),\\\\n            authors: authors\\\\n          });\\\\n        }\\\\n      }\\\\n    }\\\\n  }\\\\n  \\\\n  return {\\\\n    totalPapers: papers.length,\\\\n    sample: papers.slice(0, 10)\\\\n  };\\\\n}');\\n```\\n\\n### New console messages\\n- [ERROR] Failed to load resource: the server responded with a status of 404 (Not Found) @ https://cvp...\\n\\n### Page state\\n- Page URL: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\n- Page Title: CVPR 2025 Accepted Papers\\n- Page Snapshot (Span 1 of 502):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - button \\\"Cookie Preferences\\\" [ref=e23449] [cursor=pointer]:\\n    - img \\\"Cookie Preferences\\\" [ref=e23450] [cursor=pointer]\\n  - banner [ref=e12]:\\n    - link \\\"Skip to yearly menu bar\\\" [ref=e13] [cursor=pointer]:\\n      - /url: \\\"#child-menu\\\"\\n    - link \\\"Skip to main content\\\" [ref=e14] [cursor=pointer]:\\n      - /url: \\\"#main\\\"\\n    - generic \\\"Main Navigation\\\" [ref=e15]:\\n      - heading \\\"Main Navigation\\\" [level=2] [ref=e16]\\n      - generic [ref=e17]:\\n        - link \\\"Home\\\" [ref=e19] [cursor=pointer]:\\n          - /url: /\\n          - img \\\"conference_logo\\\" [ref=e20] [cursor=pointer]\\n        - generic [ref=e21]:\\n          - list [ref=e22]:\\n            - listitem [ref=e23]:\\n              - button \\\"CVPR\\\" [ref=e24] [cursor=pointer]\\n            - listitem [ref=e25]:\\n              - link \\\"Reset Password\\\" [ref=e26] [cursor=pointer]:\\n                - /url: /resetpassword\\n                - generic [ref=e27] [cursor=pointer]: Reset Password\\n            - listitem [ref=e28]:\\n              - link \\\"My Stuff\\\" [ref=e29] [cursor=pointer]:\\n                - /url: /MyStuff\\n                - generic [ref=e30] [cursor=pointer]: My Stuff\\n          - search \\\"Search\\\" [ref=e31]:\\n            - search [ref=e32]:\\n              - textbox \\\"Search\\\" [ref=e33]\\n              - button \\\"execute search\\\" [ref=e35] [cursor=pointer]:\\n                - generic [ref=e36] [cursor=pointer]: \uf002\\n          - link \\\"Login\\\" [ref=e37] [cursor=pointer]:\\n            - /url: \\\"/accounts/login?nextp=/virtual/2025/poster/34726 \\\"\\n            - generic [ref=e38] [cursor=pointer]: \uf2f6\\n            - text: Login\\n  - main [ref=e39]:\\n    - navigation [ref=e42]:\\n      - button \\\"Toggle navigation\\\" [ref=e44] [cursor=pointer]\\n    - generic [ref=e51]:\\n      - heading \\\"CVPR 2025 Accepted Papers\\\" [level=1] [ref=e52]\\n      - paragraph [ref=e53]:\\n        - generic [ref=e54]: This page is cached for 1 hour\\n        - text: . Changes to affiliation or name in your local\\n        - link \\\"profile\\\" [ref=e55] [cursor=pointer]:\\n          - /url: /EditProfile\\n        - text: may take up to 60 minutes to appear here.\\n      - table [ref=e57]:\\n        - rowgroup [ref=e58]:\\n          - row [ref=e59]:\\n            - cell [ref=e60]\\n            - cell [ref=e61]\\n            - cell [ref=e62]\\n          - row [ref=e63]:\\n            - cell [ref=e64]\\n            - cell [ref=e65]\\n            - cell [ref=e66]\\n          - 'row \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi ExHall D Poster #67\\\" [ref=e67]':\\n            - 'cell \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\\" [ref=e68]':\\n              - 'link \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation\\\" [ref=e69] [cursor=pointer]':\\n                - /url: https://pfnet-research.github.io/himor/\\n              - text: Poster Session 1\\n              - generic [ref=e72]: Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\n            - cell [ref=e73]\\n            - 'cell \\\"ExHall D Poster #67\\\" [ref=e74]'\\n          - 'row \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak ExHall D Poster #367\\\" [ref=e75]':\\n            - 'cell \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\\" [ref=e76]':\\n              - 'link \\\"GENIUS: A Generative Framework for Universal Multimodal Search\\\" [ref=e77] [cursor=pointer]':\\n                - /url: https://sung-yeon-kim.github.io/project_pages/GENIUS\\n              - text: Poster Session 4\\n              - generic [ref=e80]: Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\n            - cell [ref=e81]\\n            - 'cell \\\"ExHall D Poster #367\\\" [ref=e82]'\\n          - 'row \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu ExHall D Poster #400\\\" [ref=e83]':\\n            - cell \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\\" [ref=e84]:\\n              - strong [ref=e85]: Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition\\n              - img \\\"Highlight\\\" [ref=e86]\\n              - text: Poster Session 2\\n              - generic [ref=e89]: Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\n            - cell [ref=e90]\\n            - 'cell \\\"ExHall D Poster #400\\\" [ref=e91]'\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01N5AoeJ6kxWH3sHZPjevAKY"}, {"role": "assistant", "content": "The page seems to be dynamically loaded. Let me scroll down to load more content and then try again.", "tool_calls": [{"id": "toolu_01BJDXcxueJkrUuH7z2NHVxz", "type": "function", "function": {"name": "playwright_with_chunk-browser_evaluate", "arguments": "{\"function\": \"async () => {\\n  // Scroll to bottom to load all content\\n  await new Promise(resolve => {\\n    let totalHeight = 0;\\n    const distance = 1000;\\n    const timer = setInterval(() => {\\n      const scrollHeight = document.body.scrollHeight;\\n      window.scrollBy(0, distance);\\n      totalHeight += distance;\\n      \\n      if(totalHeight >= scrollHeight){\\n        clearInterval(timer);\\n        resolve();\\n      }\\n    }, 100);\\n  });\\n  \\n  // Wait a bit for content to load\\n  await new Promise(resolve => setTimeout(resolve, 3000));\\n  \\n  return 'Scrolled to bottom';\\n}\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\n\\\"Scrolled to bottom\\\"\\n\\n### Ran Playwright code\\n```js\\nawait page.evaluate('async () => {\\\\n  // Scroll to bottom to load all content\\\\n  await new Promise(resolve => {\\\\n    let totalHeight = 0;\\\\n    const distance = 1000;\\\\n    const timer = setInterval(() => {\\\\n      const scrollHeight = document.body.scrollHeight;\\\\n      window.scrollBy(0, distance);\\\\n      totalHeight += distance;\\\\n      \\\\n      if(totalHeight >= scrollHeight){\\\\n        clearInterval(timer);\\\\n        resolve();\\\\n      }\\\\n    }, 100);\\\\n  });\\\\n  \\\\n  // Wait a bit for content to load\\\\n  await new Promise(resolve => setTimeout(resolve, 3000));\\\\n  \\\\n  return \\\\'Scrolled to bottom\\\\';\\\\n}');\\n```\\n\\n### New console messages\\n- [ERROR] Failed to load resource: the server responded with a status of 404 (Not Found) @ https://cvp...\\n\\n### Page state\\n- Page URL: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\n- Page Title: CVPR 2025 Accepted Papers\\n- Page Snapshot (Span 1 of 502):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - button \\\"Cookie Preferences\\\" [ref=e23449] [cursor=pointer]:\\n    - img \\\"Cookie Preferences\\\" [ref=e23450] [cursor=pointer]\\n  - banner [ref=e12]:\\n    - link \\\"Skip to yearly menu bar\\\" [ref=e13] [cursor=pointer]:\\n      - /url: \\\"#child-menu\\\"\\n    - link \\\"Skip to main content\\\" [ref=e14] [cursor=pointer]:\\n      - /url: \\\"#main\\\"\\n    - generic \\\"Main Navigation\\\" [ref=e15]:\\n      - heading \\\"Main Navigation\\\" [level=2] [ref=e16]\\n      - generic [ref=e17]:\\n        - link \\\"Home\\\" [ref=e19] [cursor=pointer]:\\n          - /url: /\\n          - img \\\"conference_logo\\\" [ref=e20] [cursor=pointer]\\n        - generic [ref=e21]:\\n          - list [ref=e22]:\\n            - listitem [ref=e23]:\\n              - button \\\"CVPR\\\" [ref=e24] [cursor=pointer]\\n            - listitem [ref=e25]:\\n              - link \\\"Reset Password\\\" [ref=e26] [cursor=pointer]:\\n                - /url: /resetpassword\\n                - generic [ref=e27] [cursor=pointer]: Reset Password\\n            - listitem [ref=e28]:\\n              - link \\\"My Stuff\\\" [ref=e29] [cursor=pointer]:\\n                - /url: /MyStuff\\n                - generic [ref=e30] [cursor=pointer]: My Stuff\\n          - search \\\"Search\\\" [ref=e31]:\\n            - search [ref=e32]:\\n              - textbox \\\"Search\\\" [ref=e33]\\n              - button \\\"execute search\\\" [ref=e35] [cursor=pointer]:\\n                - generic [ref=e36] [cursor=pointer]: \uf002\\n          - link \\\"Login\\\" [ref=e37] [cursor=pointer]:\\n            - /url: \\\"/accounts/login?nextp=/virtual/2025/poster/34726 \\\"\\n            - generic [ref=e38] [cursor=pointer]: \uf2f6\\n            - text: Login\\n  - main [ref=e39]:\\n    - navigation [ref=e42]:\\n      - button \\\"Toggle navigation\\\" [ref=e44] [cursor=pointer]\\n    - generic [ref=e51]:\\n      - heading \\\"CVPR 2025 Accepted Papers\\\" [level=1] [ref=e52]\\n      - paragraph [ref=e53]:\\n        - generic [ref=e54]: This page is cached for 1 hour\\n        - text: . Changes to affiliation or name in your local\\n        - link \\\"profile\\\" [ref=e55] [cursor=pointer]:\\n          - /url: /EditProfile\\n        - text: may take up to 60 minutes to appear here.\\n      - table [ref=e57]:\\n        - rowgroup [ref=e58]:\\n          - row [ref=e59]:\\n            - cell [ref=e60]\\n            - cell [ref=e61]\\n            - cell [ref=e62]\\n          - row [ref=e63]:\\n            - cell [ref=e64]\\n            - cell [ref=e65]\\n            - cell [ref=e66]\\n          - 'row \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi ExHall D Poster #67\\\" [ref=e67]':\\n            - 'cell \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation Poster Session 1 Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\\" [ref=e68]':\\n              - 'link \\\"HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation\\\" [ref=e69] [cursor=pointer]':\\n                - /url: https://pfnet-research.github.io/himor/\\n              - text: Poster Session 1\\n              - generic [ref=e72]: Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi\\n            - cell [ref=e73]\\n            - 'cell \\\"ExHall D Poster #67\\\" [ref=e74]'\\n          - 'row \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak ExHall D Poster #367\\\" [ref=e75]':\\n            - 'cell \\\"GENIUS: A Generative Framework for Universal Multimodal Search Poster Session 4 Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\\" [ref=e76]':\\n              - 'link \\\"GENIUS: A Generative Framework for Universal Multimodal Search\\\" [ref=e77] [cursor=pointer]':\\n                - /url: https://sung-yeon-kim.github.io/project_pages/GENIUS\\n              - text: Poster Session 4\\n              - generic [ref=e80]: Sungyeon Kim \u00b7 Xinliang Zhu \u00b7 Xiaofan Lin \u00b7 Muhammet Bastan \u00b7 Douglas Gray \u00b7 Suha Kwak\\n            - cell [ref=e81]\\n            - 'cell \\\"ExHall D Poster #367\\\" [ref=e82]'\\n          - 'row \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu ExHall D Poster #400\\\" [ref=e83]':\\n            - cell \\\"Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition Highlight Poster Session 2 Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\\" [ref=e84]:\\n              - strong [ref=e85]: Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition\\n              - img \\\"Highlight\\\" [ref=e86]\\n              - text: Poster Session 2\\n              - generic [ref=e89]: Junyi Wu \u00b7 Yan Huang \u00b7 Min Gao \u00b7 Yuzhen Niu \u00b7 Yuzhong Chen \u00b7 Qiang Wu\\n            - cell [ref=e90]\\n            - 'cell \\\"ExHall D Poster #400\\\" [ref=e91]'\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01BJDXcxueJkrUuH7z2NHVxz"}, {"role": "assistant", "content": "Let me try a different approach - I'll use the fetch tool to get the HTML directly and parse it.", "tool_calls": [{"id": "toolu_01TKdEiqHsBxn89ALh2SRoK8", "type": "function", "function": {"name": "fetch-fetch_html", "arguments": "{\"url\": \"https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\n\\n\\n\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\\\"en\\\" style=\\\"scroll-padding-top: 70px;\\\"> \\n\\n<head>\\n    \\n    <meta charset=\\\"utf-8\\\">\\n    <meta http-equiv=\\\"X-UA-Compatible\\\" content=\\\"IE=edge\\\">\\n    <meta name=\\\"apple-mobile-web-app-capable\\\" content=\\\"yes\\\">\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\">\\n    <script src=\\\"/static/virtual/js/virtual.js\\\"></script>\\n    <meta name=\\\"google-site-verification\\\" content=\\\"0jwPnVXIAk4FvFdT37dwMmd-kjHF86e5DKwvqlStUW0\\\">\\n\\n\\n    \\n    <link rel=\\\"stylesheet\\\" href=\\\"/static/core/css/core.css\\\" type=\\\"text/css\\\">\\n    <link rel=\\\"stylesheet\\\" href=\\\"/static/virtual/css/virtual.css\\\" type=\\\"text/css\\\">\\n     <link href=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\\\" rel=\\\"stylesheet\\\" integrity=\\\"sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65\\\" crossorigin=\\\"anonymous\\\">\\n\\n    <link rel=\\\"stylesheet\\\" href=\\\"/static/core/css/custom.css\\\" type=\\\"text/css\\\">\\n    \\n    <link rel=\\\"stylesheet\\\" href=\\\"https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/css/bootstrap-select.min.css\\\">\\n    <link href=\\\"https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap\\\" rel=\\\"stylesheet\\\">\\n    <script nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\">\\n        window.MathJax = {\\n          tex: {\\n            inlineMath: [['$', '$'], ['\\\\\\\\(', '\\\\\\\\)']],\\n            displayMath: [['\\\\\\\\[', '\\\\\\\\]']],\\n            processEscapes: true,\\n          },\\n          options: {\\n            ignoreHtmlClass: 'no-mathjax',\\n          }\\n        };\\n    </script>\\n\\n    <script id=\\\"MathJax-script\\\" async\\n            src=\\\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\\\"\\n            nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\" >\\n    </script>\\n\\n    <!--This script keeps local links inside the web app rather than opening them\\nin Safari, and has nothing to do with editing or Aloha.-->\\n\\n<script nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\">\\n    (function (document, navigator, standalone) {\\n\\n        if ((standalone in navigator) && navigator[standalone]) {\\n            var curnode, location = document.location, stop = /^(a|html)$/i;\\n            document.addEventListener('click', function (e) {\\n                curnode = e.target;\\n                while (!(stop).test(curnode.nodeName)) {\\n                    curnode = curnode.parentNode;\\n                }\\n\\n                if (\\n                    'href' in curnode && // is a link\\n                    (chref = curnode.href).replace(location.href, '').indexOf('#') && // is not an anchor\\n                    (!(/^[a-z\\\\+\\\\.\\\\-]+:/i).test(chref) ||                       // either does not have a proper scheme (relative links)\\n                        chref.indexOf(location.protocol + '//' + location.host) === 0) // or is in the same protocol and domain\\n                ) {\\n                    e.preventDefault();\\n                    location.href = curnode.href;\\n                }\\n            }, false);\\n        }\\n    })(document, window.navigator, 'standalone');\\n</script>\\n\\n\\n<style>\\n\\n    @media screen and (min-width: 767px) {\\n        .blurb {\\n            min-height: 260px;\\n        }\\n    }\\n</style>\\n    \\n\\n<script src=\\\"https://code.jquery.com/jquery-3.6.1.min.js\\\"\\n        integrity=\\\"sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=\\\" crossorigin=\\\"anonymous\\\">\\n</script>\\n\\n<script nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\">\\n    if (typeof jQuery === 'undefined') {\\n        var script = document.createElement('script');\\n        script.type = 'text/javascript';\\n        script.src = \\\"/static/core/js/jquery-3.6.1.min.js\\\";\\n        document.head.appendChild(script);\\n    }\\n</script>\\n\\n    \\n    <script nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\">\\n        var $ = jQuery;\\n    </script>\\n\\n    \\n    <script src=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js\\\" integrity=\\\"sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4\\\" crossorigin=\\\"anonymous\\\"></script>\\n\\n    <script src=\\\"/static/core/js/ajax-csrf-snippet.js\\\"></script>\\n    <script src=\\\"https://kit.fontawesome.com/be44b7e05d.js\\\" crossorigin=\\\"anonymous\\\"></script>\\n    <script src=\\\"https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/js/bootstrap-select.min.js\\\"></script>\\n\\n\\n    <style nonce=\\\"BNUEyA8gcQ5TvtKG6613BQ==\\\">\\n        body {\\n            font-family: Exo;}\\n    </style>\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n    <link rel=\\\"stylesheet\\\"\\n          href=\\\"https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800\\\">\\n    <link rel=\\\"stylesheet\\\" href=\\\"https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic\\\">\\n    <link href=\\\"https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap\\\" rel=\\\"stylesheet\\\">\\n    <link rel=\\\"stylesheet\\\" href=\\\"/static/virtual/css/virtual.css\\\">\\n    <script src=\\\"https://d3js.org/d3.v5.min.js\\\"></script>\\n    <script src=\\\"https://cdnjs.cloudflare.com/ajax/libs/corejs-typeahead/1.3.1/typeahead.bundle.min.js\\\" integrity=\\\"sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==\\\" crossorigin=\\\"anonymous\\\"></script>\\n    <script src=\\\"/static/core/js/moment/min/moment.min.js\\\"></script>\\n    <script src=\\\"https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\\\"></script>\\n    <script src=\\\"/static/core/js/ajax-csrf-snippet.js\\\" ></script>\\n    <script src=\\\"/static/virtual/js/virtual.js\\\"></script>\\n    \\n\\n    \\n\\n    \\n    <link rel=\\\"canonical\\\" href=\\\"https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\\\">\\n\\n    <title>CVPR 2025 Accepted Papers</title>\\n</head>\\n\\n<body>\\n<!-- <h1><a href=\\\"#\\\" onclick=\\\"alert('CSP Test');\\\">CSP Test</a></h1> -->\\n\\n\\n\\n\\n<div class=\\\"noprint\\\">\\n    \\n        <!--Navbar start-->\\n<header>\\n    <a href=\\\"#child-menu\\\" class=\\\"off-screen\\\">Skip to yearly menu bar</a>\\n    <a href=\\\"#main\\\" class=\\\"off-screen\\\">Skip to main content</a>\\n    <div id=\\\"id_navbar\\\" class=\\\"navbar navbar-expand-sm navbar-dark\\\" aria-label=\\\"Main Navigation\\\"\\n         style=\\\"background-color: #1B427D;\\\">\\n        <h2 class=\\\"off-screen\\\">Main Navigation</h2>\\n        <div class=\\\"container-fluid\\\">\\n            <div><a class=\\\"navbar-brand\\\" href=\\\"/\\\" aria-label=\\\"Home\\\">\\n\\n                <img src=\\\"/static/core/img/cvpr-navbar-logo.svg\\\" alt=\\\"conference_logo\\\" height=\\\"40\\\"></a></div>\\n\\n\\n            <button class=\\\"navbar-toggler\\\" type=\\\"button\\\" data-bs-toggle=\\\"collapse\\\"\\n                    data-bs-target=\\\"#navbarToggler1\\\"\\n                    aria-expanded=\\\"false\\\" aria-label=\\\"Toggle main navigation\\\">\\n                <span class=\\\"navbar-toggler-icon\\\"></span>\\n            </button>\\n\\n            <div class=\\\"collapse navbar-collapse\\\" id=\\\"navbarToggler1\\\">\\n                <ul class=\\\"navbar-nav me-auto mb-2 mb-lg-0\\\">\\n                    \\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            CVPR\\n        </a>\\n        <ul class=\\\"dropdown-menu dropdown-menu-dark\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CodeOfConduct\\\">\\n                    <span >\\n                        Code of Conduct\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Profile/create\\\">\\n                    <span >\\n                        Create Profile\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/resetpassword\\\">\\n                    <span >\\n                        Reset / Forgot Password\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2024/PrivacyPolicy\\\">\\n                    <span >\\n                        Privacy Policy\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Help/Contact\\\">\\n                    <span >\\n                        Contact CVPR\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/FAQ\\\">\\n                    <span >\\n                        HELP/FAQ\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/resetpassword\\\">\\n                    <span >\\n                        Reset Password\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/MyStuff\\\">\\n                    <span >\\n                        My Stuff\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n                </ul>\\n\\n                \\n                <form class=\\\"d-flex mx-2\\\" aria-label=\\\"Search\\\" role=\\\"search\\\" action=\\\"/search\\\">\\n                    <div class=\\\"input-group\\\" role=\\\"search\\\" style=\\\"outline-color:green;\\\">\\n                        <input type=\\\"text\\\" class=\\\"form-control\\\" placeholder=\\\"Search\\\" name=\\\"q\\\"\\n                               value=\\\"\\\"\\n                               aria-label=\\\"Search\\\" aria-describedby=\\\"btnGroupAddon\\\"\\n                                id=\\\"navbar-search\\\">\\n                        <div class=\\\"input-group-text btn-primary\\\" id=\\\"btnGroupAddon\\\">\\n                            <button style=\\\"border: none; background-color: transparent; padding: 0;\\\" type=\\\"submit\\\" aria-label=\\\"execute search\\\">\\n                                <i class=\\\"fa-solid fa-magnifying-glass\\\"></i>\\n                            </button>\\n                        </div>\\n                    </div>\\n                </form>\\n                \\n\\n                \\n                    <a href=\\\"/accounts/login?nextp=/virtual/2025/poster/34726 \\\" class=\\\"navbar-brand\\\"><span\\n                            class=\\\"fa-solid fa-right-to-bracket\\\"></span> Login</a>\\n                \\n\\n            </div>\\n        </div>\\n    </div>\\n</header>\\n<!--Navbar end-->\\n    \\n</div><!--noprint div-->\\n\\n<!--This holds the whole page including the navbar-->\\n<main id=\\\"main\\\">\\n    \\n        <div class=\\\"container-fluid\\\">\\n            <!--Navbar start-->\\n\\n<div class=\\\"dropdown\\\" id=\\\"child-menu\\\">\\n    <nav class=\\\"align-middle navbar navbar-expand-lg mx-4 border border-3 border-top-0 rounded-bottom\\\"\\n         style=\\\"min-height: 57px; background-color: #F6f6f6;\\\">\\n        <div class=\\\"container-fluid\\\">\\n\\n            <button class=\\\"navbar-toggler\\\" type=\\\"button\\\" data-bs-toggle=\\\"collapse\\\"\\n                    data-bs-target=\\\"#navbarToggler1029\\\"\\n                    aria-expanded=\\\"false\\\" aria-label=\\\"Toggle navigation\\\">\\n                <span class=\\\"navbar-toggler-icon\\\"></span>\\n            </button>\\n\\n            <div class=\\\"collapse navbar-collapse\\\" id=\\\"navbarToggler1029\\\">\\n                <ul class=\\\"navbar-nav me-auto mb-lg-0\\\">\\n                    \\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle border-3  btn btn-primary text-white p-1\\\" style= \\\"background-color: #070bff; font-size: 1.2em;\\\"\\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Select Year: (2025)\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item \\\" >\\n                <a class=\\\"dropdown-item p-1\\\"\\n                   href=\\\"/Conferences/2026\\\">2026\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item \\\" >\\n                <a class=\\\"dropdown-item p-1\\\"\\n                   href=\\\"/Conferences/2025\\\">2025\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item \\\" >\\n                <a class=\\\"dropdown-item p-1\\\"\\n                   href=\\\"/Conferences/2024\\\">2024\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item \\\" >\\n                <a class=\\\"dropdown-item p-1\\\"\\n                   href=\\\"/Conferences/2023\\\">2023\\n                </a>\\n                \\n            </li>\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Dates\\\">\\n                    <span >\\n                        Dates\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Calls\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForPapers\\\">\\n                    <span >\\n                        Call for Papers\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForTutorials\\\">\\n                    <span >\\n                        Call for Tutorial Proposals\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/WorkshopProposals\\\">\\n                    <span >\\n                        Call for Workshop Proposals\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForMusicalPerformance\\\">\\n                    <span >\\n                        Call for Musical Performance\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForSocials\\\">\\n                    <span >\\n                        Call for Socials\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForAIArt\\\">\\n                    <span >\\n                        Call for AI Art\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForDemos\\\">\\n                    <span >\\n                        Call for Demos\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CallForDoctoralConsortium\\\">\\n                    <span >\\n                        Call for Participation: Doctoral Consortium\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Author &amp; Reviewer Guides\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/AuthorGuidelines\\\">\\n                    <span >\\n                        Author Guidelines\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/AuthorSuggestedPractices\\\">\\n                    <span >\\n                        Author Suggested Practices\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/EthicsGuidelines\\\">\\n                    <span >\\n                        Author Ethics Guidelines\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/YouTubeVideoPosterUpload\\\">\\n                    <span >\\n                        YouTube and Poster Art Uploads\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" style=\\\"pointer-events: none;   cursor: default;   text-decoration: none;   color: black;\\\">\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"\\\">\\n                    <span >\\n                        Reviewers\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ReviewerGuidelines\\\">\\n                    <span >\\n                        Reviewer Guidelines\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/PosterPrintingInformation\\\">\\n                    <span >\\n                        Oral/Poster Guidelines\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" style=\\\"pointer-events: none;   cursor: default;   text-decoration: none;   color: black;\\\">\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"\\\">\\n                    <span >\\n                        Authors\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CVPRChanges\\\">\\n                    <span >\\n                        Changes for 2025\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CompleteYourORProfile\\\">\\n                    <span >\\n                        How to complete your OpenReview profile\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Clarification\\\">\\n                    <span >\\n                        Clarification\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://docs.google.com/document/d/1O3xcBy7O93hb9FY6c3Wg7fc8bKvvZFVvF3Ynmjhsmlg/edit?usp=sharing\\\">\\n                    <span >\\n                        Camera-Ready Submission Instructions\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://docs.google.com/document/d/1t5-TzipreuxSdLqlq5dnJXXrwTdcMy3moNlr4nEZw00/edit?usp=sharing\\\">\\n                    <span >\\n                        Author Submission Site Guide\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://media.eventhosts.cc/Conferences/CVPR2025/Im_Presenting_Social_Toolkit.zip\\\">\\n                    <span >\\n                        I'm Presenting Social Toolkit\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Attend\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_main_conf_2025.pdf\\\">\\n                    <span >\\n                        Main Conference Program\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/FAQ\\\">\\n                    <span >\\n                        <strong>2025 In-person FAQ</strong>\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Pricing2\\\">\\n                    <span >\\n                        Register\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/InvitationLetter\\\">\\n                    <span >\\n                        Invitation Letter\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ProgramOverview\\\">\\n                    <span >\\n                        Program Overview\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://www.scholar-inbox.com/conference/cvpr/2025\\\">\\n                    <span >\\n                        Scholar Inbox (Paper Recommendation System)\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/BroadeningParticipation\\\">\\n                    <span >\\n                        Broadening Participation\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Hotels\\\">\\n                    <span >\\n                        Book Your Hotel\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/CodeOfConduct\\\">\\n                    <span >\\n                        Code of Conduct\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/KeynotesAndPanels\\\">\\n                    <span >\\n                        Keynotes & Panels\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/tutorial-list\\\">\\n                    <span >\\n                        Tutorials\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/workshop-list\\\">\\n                    <span >\\n                        Workshops\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://thecvf-art.com\\\">\\n                    <span >\\n                        Art Program\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Expo\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Sponsors\\\">\\n                    <span >\\n                        Sponsors\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ExhibitorInformation\\\">\\n                    <span >\\n                        Exhibitor Information\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ExpoSchedule\\\">\\n                    <span >\\n                        Expo Schedule\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"https://hallerickson.ungerboeck.com/prod/app85.cshtml?aat=42382b5775704b416d4c42786d3538456845676372793061315862414a5267714b7a624d465848315447303d\\\">\\n                    <span >\\n                        Sponsor, Exhibitor List &amp; Floor-plan\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/PromotionalOpportunities\\\">\\n                    <span >\\n                        Promotional Opportunities\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ExhibitorManual\\\">\\n                    <span >\\n                        2025 Exhibitor Manual\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/PRProfessionals\\\">\\n                    <span >\\n                        Exhibitor/Sponsor PR Professionals\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Media\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/PressLandingPage\\\">\\n                    <span >\\n                        Media Center\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/MediaPass\\\">\\n                    <span >\\n                        Get Media Pass\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/NewsAndResources\\\">\\n                    <span >\\n                        News and Resources\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    <li class=\\\"dropdown-item dropdown pe-3\\\">\\n        <a class=\\\"nav-link dropdown-toggle  p-1\\\" \\n           href=\\\"#\\\"\\n           role=\\\"button\\\" data-bs-toggle=\\\"dropdown\\\" aria-expanded=\\\"false\\\">\\n            Organization\\n        </a>\\n        <ul class=\\\"dropdown-menu\\\">\\n            \\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/Organizers\\\">\\n                    <span >\\n                        Organizing Committee\\n                    </span>\\n                </a>\\n                \\n                    <hr class=\\\"dropdown-divider\\\" aria-hidden=\\\"true\\\">\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/Conferences/2025/ProgramCommittee\\\">\\n                    <span >\\n                        Program Committee\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n        </ul>\\n    </li>\\n    \\n\\n\\n\\n    \\n        \\n            <li class=\\\"dropdown-item  pe-2\\\" >\\n                <a class=\\\"nav-link p-1\\\"  href=\\\"/virtual/2025/index.html\\\">\\n                    <span >\\n                        Conference Site\\n                    </span>\\n                </a>\\n                \\n            </li>\\n\\n        \\n\\n    \\n\\n\\n\\n                </ul>\\n            </div>\\n        </div>\\n    </nav>\\n</div>\\n    <!--Navbar end-->\\n        </div>\\n        <br><br>\\n    \\n    \\n        \\n        <div class=\\\"container\\\">\\n    \\n    \\n\\n    \\n    <div class=\\\"container\\\">\\n        \\n            <!--JUst above the HTML in document-snippet.html-->\\n            <div> <div>\\n<h1>CVPR 2025 Accepted Papers</h1>\\n\\n<p><span class=\\\"highlight\\\">This page is cached for 1 hour</span>. \u00a0Changes to affiliation or name in your local\u00a0<a href=\\\"/EditProfile\\\">profile</a> may take up to 60 minutes to appear here.</p>\\n\\n<div>\\n\\n<table>\\n    <tr>\\n        <td></td>\\n        <td></td>\\n        <td></td>\\n    </tr>\\n\\n\\n    <tr>\\n        \\n            <th></th>\\n        \\n\\n        \\n            <th></th>\\n            <th></th>\\n        \\n    </tr>\\n    \\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://pfnet-research.github.io/himor/\\\" target=\\\"_blank\\\">HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation</a>\\n                    \\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yiming Liang &middot; Tianhan Xu &middot; Yuta Kikuchi\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #67\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://sung-yeon-kim.github.io/project_pages/GENIUS\\\" target=\\\"_blank\\\">GENIUS: A Generative Framework for Universal Multimodal Search</a>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Sungyeon Kim &middot; Xinliang Zhu &middot; Xiaofan Lin &middot; Muhammet Bastan &middot; Douglas Gray &middot; Suha Kwak\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #367\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition</strong>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Junyi Wu &middot; Yan Huang &middot; Min Gao &middot; Yuzhen Niu &middot; Yuzhong Chen &middot; Qiang Wu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #400\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Mark Boss &middot; Zixuan Huang &middot; Aaryaman Vasishta &middot; Varun Jampani\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #37\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Towards Precise Embodied Dialogue Localization via Causality Guided Diffusion</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Haoyu Wang &middot; Le Wang &middot; Sanping Zhou &middot; Jingyi Tian &middot; Zheng Qin &middot; Yabing Wang &middot; Gang Hua &middot; Wei Tang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #257\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Customized Condition Controllable Generation for Video Soundtrack</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Fan Qi &middot; KunSheng Ma &middot; Changsheng Xu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #277\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/mlvlab/EfficientViM\\\" target=\\\"_blank\\\">EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality</a>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Sanghyeok Lee &middot; Joonmyung Choi &middot; Hyunwoo J. Kim\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #408\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Keyu Tu &middot; Mengqi Huang &middot; Zhuowei Chen &middot; Zhendong Mao\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #258\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Ali Athar &middot; Xueqing Deng &middot; Liang-Chieh Chen\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #308\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>A Universal Scale-Adaptive Deformable Transformer for Image Restoration across Diverse Artifacts</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Xuyi He &middot; Yuhui Quan &middot; Ruotao Xu &middot; Hui Ji\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #199\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                James Burgess &middot; Jeffrey J Nirschl &middot; Laura Bravo-S\u00e1nchez &middot; Alejandro Lozano &middot; Sanket Rajan Gupte &middot; Jesus G. Galaz-Montoya &middot; Yuhui Zhang &middot; Yuchang Su &middot; Disha Bhowmik &middot; Zachary Coman &middot; Sarina M. Hasan &middot; Alexandra Johannesson &middot; William D. Leineweber &middot; Malvika G Nair &middot; Ridhi Yarlagadda &middot; Connor Zuraski &middot; Wah Chiu &middot; Sarah Cohen &middot; Jan N. Hansen &middot; Manuel D Leonetti &middot; Chad Liu &middot; Emma Lundberg &middot; Serena Yeung\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #357\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yu Mao &middot; Jun Wang &middot; Nan Guan &middot; Chun Jason Xue\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #305\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Gromov\u2013Wasserstein Problem with Cyclic Symmetry</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Shoichiro Takeda &middot; Yasunori Akagi\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #69\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images</strong>\\n                    \\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Chih-Hao Lin &middot; Jia-Bin Huang &middot; Zhengqin Li &middot; Zhao Dong &middot; Christian Richardt &middot; Michael Zollhoefer &middot; Tuotuo Li &middot; Johannes Kopf &middot; Shenlong Wang &middot; Changil Kim\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #28\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Junjin Xiao &middot; Qing Zhang &middot; Yongwei Nie &middot; Lei Zhu &middot; Wei-Shi Zheng\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #51\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>SDBF: Steep-Decision-Boundary Fingerprinting for Hard-Label Tampering Detection of DNN Models</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Xiaofan Bai &middot; Shixin Li &middot; Xiaojing Ma &middot; Bin Benjamin Zhu &middot; Dongmei Zhang &middot; Linchen Yu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #299\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>EnliveningGS: Active Locomotion of 3DGS</strong>\\n                    \\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Siyuan Shen &middot; Tianjia Shao &middot; Kun Zhou &middot; Chenfanfu Jiang &middot; Yin Yang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #68\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/WenRuiCai/SPMTrack\\\" target=\\\"_blank\\\">SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking</a>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Wenrui Cai &middot; Qingjie Liu &middot; Yunhong Wang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #100\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Felix Wimbauer &middot; Weirong Chen &middot; Dominik Muhle &middot; Christian Rupprecht &middot; Daniel Cremers\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #85\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://yinwen2019.github.io/ucdver/\\\" target=\\\"_blank\\\">Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition</a>\\n                    \\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Wen Yin &middot; Yong Wang &middot; Guiduo Duan &middot; Dongyang Zhang &middot; XIN Hu &middot; Yuan-Fang Li &middot; Tao He\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #354\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Distilling Multi-modal Large Language Models for Autonomous Driving</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Deepti Hegde &middot; Rajeev Yasarla &middot; Hong Cai &middot; Shizhong Han &middot; Apratim Bhattacharyya &middot; Shweta Mahajan &middot; Litian Liu &middot; Risheek Garrepalli &middot; Vishal M. Patel &middot; Fatih Porikli\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #135\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Jinneyong Kim &middot; Seung-Hwan Baek\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #80\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding</strong>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Zhaoran Zhao &middot; Peng Lu &middot; Anran Zhang &middot; Pei Pei Li &middot; Xia Li &middot; Xuannan Liu &middot; Yang Hu &middot; Shiyi Chen &middot; liweiwang &middot; Wenhao Guo\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #360\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/vladan-stojnic/LPOSS\\\" target=\\\"_blank\\\">LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation</a>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Vladan Stojni\u0107 &middot; Yannis Kalantidis &middot; Jiri Matas &middot; Giorgos Tolias\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #421\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>PhysGen3D: Crafting a Miniature Interactive World from a Single Image</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Boyuan Chen &middot; Hanxiao Jiang &middot; Shaowei Liu &middot; Saurabh Gupta &middot; Yunzhu Li &middot; Hao Zhao &middot; Shenlong Wang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #71\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Scaling Properties of Diffusion Models For Perceptual Tasks</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Rahul Ravishankar &middot; Zeeshan Patel &middot; Jathushan Rajasegaran &middot; Jitendra Malik\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #219\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://hd-epic.github.io/\\\" target=\\\"_blank\\\">HD-EPIC: A Highly-Detailed Egocentric Video Dataset</a>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Toby Perrett &middot; Ahmad Darkhalil &middot; Saptarshi Sinha &middot; Omar Emara &middot; Sam Pollard &middot; Kranti Kumar Parida &middot; Kaiting Liu &middot; Prajwal Gatti &middot; Siddhant Bansal &middot; Kevin Flanagan &middot; Jacob Chalk &middot; Zhifan Zhu &middot; Rhodri Guerrier &middot; Fahd Abdelazim &middot; Bin Zhu &middot; Davide Moltisanti &middot; Michael Wray &middot; Hazel Doughty &middot; Dima Damen\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #276\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/MiSsU-HH/Exact\\\" target=\\\"_blank\\\">Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation</a>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Hao Zhu &middot; Yan Zhu &middot; Jiayu Xiao &middot; Tianxiang Xiao &middot; Yike Ma &middot; Yucheng Zhang &middot; Feng Dai\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #324\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://mdyao.github.io/PolarFree/\\\" target=\\\"_blank\\\">PolarFree: Polarization-based Reflection-Free Imaging</a>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Mingde Yao &middot; Menglu Wang &middot; King Man Tam &middot; Lingen Li &middot; Tianfan Xue &middot; Jinwei Gu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #22\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://actionlab-cv.github.io/HMoRe\\\" target=\\\"_blank\\\">H-MoRe: Learning Human-centric Motion Representation for Action Analysis</a>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Zhanbo Huang &middot; Xiaoming Liu &middot; Yu Kong\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #156\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Can K\u00fc\u00e7\u00fcks\u00f6zen &middot; Yucel Yemez\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #415\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Effortless Active Labeling for Long-Term Test-Time Adaptation</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Guowei Wang &middot; Changxing Ding\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #439\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Daniel Etaat &middot; Dvij Rajesh Kalaria &middot; Nima Rahmanian &middot; Shankar Sastry\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #168\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Pay Attention to the Foreground in Object-Centric Learning</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Pinzhuo Tian &middot; Shengjie Yang &middot; Hang Yu &middot; Alex C. Kot\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #396\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://lttm.github.io/MultimodalStudio/\\\" target=\\\"_blank\\\">MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for Neural Rendering across Multiple Imaging Modalities</a>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Federico Lincetto &middot; Gianluca Agresti &middot; Mattia Rossi &middot; Pietro Zanuttigh\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #29\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Dense-SfM: Structure from Motion with Dense Consistent Matching</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                JongMin Lee &middot; Sungjoo Yoo\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #98\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yue Gao &middot; Hong-Xing Yu &middot; Bo Zhu &middot; Jiajun Wu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #32\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                zhuangzhuang chen &middot; hualiang wang &middot; Chubin Ou &middot; Xiaomeng Li\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #483\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/Land5cape/DeepCausalQuality\\\" target=\\\"_blank\\\">Image Quality Assessment: Investigating Causal Perceptual Effects with Abductive Counterfactual Inference</a>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Wenhao Shen &middot; Mingliang Zhou &middot; Yu Chen &middot; Xuekai WEI &middot; Yong Feng &middot; Huayan Pu &middot; Weijia Jia\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #208\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Luyao Tang &middot; Chaoqi Chen &middot; Yuxuan Yuan &middot; Zeyu Zhang &middot; Yue Huang &middot; Kun Zhang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #418\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yutao Tang &middot; Yuxiang Guo &middot; Deming Li &middot; Cheng Peng\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #64\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://hanzhic.github.io/vidbot-project/\\\" target=\\\"_blank\\\">VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation</a>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Hanzhi Chen &middot; Boyang Sun &middot; Anran Zhang &middot; Marc Pollefeys &middot; Stefan Leutenegger\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #143\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Learning Person-Specific Animatable Face Models from In-the-Wild Images via a Shared Base Model</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yuxiang Mao &middot; Zhenfeng Fan &middot; Zhijie Zhang &middot; Zhiheng Zhang &middot; Shihong Xia\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #15\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://aigc-explorer.github.io/TIMotion-page/\\\" target=\\\"_blank\\\">TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation</a>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yabiao Wang &middot; Shuo Wang &middot; Jiangning Zhang &middot; Ke Fan &middot; Jiafu Wu &middot; Xuezhucun Xue &middot; Yong Liu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #173\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos</strong>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Sagnik Majumder &middot; Tushar Nagarajan &middot; Ziad Al-Halah &middot; Reina Pradhan &middot; Kristen Grauman\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #275\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Hybrid Reciprocal Transformer with Triplet Feature Alignment for Scene Graph Generation</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Jiawei Fu &middot; ZHANG Tiantian &middot; Kai Chen &middot; Qi Dou\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #343\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>TransPixeler: Advancing Text-to-Video Generation with Transparency</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Luozhou Wang &middot; Yijun Li &middot; ZhiFei Chen &middot; Jui-Hsien Wang &middot; Zhifei Zhang &middot; He Zhang &middot; Zhe Lin &middot; Ying-Cong Chen\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #232\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/ncTimTang/AKS\\\" target=\\\"_blank\\\">Adaptive Keyframe Sampling for Long Video Understanding</a>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Xi Tang &middot; Jihao Qiu &middot; Lingxi Xie &middot; Yunjie Tian &middot; Jianbin Jiao &middot; Qixiang Ye\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #284\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Person De-reidentification: A Variation-guided Identity Shift Modeling</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yi-Xing Peng &middot; Yu-Ming Tang &middot; Kun-Yu Lin &middot; Qize Yang &middot; Jingke Meng &middot; Xihan Wei &middot; Wei-Shi Zheng\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #304\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Lue Fan &middot; Hao ZHANG &middot; Qitai Wang &middot; Hongsheng Li &middot; Zhaoxiang Zhang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #131\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Hasin Us Sami &middot; Swapneel Sen &middot; Amit K. Roy-Chowdhury &middot; Srikanth Krishnamurthy &middot; Basak Guler\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #462\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Ho-Joong Kim &middot; Yearang Lee &middot; Jung-Ho Hong &middot; Seong-Whan Lee\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #312\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Runtao Liu &middot; Haoyu Wu &middot; Zheng Ziqiang &middot; Chen Wei &middot; Yingqing He &middot; Renjie Pi &middot; Qifeng Chen\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #252\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://github.com/MaxZanella/StatA\\\" target=\\\"_blank\\\">Realistic Test-Time Adaptation of Vision-Language Models</a>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Maxime Zanella &middot; Cl\u00e9ment Fuchs &middot; Christophe De Vleeschouwer &middot; Ismail Ben Ayed\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #388\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://gynjn.github.io/selfsplat/\\\" target=\\\"_blank\\\">SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting</a>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Gyeongjin Kang &middot; Jisang Yoo &middot; Jihyeon Park &middot; Seungtae Nam &middot; Hyeonsoo Im &middot; Shin sangheon &middot; Sangpil Kim &middot; Eunbyung Park\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #92\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</strong>\\n                    \\n                    \\n                    Poster Session 2<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Gonglin Chen &middot; Tianwen Fu &middot; Haiwei Chen &middot; Wenbin Teng &middot; Hanyuan Xiao &middot; Yajie Zhao\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #97\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Nannan Li &middot; Kevin Shih &middot; Bryan A. Plummer\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #18\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs</strong>\\n                    \\n                    \\n                    Poster Session 4<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yi Fang &middot; Bowen Jin &middot; Jiacheng Shen &middot; Sirui Ding &middot; Qiaoyu Tan &middot; Jiawei Han\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #349\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Erasing Undesirable Influence in Diffusion Models</strong>\\n                    \\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Jing Wu &middot; Trung Le &middot; Munawar Hayat &middot; Mehrtash Harandi\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #200\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://quan-meng.github.io/projects/lt3sd/\\\" target=\\\"_blank\\\">LT3SD: Latent Trees for 3D Scene Diffusion</a>\\n                    \\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Quan Meng &middot; Lei Li &middot; Matthias Nie\u00dfner &middot; Angela Dai\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #45\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Jikang Cheng &middot; Zhiyuan Yan &middot; Ying Zhang &middot; Li Hao &middot; Jiaxin Ai &middot; Qin Zou &middot; Chen Li &middot; Zhongyuan Wang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #313\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>Closest Neighbors are Harmful for Lightweight Masked Auto-encoders</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Jian Meng &middot; Ahmed Hasssan &middot; Li Yang &middot; Deliang Fan &middot; Jinwoo Shin &middot; Jae-sun Seo\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #400\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning</strong>\\n                    \\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Qianli Ma &middot; Xuefei Ning &middot; Dongrui Liu &middot; Li Niu &middot; Linfeng Zhang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #212\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</strong>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 6<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Mehdi Zayene &middot; Albias Havolli &middot; Jannik Endres &middot; Charles Corbi\u00e8re &middot; Alexandre Ben Ahmed Kontouli &middot; Salim Cherkaoui &middot; Alex Alahi\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #79\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <strong>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</strong>\\n                    \\n                    \\n                    Poster Session 3<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Ning Gao &middot; Yilun Chen &middot; Shuai Yang &middot; Xinyi Chen &middot; Yang Tian &middot; Hao Li &middot; Haifeng Huang &middot; Hanqing Wang &middot; Tai Wang &middot; Jiangmiao Pang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #148\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <strong>SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons</strong>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 1<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yuanyou Xu &middot; Zongxin Yang &middot; Yi Yang\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #14\\n                </td>\\n            </tr>\\n        \\n            <tr>\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://yikai-wang.github.io/asuka/\\\" target=\\\"_blank\\\">Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency</a>\\n                    \\n                    \\n                        <img title=\\\"Highlight\\\" height=\\\"25px\\\"\\n                             src=\\\"/static/core/img/award.svg\\\">\\n\\n                    \\n                    Poster Session 5<br>\\n                    <div class=\\\"indented\\\">\\n                        <i>\\n                            \\n                                Yikai Wang &middot; Chenjie Cao &middot; Junqiu Yu &middot; Ke Fan &middot; Xiangyang Xue &middot; Yanwei Fu\\n                            \\n                        </i>\\n                    </div>\\n                </td>\\n\\n                <td>\\n\\n                    \\n                </td>\\n                <td style=\\\"width:20%;\\\">\\n                      ExHall D \\n                    Poster #208\\n                </td>\\n            </tr>\\n        \\n            <tr style=\\\"background-color: #f3f3f3\\\">\\n\\n                <td>\\n                    \\n                        <a href=\\\"https://cybertronagent.github.io/Optimus-2.github.io/\\\" target=\\\"_blank\\\">Optimus-2: Multimodal M ...\n\n(The output of the tool call (shortuuid identifier: AzopUonH7SRSaZpEVfJUEA) is too long! Only the first 100000 characters are shown here. The original output length is 2726295 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/AzopUonH7SRSaZpEVfJUEA.json. Please check this file carefully, as it may be very long!)", "tool_call_id": "toolu_01TKdEiqHsBxn89ALh2SRoK8"}, {"role": "assistant", "content": "Great! I got the HTML content. Now let me parse it to extract all authors and count their publications. Let me write a script to process this data.", "tool_calls": [{"id": "toolu_01NaExgcu9rvHNLNV4QL4fW3", "type": "function", "function": {"name": "filesystem-write_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/parse_cvpr_2025.py\", \"content\": \"import re\\nfrom collections import Counter\\nimport json\\n\\n# Read the HTML content\\nwith open('/workspace/dumps/workspace/.overlong_tool_outputs/AzopUonH7SRSaZpEVfJUEA.json', 'r') as f:\\n    data = json.load(f)\\n    html_content = data['content']\\n\\n# Find all author sections using regex\\n# Pattern to match author sections in the HTML\\npattern = r'<div class=\\\"indented\\\">\\\\s*<i>\\\\s*(.*?)\\\\s*</i>\\\\s*</div>'\\nmatches = re.findall(pattern, html_content, re.DOTALL)\\n\\n# Extract all authors\\nall_authors = []\\nfor match in matches:\\n    # Split by &middot; to get individual authors\\n    authors = match.split('&middot;')\\n    for author in authors:\\n        # Clean up the author name\\n        author = author.strip()\\n        # Remove any HTML tags\\n        author = re.sub(r'<[^>]+>', '', author)\\n        author = author.strip()\\n        if author and len(author) > 0:\\n            all_authors.append(author)\\n\\n# Count publications per author\\nauthor_counts = Counter(all_authors)\\n\\n# Get top authors\\ntop_authors = author_counts.most_common(50)\\n\\nprint(f\\\"Total papers found: {len(matches)}\\\")\\nprint(f\\\"Total unique authors: {len(author_counts)}\\\")\\nprint(\\\"\\\\nTop 50 authors by number of publications at CVPR 2025:\\\")\\nprint(\\\"=\\\" * 60)\\nfor i, (author, count) in enumerate(top_authors, 1):\\n    print(f\\\"{i}. {author}: {count} papers\\\")\\n\\n# Save the results\\nwith open('/workspace/dumps/workspace/cvpr_2025_author_stats.txt', 'w') as f:\\n    f.write(f\\\"Total papers found: {len(matches)}\\\\n\\\")\\n    f.write(f\\\"Total unique authors: {len(author_counts)}\\\\n\\\\n\\\")\\n    f.write(\\\"Top 50 authors by number of publications at CVPR 2025:\\\\n\\\")\\n    f.write(\\\"=\\\" * 60 + \\\"\\\\n\\\")\\n    for i, (author, count) in enumerate(top_authors, 1):\\n        f.write(f\\\"{i}. {author}: {count} papers\\\\n\\\")\\n\\nprint(\\\"\\\\nResults saved to cvpr_2025_author_stats.txt\\\")\\n\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/parse_cvpr_2025.py\",\"annotations\":null}", "tool_call_id": "toolu_01NaExgcu9rvHNLNV4QL4fW3"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01D1JzqSwTUYyWrMHEj2RwYD", "type": "function", "function": {"name": "filesystem-read_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/parse_cvpr_2025.py\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"import re\\nfrom collections import Counter\\nimport json\\n\\n# Read the HTML content\\nwith open('/workspace/dumps/workspace/.overlong_tool_outputs/AzopUonH7SRSaZpEVfJUEA.json', 'r') as f:\\n    data = json.load(f)\\n    html_content = data['content']\\n\\n# Find all author sections using regex\\n# Pattern to match author sections in the HTML\\npattern = r'<div class=\\\"indented\\\">\\\\s*<i>\\\\s*(.*?)\\\\s*</i>\\\\s*</div>'\\nmatches = re.findall(pattern, html_content, re.DOTALL)\\n\\n# Extract all authors\\nall_authors = []\\nfor match in matches:\\n    # Split by &middot; to get individual authors\\n    authors = match.split('&middot;')\\n    for author in authors:\\n        # Clean up the author name\\n        author = author.strip()\\n        # Remove any HTML tags\\n        author = re.sub(r'<[^>]+>', '', author)\\n        author = author.strip()\\n        if author and len(author) > 0:\\n            all_authors.append(author)\\n\\n# Count publications per author\\nauthor_counts = Counter(all_authors)\\n\\n# Get top authors\\ntop_authors = author_counts.most_common(50)\\n\\nprint(f\\\"Total papers found: {len(matches)}\\\")\\nprint(f\\\"Total unique authors: {len(author_counts)}\\\")\\nprint(\\\"\\\\nTop 50 authors by number of publications at CVPR 2025:\\\")\\nprint(\\\"=\\\" * 60)\\nfor i, (author, count) in enumerate(top_authors, 1):\\n    print(f\\\"{i}. {author}: {count} papers\\\")\\n\\n# Save the results\\nwith open('/workspace/dumps/workspace/cvpr_2025_author_stats.txt', 'w') as f:\\n    f.write(f\\\"Total papers found: {len(matches)}\\\\n\\\")\\n    f.write(f\\\"Total unique authors: {len(author_counts)}\\\\n\\\\n\\\")\\n    f.write(\\\"Top 50 authors by number of publications at CVPR 2025:\\\\n\\\")\\n    f.write(\\\"=\\\" * 60 + \\\"\\\\n\\\")\\n    for i, (author, count) in enumerate(top_authors, 1):\\n        f.write(f\\\"{i}. {author}: {count} papers\\\\n\\\")\\n\\nprint(\\\"\\\\nResults saved to cvpr_2025_author_stats.txt\\\")\\n\",\"annotations\":null}", "tool_call_id": "toolu_01D1JzqSwTUYyWrMHEj2RwYD"}, {"role": "assistant", "content": "Let me run this Python script locally by executing it with the browser evaluate function or by using a simpler approach. Let me create a simpler version that I can execute directly.", "tool_calls": [{"id": "toolu_013RjEsPBerAREazGbhXAU5J", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"CVPR 2025 most prolific authors publication statistics\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Most Influential CVPR Papers (2025-09 Version) - Paper Digest\nLink: https://www.paperdigest.org/2025/09/most-influential-cvpr-papers-2025-09-version/\nSnippet: To browse the most productive CVPR authors by year ranked by #papers accepted, here are the most productive CVPR authors grouped by year. This ...\nSitelinks: No sitelinks\n\nTitle: Best Papers at CVPR Reveal New Results with Neural Networks for ...\nLink: https://cvpr.thecvf.com/Conferences/2025/News/Awards_Press\nSnippet: All award-winning papers demonstrate exceptional results that help to advance computer vision, artificial intelligence (AI), and more.\nSitelinks: [{'title': 'VGGT: Visual Geometry...', 'link': 'https://cvpr.thecvf.com/virtual/2025/oral/35294'}, {'title': 'Neural Inverse Rendering from...', 'link': 'https://cvpr.thecvf.com/virtual/2025/oral/35315'}, {'title': 'Oral', 'link': 'https://cvpr.thecvf.com/virtual/2025/oral/35281'}]\n\nTitle: CVPR 2025 Statistics - Paper Copilot\nLink: https://papercopilot.com/statistics/cvpr-statistics/cvpr-2025-statistics/\nSnippet: CVPR 2025 Statistics ; 11532 \u00b7 3.30 \u00b7 4.30 ; 2719 (23.58%) \u00b7 3.00 \u00b7 3.60 ; 2305 (19.99%) \u00b7 3.00 \u00b7 3.60 ; 324 (2.81%) \u00b7 3.60 \u00b7 3.60\nSitelinks: No sitelinks\n\nTitle: Paper Digest: CVPR 2025 Papers & Highlights\nLink: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\nSnippet: If you are interested in browsing papers by author, we have a comprehensive list of ~ 12,000 authors (CVPR-2025). Additionally, you may want to ...\nSitelinks: No sitelinks\n\nTitle: Number of Citations of Papers Awarded at CVPR conferences\nLink: https://drsleep.github.io/technical/Number-of-Citations-of-Papers-Awarded-at-CVPR-Conferences/\nSnippet: The two top groups of the LH prize and the best paper award winners have collectively accrued more than 230K citations (151K and 84K, respectively).\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Best Papers and Best Demos\nLink: https://cvpr.thecvf.com/Conferences/2025/BestPapersDemos\nSnippet: Authors: Anagh Malik, Benjamin Attal, Andrew Xie, Matthew O'Toole, David B. Lindell. Best Paper Honorable Mention: ID: 4119. Paper Name: MegaSaM: Accurate ...\nSitelinks: No sitelinks\n\nTitle: Three of the Hottest Topics in Computer Vision Today - CVPR\nLink: https://cvpr.thecvf.com/Conferences/2025/News/Paper_Trends\nSnippet: This year, CVPR experienced a 13% rise in submissions, leading to a total of 13,008 papers from more than 40,000 unique authors from around the world. ... For ...\nSitelinks: No sitelinks\n\nTitle: CVPR Statistics - Paper Copilot\nLink: https://papercopilot.com/statistics/cvpr-statistics/\nSnippet: CVPR 2025 \u00b7 statistic \u00b7 paperlist, 13008 min: 1.00, max: 5.00 avg: 3.49, std: 0.82, 2878 (22.12%) min: 1.00, max: 4.00 avg: 3.25, std: 0.64 ; CVPR 2024 \u00b7 statistic\nSitelinks: No sitelinks\n\nTitle: CVPR'24 in Numbers - LatticeFlow AI\nLink: https://latticeflow.ai/news/cvpr24-in-numbers\nSnippet: At this year's CVPR, there are 30 authors with 10+ papers, the most prolific of which is prof. Yu Qiao with 26 accepted papers. Naturally, this ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: CVPR 2025 Accepted Papers ; A Unified Framework for Heterogeneous Semi-supervised Learning Poster Session 3. Marzi Heidari \u00b7 Abdullah Alchihabi \u00b7 Hao Yan \u00b7 ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_013RjEsPBerAREazGbhXAU5J"}, {"role": "assistant", "content": "Good! I found that Paper Digest has CVPR 2025 author statistics. Let me check that resource.", "tool_calls": [{"id": "toolu_01VtYYjouafZVarHs3hy5bHf", "type": "function", "function": {"name": "playwright_with_chunk-browser_navigate", "arguments": "{\"url\": \"https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Navigate to https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\\nawait page.goto('https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/');\\n```\\n\\n### New console messages\\n- [LOG] JQMIGRATE: Migrate is installed, version 3.3.2 @ https://www.paperdigest.org/wp-includes/js/jq...\\n\\n### Page state\\n- Page URL: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\\n- Page Title: Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest\\n- Page Snapshot (Span 1 of 539):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - banner [ref=e10]:\\n    - generic [ref=e13]:\\n      - link \\\"header dark logo\\\" [ref=e15] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"header dark logo\\\" [ref=e16] [cursor=pointer]\\n      - link \\\"\uf20e\\\" [ref=e18] [cursor=pointer]:\\n        - /url: \\\"#navigation\\\"\\n        - generic [ref=e19] [cursor=pointer]: \uf20e\\n  - generic [ref=e23]:\\n    - generic [ref=e24]:\\n      - generic [ref=e25]:\\n        - 'heading \\\"Paper Digest: CVPR 2025 Papers & Highlights\\\" [level=1] [ref=e26]'\\n        - generic [ref=e27]:\\n          - generic [ref=e29]:\\n            - generic [ref=e30]: \uf402\\n            - time [ref=e31]: June 7, 2025\\n          - generic [ref=e34]:\\n            - generic [ref=e35]: \uf47e\\n            - link \\\"admin\\\" [ref=e36] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/author/admin/\\n      - generic [ref=e37]:\\n        - paragraph [ref=e38]: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) is one of the top computer vision conferences in the world. To help the community quickly catch up on the work presented in this conference, Paper Digest Team processed all accepted papers, and generated one highlight sentence (typically the main topic) for each paper. Readers are encouraged to read these machine generated highlights to quickly get the main idea of each paper.\\n        - paragraph [ref=e39]:\\n          - text: \\\"Note: CVPR-2025 accepts more than 2,800 papers, this page only includes 500 of them selected by our daily paper digest algorithm. Interested users can choose to read\\\"\\n          - link \\\"All 2,800 CVPR-2025 papers\\\" [ref=e40] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?topic=cvpr&year=2025\\n          - text: in a separate page.\\n        - paragraph [ref=e41]:\\n          - text: To search for papers presented at CVPR-2025 on a specific topic, please make use of the\\n          - link \\\"search by venue (CVPR-2025)\\\" [ref=e42] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=search&topic=cvpr&year=2025\\n          - text: service. To summarize the latest research published at CVPR-2025 on a specific topic, you can utilize the\\n          - link \\\"review by venue (CVPR-2025)\\\" [ref=e43] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=review&topic=cvpr&year=2025\\n          - text: service. If you are interested in browsing papers by author, we have a comprehensive list of\\n          - link \\\"~ 12,000 authors (CVPR-2025)\\\" [ref=e44] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\\n          - text: . Additionally, you may want to explore our\\n          - link \\\"\u201cBest Paper\u201d Digest (CVPR)\\\" [ref=e45] [cursor=pointer]:\\n            - /url: https://resources.paperdigest.org/2025/03/most-influential-cvpr-papers-2025-03-version/\\n          - text: \\\", which lists the most influential CVPR papers since 1988.\\\"\\n        - paragraph [ref=e46]:\\n          - text: We\u2019ve developed a service \u2013\\n          - link \\\"CVPR-2025 Research Report\\\" [ref=e47] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/research/?topic=cvpr&year=2025\\n          - text: that synthesizes the latest findings from CVPR 2025 into comprehensive reports. For instance, we\u2019ve generated sample reports on\\n          - 'link \\\"Advances in 3D from Multi-View and Sensors: Insights from CVPR 2025 Papers\\\" [ref=e48] [cursor=pointer]':\\n            - /url: https://www.paperdigest.org/report/?id=advances-in-3d-from-multi-view-and-sensors-insights-from-cvpr-2025-papers\\n          - text: and\\n          - 'link \\\"Advances in Image and Video Synthesis: Insights from CVPR 2025 Papers\\\" [ref=e49] [cursor=pointer]':\\n            - /url: https://www.paperdigest.org/report/?id=advances-in-image-and-video-synthesis-insights-from-cvpr-2025-papers\\n          - text: . We encourage interested users to utilize our service to create tailored reports on other emerging topics.\\n        - paragraph [ref=e50]:\\n          - text: This curated list is created by the Paper Digest Team. Experience the cutting-edge capabilities of Paper Digest, an innovative AI-powered research platform that gets you the personalized and comprehensive updates on the latest research in your field. It also empowers you to\\n          - link \\\"read articles\\\" [ref=e51] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/reader\\n          - text: \\\",\\\"\\n          - link \\\"write articles\\\" [ref=e52] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/writer\\n          - text: \\\",\\\"\\n          - link \\\"get answers\\\" [ref=e53] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/copilot\\n          - text: \\\",\\\"\\n          - link \\\"conduct literature reviews\\\" [ref=e54] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/review\\n          - text: and\\n          - link \\\"generate research reports\\\" [ref=e55] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/research\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01VtYYjouafZVarHs3hy5bHf"}, {"role": "assistant", "content": "Perfect! I can see there's a link to browse by authors. Let me click on that link.", "tool_calls": [{"id": "toolu_01XwqWBo8KDY8zvy4k2TvkG6", "type": "function", "function": {"name": "playwright_with_chunk-browser_click", "arguments": "{\"element\": \"Link to browse authors list\", \"ref\": \"e44\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Click Link to browse authors list\\nawait page.getByRole('link', { name: '~ 12,000 authors (CVPR-2025)' }).click();\\n```\\n\\n### Open tabs\\n- 0: (current) [Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest] (https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/)\\n- 1: [CVPR-2025 Paper Digest] (https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025)\\n\\n### New console messages\\n- [LOG] JQMIGRATE: Migrate is installed, version 3.3.2 @ https://www.paperdigest.org/wp-includes/js/jq...\\n\\n### Page state\\n- Page URL: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\\n- Page Title: Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest\\n- Page Snapshot (Span 1 of 539):\\n```yaml\\n- generic [ref=e1]:\\n  - banner [ref=e10]:\\n    - generic [ref=e13]:\\n      - link \\\"header dark logo\\\" [ref=e15] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"header dark logo\\\" [ref=e16] [cursor=pointer]\\n      - link \\\"\uf20e\\\" [ref=e18] [cursor=pointer]:\\n        - /url: \\\"#navigation\\\"\\n        - generic [ref=e19] [cursor=pointer]: \uf20e\\n  - generic [ref=e23]:\\n    - generic [ref=e24]:\\n      - generic [ref=e25]:\\n        - 'heading \\\"Paper Digest: CVPR 2025 Papers & Highlights\\\" [level=1] [ref=e26]'\\n        - generic [ref=e27]:\\n          - generic [ref=e29]:\\n            - generic [ref=e30]: \uf402\\n            - time [ref=e31]: June 7, 2025\\n          - generic [ref=e34]:\\n            - generic [ref=e35]: \uf47e\\n            - link \\\"admin\\\" [ref=e36] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/author/admin/\\n      - generic [ref=e37]:\\n        - paragraph [ref=e38]: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) is one of the top computer vision conferences in the world. To help the community quickly catch up on the work presented in this conference, Paper Digest Team processed all accepted papers, and generated one highlight sentence (typically the main topic) for each paper. Readers are encouraged to read these machine generated highlights to quickly get the main idea of each paper.\\n        - paragraph [ref=e39]:\\n          - text: \\\"Note: CVPR-2025 accepts more than 2,800 papers, this page only includes 500 of them selected by our daily paper digest algorithm. Interested users can choose to read\\\"\\n          - link \\\"All 2,800 CVPR-2025 papers\\\" [ref=e40] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?topic=cvpr&year=2025\\n          - text: in a separate page.\\n        - paragraph [ref=e41]:\\n          - text: To search for papers presented at CVPR-2025 on a specific topic, please make use of the\\n          - link \\\"search by venue (CVPR-2025)\\\" [ref=e42] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=search&topic=cvpr&year=2025\\n          - text: service. To summarize the latest research published at CVPR-2025 on a specific topic, you can utilize the\\n          - link \\\"review by venue (CVPR-2025)\\\" [ref=e43] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=review&topic=cvpr&year=2025\\n          - text: service. If you are interested in browsing papers by author, we have a comprehensive list of\\n          - link \\\"~ 12,000 authors (CVPR-2025)\\\" [active] [ref=e44] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\\n          - text: . Additionally, you may want to explore our\\n          - link \\\"\u201cBest Paper\u201d Digest (CVPR)\\\" [ref=e45] [cursor=pointer]:\\n            - /url: https://resources.paperdigest.org/2025/03/most-influential-cvpr-papers-2025-03-version/\\n          - text: \\\", which lists the most influential CVPR papers since 1988.\\\"\\n        - paragraph [ref=e46]:\\n          - text: We\u2019ve developed a service \u2013\\n          - link \\\"CVPR-2025 Research Report\\\" [ref=e47] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/research/?topic=cvpr&year=2025\\n          - text: that synthesizes the latest findings from CVPR 2025 into comprehensive reports. For instance, we\u2019ve generated sample reports on\\n          - 'link \\\"Advances in 3D from Multi-View and Sensors: Insights from CVPR 2025 Papers\\\" [ref=e48] [cursor=pointer]':\\n            - /url: https://www.paperdigest.org/report/?id=advances-in-3d-from-multi-view-and-sensors-insights-from-cvpr-2025-papers\\n          - text: and\\n          - 'link \\\"Advances in Image and Video Synthesis: Insights from CVPR 2025 Papers\\\" [ref=e49] [cursor=pointer]':\\n            - /url: https://www.paperdigest.org/report/?id=advances-in-image-and-video-synthesis-insights-from-cvpr-2025-papers\\n          - text: . We encourage interested users to utilize our service to create tailored reports on other emerging topics.\\n        - paragraph [ref=e50]:\\n          - text: This curated list is created by the Paper Digest Team. Experience the cutting-edge capabilities of Paper Digest, an innovative AI-powered research platform that gets you the personalized and comprehensive updates on the latest research in your field. It also empowers you to\\n          - link \\\"read articles\\\" [ref=e51] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/reader\\n          - text: \\\",\\\"\\n          - link \\\"write articles\\\" [ref=e52] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/writer\\n          - text: \\\",\\\"\\n          - link \\\"get answers\\\" [ref=e53] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/copilot\\n          - text: \\\",\\\"\\n          - link \\\"conduct literature reviews\\\" [ref=e54] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/review\\n          - text: and\\n          - link \\\"generate research reports\\\" [ref=e55] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/research\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01XwqWBo8KDY8zvy4k2TvkG6"}, {"role": "assistant", "content": "It opened a new tab. Let me switch to that tab.", "tool_calls": [{"id": "toolu_01CdS7fJYttgmr8ECsVDM8NM", "type": "function", "function": {"name": "playwright_with_chunk-browser_tab_select", "arguments": "{\"index\": 1}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Open tabs\\n- 0: [Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest] (https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/)\\n- 1: (current) [CVPR-2025 Paper Digest] (https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025)\\n\\n### Page state\\n- Page URL: https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\\n- Page Title: CVPR-2025 Paper Digest\\n- Page Snapshot (Span 1 of 4):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - navigation [ref=e3]:\\n    - generic [ref=e4]:\\n      - link \\\"Paper Digest Paper Digest\\\" [ref=e6] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"Paper Digest\\\" [ref=e7] [cursor=pointer]\\n        - text: Paper Digest\\n      - list [ref=e9]:\\n        - listitem [ref=e10]:\\n          - link \\\"\ue104 Research \uf0d7\\\" [ref=e11] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e12] [cursor=pointer]: \ue104\\n            - text: Research\\n            - generic [ref=e13] [cursor=pointer]: \uf0d7\\n        - listitem [ref=e14]:\\n          - link \\\"\u270f Read & Write \uf0d7\\\" [ref=e15] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e16] [cursor=pointer]: \u270f\\n            - text: Read & Write\\n            - generic [ref=e17] [cursor=pointer]: \uf0d7\\n        - text: \ue003 \uf0d7 \ue123 \uf0d7\\n        - listitem [ref=e18]:\\n          - link \\\"\ue135 Digests \uf0d7\\\" [ref=e19] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e20] [cursor=pointer]: \ue135\\n            - text: Digests\\n            - generic [ref=e21] [cursor=pointer]: \uf0d7\\n        - text: \ue008 \uf0d7\\n        - listitem [ref=e22]:\\n          - link \\\"\ue008 Login\\\" [ref=e23] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/user_login\\n            - generic [ref=e24] [cursor=pointer]: \ue008\\n            - text: Login\\n  - generic [ref=e26]:\\n    - generic [ref=e30]:\\n      - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e31]:\\n        - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e32]:\\n          - generic [ref=e33]: CVPR-2025 Paper Digest\\n      - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e34]:\\n        - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e35]:\\n          - generic [ref=e36]: Search, review & relate the papers presented at CVPR-2025\\n      - textbox \\\"Search journals & conferences on...\\\" [disabled] [ref=e37]: \\\"*.*\\\"\\n      - button \\\"\uf002\\\" [ref=e38] [cursor=pointer]:\\n        - generic [ref=e39] [cursor=pointer]: \uf002\\n      - heading \\\"Browse Search Review Authors \ue022 Featured Papers \u00b7 \u270f Research Report \u00b7 \ue164 Jump to Another Year \u00b7 back to \ue135 Digest Console\\\" [level=6] [ref=e41]:\\n        - generic [ref=e42]:\\n          - generic [ref=e43]:\\n            - radio \\\"Browse\\\" [ref=e44]\\n            - generic \\\"browse all papers from this venue\\\" [ref=e45]: Browse\\n            - radio \\\"Search\\\" [ref=e46]\\n            - generic \\\"search papers presented at this venue\\\" [ref=e47]: Search\\n            - radio \\\"Review\\\" [ref=e48]\\n            - generic \\\"summarize the research around any topic based on the papers presented at this venue\\\" [ref=e49]: Review\\n            - radio \\\"Authors\\\" [checked] [ref=e50]\\n            - generic \\\"browse the top authors of this venue\\\" [ref=e51]: Authors\\n          - text: \uf073 \uf1ab\\n          - generic [ref=e52]:\\n            - link \\\"\ue022 Featured Papers\\\" [ref=e54] [cursor=pointer]:\\n              - /url: javascript:review_top_picks()\\n              - generic [ref=e55] [cursor=pointer]: \ue022\\n              - generic \\\"Review our favorite selections from this venue\\\" [ref=e57] [cursor=pointer]: Featured Papers\\n            - text: \u00b7\\n            - link \\\"\u270f Research Report\\\" [ref=e58] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/research/?topic=cvpr&year=2025\\n              - generic [ref=e59] [cursor=pointer]: \u270f\\n              - generic \\\"Generate research reports from this venue\\\" [ref=e61] [cursor=pointer]: Research Report\\n            - text: \u00b7\\n            - link \\\"\ue164 Jump to Another Year\\\" [ref=e62] [cursor=pointer]:\\n              - /url: javascript:show_specific_year_message()\\n              - generic [ref=e63] [cursor=pointer]: \ue164\\n              - generic \\\"Get the webpage address for a specific year\\\" [ref=e65] [cursor=pointer]: Jump to Another Year\\n            - text: \u00b7 back to\\n            - link \\\"\ue135 Digest Console\\\" [ref=e66] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/digest\\n              - generic [ref=e67] [cursor=pointer]: \ue135\\n              - generic [ref=e69] [cursor=pointer]: Digest Console\\n    - generic [ref=e76]:\\n      - generic [ref=e78]:\\n        - text: Show\\n        - combobox \\\"Show entries\\\" [ref=e79]:\\n          - option \\\"5\\\" [selected]\\n          - option \\\"10\\\"\\n          - option \\\"25\\\"\\n          - option \\\"100\\\"\\n        - text: entries\\n      - generic [ref=e85]:\\n        - text: Filter\\n        - searchbox \\\"Filter\\\" [ref=e86]\\n      - grid [ref=e87]:\\n        - rowgroup [ref=e88]:\\n          - 'row \\\"ID: activate to sort column descending Author: activate to sort column ascending #Papers: activate to sort column ascending #First-author: activate to sort column ascending Date: activate to sort column ascending Details: activate to sort column ascending\\\" [ref=e89]':\\n            - 'gridcell \\\"ID: activate to sort column descending\\\" [ref=e90] [cursor=pointer]':\\n              - generic [ref=e91] [cursor=pointer]: ID\\n            - 'gridcell \\\"Author: activate to sort column ascending\\\" [ref=e92] [cursor=pointer]':\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01CdS7fJYttgmr8ECsVDM8NM"}, {"role": "assistant", "content": "Great! I can see this page has author statistics. Let me navigate to see more of the content to find the top authors.", "tool_calls": [{"id": "toolu_011wk7yTMuiXSgaJzfv9zhq1", "type": "function", "function": {"name": "playwright_with_chunk-browser_snapshot_navigate_to_next_span", "arguments": ""}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\nNavigated to span 2 of 4\\n```yaml\\n              - generic [ref=e93] [cursor=pointer]: Expert (To access\\n              - generic \\\"Anonymous users can see up to 10 results; after log in, all results will be presented.\\\" [ref=e94] [cursor=pointer]: all 12,247 results\\n              - generic [ref=e95] [cursor=pointer]:\\n                - text: \\\", please\\\"\\n                - link \\\"Log in\\\" [ref=e96] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/user_login/?service=expert_search\\n                - text: )\\n            - 'gridcell \\\"#Papers: activate to sort column ascending\\\" [ref=e97] [cursor=pointer]':\\n              - generic [ref=e98] [cursor=pointer]: \\\"#Papers\\\"\\n            - 'gridcell \\\"#First-author: activate to sort column ascending\\\" [ref=e99] [cursor=pointer]':\\n              - generic \\\"# first-author papers\\\" [ref=e100] [cursor=pointer]: \\\"#First-author\\\"\\n            - 'gridcell \\\"Date: activate to sort column ascending\\\" [ref=e101] [cursor=pointer]':\\n              - generic [ref=e102] [cursor=pointer]: Date\\n            - 'gridcell \\\"Details: activate to sort column ascending\\\" [ref=e103] [cursor=pointer]':\\n              - generic [ref=e104] [cursor=pointer]: Details\\n        - rowgroup [ref=e105]:\\n          - row \\\"1 JIAN YANG Summary Similar People More 21 1 2025-06-07 Related Papers\\\" [ref=e106]:\\n            - gridcell \\\"1\\\" [ref=e107]\\n            - gridcell \\\"JIAN YANG Summary Similar People More\\\" [ref=e108]:\\n              - link \\\"JIAN YANG\\\" [ref=e109] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=jian_yang\\n                - generic [ref=e110] [cursor=pointer]: JIAN YANG\\n              - generic [ref=e112]:\\n                - link \\\"Summary\\\" [ref=e113] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/review?q=jian_yang\\n                  - generic [ref=e114] [cursor=pointer]: Summary\\n                - link \\\"Similar People\\\" [ref=e115] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/similar_people/?name=jian_yang\\n                  - generic [ref=e116] [cursor=pointer]: Similar People\\n                - link \\\"More\\\" [ref=e117] [cursor=pointer]:\\n                  - /url: https://www.google.com/search?q=%22JIAN+YANG%22+OR+%22JIAN+YANG+cvpr%22 site%3Alinkedin.com\\n                  - generic [ref=e118] [cursor=pointer]: More\\n            - gridcell \\\"21\\\" [ref=e119]\\n            - gridcell \\\"1\\\" [ref=e120]\\n            - gridcell \\\"2025-06-07\\\" [ref=e121]\\n            - gridcell \\\"Related Papers\\\" [ref=e122]:\\n              - link \\\"Related Papers\\\" [ref=e123] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=JIAN_YANG&from=cvpr&year=2025\\n          - row \\\"2 YUJUN SHEN Summary Similar People More 17 0 2025-06-07 Related Papers\\\" [ref=e124]:\\n            - gridcell \\\"2\\\" [ref=e125]\\n            - gridcell \\\"YUJUN SHEN Summary Similar People More\\\" [ref=e126]:\\n              - link \\\"YUJUN SHEN\\\" [ref=e127] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=yujun_shen\\n                - generic [ref=e128] [cursor=pointer]: YUJUN SHEN\\n              - generic [ref=e130]:\\n                - link \\\"Summary\\\" [ref=e131] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/review?q=yujun_shen\\n                  - generic [ref=e132] [cursor=pointer]: Summary\\n                - link \\\"Similar People\\\" [ref=e133] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/similar_people/?name=yujun_shen\\n                  - generic [ref=e134] [cursor=pointer]: Similar People\\n                - link \\\"More\\\" [ref=e135] [cursor=pointer]:\\n                  - /url: https://www.google.com/search?q=%22YUJUN+SHEN%22+OR+%22YUJUN+SHEN+cvpr%22 site%3Alinkedin.com\\n                  - generic [ref=e136] [cursor=pointer]: More\\n            - gridcell \\\"17\\\" [ref=e137]\\n            - gridcell \\\"0\\\" [ref=e138]\\n            - gridcell \\\"2025-06-07\\\" [ref=e139]\\n            - gridcell \\\"Related Papers\\\" [ref=e140]:\\n              - link \\\"Related Papers\\\" [ref=e141] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=YUJUN_SHEN&from=cvpr&year=2025\\n          - row \\\"3 YANG LIU Summary Similar People More 14 2 2025-06-07 Related Papers\\\" [ref=e142]:\\n            - gridcell \\\"3\\\" [ref=e143]\\n            - gridcell \\\"YANG LIU Summary Similar People More\\\" [ref=e144]:\\n              - link \\\"YANG LIU\\\" [ref=e145] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=yang_liu\\n                - generic [ref=e146] [cursor=pointer]: YANG LIU\\n              - generic [ref=e148]:\\n                - link \\\"Summary\\\" [ref=e149] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/review?q=yang_liu\\n                  - generic [ref=e150] [cursor=pointer]: Summary\\n                - link \\\"Similar People\\\" [ref=e151] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/similar_people/?name=yang_liu\\n                  - generic [ref=e152] [cursor=pointer]: Similar People\\n```\\n\",\"annotations\":null}", "tool_call_id": "toolu_011wk7yTMuiXSgaJzfv9zhq1"}, {"role": "assistant", "content": "Excellent! I can see the top authors now:\n1. JIAN YANG - 21 papers\n2. YUJUN SHEN - 17 papers  \n3. YANG LIU - 14 papers\n\nLet me continue to see more authors to get a better picture.", "tool_calls": [{"id": "toolu_01MoTQFpGoxZj9hw5t9Fuuyb", "type": "function", "function": {"name": "playwright_with_chunk-browser_snapshot_navigate_to_next_span", "arguments": ""}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\nNavigated to span 3 of 4\\n```yaml\\n                - link \\\"More\\\" [ref=e153] [cursor=pointer]:\\n                  - /url: https://www.google.com/search?q=%22YANG+LIU%22+OR+%22YANG+LIU+cvpr%22 site%3Alinkedin.com\\n                  - generic [ref=e154] [cursor=pointer]: More\\n            - gridcell \\\"14\\\" [ref=e155]\\n            - gridcell \\\"2\\\" [ref=e156]\\n            - gridcell \\\"2025-06-07\\\" [ref=e157]\\n            - gridcell \\\"Related Papers\\\" [ref=e158]:\\n              - link \\\"Related Papers\\\" [ref=e159] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=YANG_LIU&from=cvpr&year=2025\\n          - row \\\"4 ZIWEI LIU Summary Similar People More 14 0 2025-06-07 Related Papers\\\" [ref=e160]:\\n            - gridcell \\\"4\\\" [ref=e161]\\n            - gridcell \\\"ZIWEI LIU Summary Similar People More\\\" [ref=e162]:\\n              - link \\\"ZIWEI LIU\\\" [ref=e163] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=ziwei_liu\\n                - generic [ref=e164] [cursor=pointer]: ZIWEI LIU\\n              - generic [ref=e166]:\\n                - link \\\"Summary\\\" [ref=e167] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/review?q=ziwei_liu\\n                  - generic [ref=e168] [cursor=pointer]: Summary\\n                - link \\\"Similar People\\\" [ref=e169] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/similar_people/?name=ziwei_liu\\n                  - generic [ref=e170] [cursor=pointer]: Similar People\\n                - link \\\"More\\\" [ref=e171] [cursor=pointer]:\\n                  - /url: https://www.google.com/search?q=%22ZIWEI+LIU%22+OR+%22ZIWEI+LIU+cvpr%22 site%3Alinkedin.com\\n                  - generic [ref=e172] [cursor=pointer]: More\\n            - gridcell \\\"14\\\" [ref=e173]\\n            - gridcell \\\"0\\\" [ref=e174]\\n            - gridcell \\\"2025-06-07\\\" [ref=e175]\\n            - gridcell \\\"Related Papers\\\" [ref=e176]:\\n              - link \\\"Related Papers\\\" [ref=e177] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=ZIWEI_LIU&from=cvpr&year=2025\\n          - row \\\"5 MARC POLLEFEYS Summary Similar People More 14 0 2025-06-07 Related Papers\\\" [ref=e178]:\\n            - gridcell \\\"5\\\" [ref=e179]\\n            - gridcell \\\"MARC POLLEFEYS Summary Similar People More\\\" [ref=e180]:\\n              - link \\\"MARC POLLEFEYS\\\" [ref=e181] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=marc_pollefeys\\n                - generic [ref=e182] [cursor=pointer]: MARC POLLEFEYS\\n              - generic [ref=e184]:\\n                - link \\\"Summary\\\" [ref=e185] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/review?q=marc_pollefeys\\n                  - generic [ref=e186] [cursor=pointer]: Summary\\n                - link \\\"Similar People\\\" [ref=e187] [cursor=pointer]:\\n                  - /url: https://www.paperdigest.org/similar_people/?name=marc_pollefeys\\n                  - generic [ref=e188] [cursor=pointer]: Similar People\\n                - link \\\"More\\\" [ref=e189] [cursor=pointer]:\\n                  - /url: https://www.google.com/search?q=%22MARC+POLLEFEYS%22+OR+%22MARC+POLLEFEYS+cvpr%22 site%3Alinkedin.com\\n                  - generic [ref=e190] [cursor=pointer]: More\\n            - gridcell \\\"14\\\" [ref=e191]\\n            - gridcell \\\"0\\\" [ref=e192]\\n            - gridcell \\\"2025-06-07\\\" [ref=e193]\\n            - gridcell \\\"Related Papers\\\" [ref=e194]:\\n              - link \\\"Related Papers\\\" [ref=e195] [cursor=pointer]:\\n                - /url: https://www.paperdigest.org/isearch?name=MARC_POLLEFEYS&from=cvpr&year=2025\\n      - status [ref=e196]: Showing 1 to 5 of 50 entries\\n      - generic [ref=e197]:\\n        - generic [ref=e198]: Previous\\n        - generic [ref=e199]:\\n          - generic [ref=e200] [cursor=pointer]: \\\"1\\\"\\n          - generic [ref=e201] [cursor=pointer]: \\\"2\\\"\\n          - generic [ref=e202] [cursor=pointer]: \\\"3\\\"\\n          - generic [ref=e203] [cursor=pointer]: \\\"4\\\"\\n          - generic [ref=e204] [cursor=pointer]: \\\"5\\\"\\n          - generic [ref=e205]: \u2026\\n          - generic [ref=e206] [cursor=pointer]: \\\"10\\\"\\n        - generic [ref=e207] [cursor=pointer]: Next\\n    - button \\\"\ue131 More Results\\\" [ref=e211] [cursor=pointer]:\\n      - generic [ref=e212] [cursor=pointer]: \ue131\\n      - generic [ref=e213] [cursor=pointer]: More Results\\n  - contentinfo [ref=e215]:\\n    - heading \\\"\ue066 Digest \u00b7 \ue022 Read \u00b7 \ue065 Write \u00b7 \ue067 Answer \u00b7 \ue043 Review\\\" [level=5] [ref=e216]:\\n      - link \\\"\ue066 Digest\\\" [ref=e217] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/digest/\\n        - generic \\\"Journal & conference digests\\\" [ref=e218] [cursor=pointer]:\\n          - generic [ref=e219] [cursor=pointer]: \ue066\\n          - text: Digest\\n      - text: \u00b7\\n      - link \\\"\ue022 Read\\\" [ref=e220] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/reader/\\n        - generic \\\"AI academic reader - your AI-powered academic PDF reader\\\" [ref=e221] [cursor=pointer]:\\n          - generic [ref=e222] [cursor=pointer]: \ue022\\n          - text: Read\\n      - text: \u00b7\\n      - link \\\"\ue065 Write\\\" [ref=e223] [cursor=pointer]:\\n```\\n\",\"annotations\":null}", "tool_call_id": "toolu_01MoTQFpGoxZj9hw5t9Fuuyb"}, {"role": "assistant", "content": "Perfect! I can see the top 5 authors:\n1. JIAN YANG - 21 papers\n2. YUJUN SHEN - 17 papers\n3. YANG LIU - 14 papers\n4. ZIWEI LIU - 14 papers\n5. MARC POLLEFEYS - 14 papers\n\nNow I need to research these top authors to see which ones work on visual generative models (diffusion models, flow matching, video generation, cross-modal generation) and are affiliated with Hong Kong universities. Let me search for information about each of them.", "tool_calls": [{"id": "toolu_01BLgiRkqMekje8fNWTPvfoE", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"Jian Yang computer vision CVPR 2025 research interests diffusion models generative models affiliation university\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Generative Models for Computer Vision - GitHub Pages\nLink: https://generative-vision.github.io/workshop-CVPR-25/\nSnippet: 3rd Workshop on Generative Models for Computer Vision. CVPR 2025. 8:45am - 5:00pm, Tuesday, June 11th, 2025 Grand A2, Music City Center, Nashville, Tennessee ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Awards - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/virtual/2025/awards_detail\nSnippet: Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions.\nSitelinks: No sitelinks\n\nTitle: Prometheus - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Prometheus_3D-Aware_Latent_Diffusion_Models_for_Feed-Forward_Text-to-3D_Scene_Generation_CVPR_2025_paper.html\nSnippet: In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds.\nSitelinks: No sitelinks\n\nTitle: [PDF] arXiv:2502.06619v2 [cs.CV] 11 Feb 2025\nLink: https://arxiv.org/pdf/2502.06619?\nSnippet: We investigate the feasibility of leveraging a pre-trained diffusion model as an expert to enhance generalizable feature learning for DG Re-ID ...\nSitelinks: No sitelinks\n\nTitle: Text-Only Diffusion Models with Enhanced Interaction Awareness\nLink: https://cvpr.thecvf.com/virtual/2025/poster/33255\nSnippet: Recent large-scale text-to-image diffusion models generate photorealistic images but often struggle to accurately depict interactions between humans and objects ...\nSitelinks: No sitelinks\n\nTitle: Google at CVPR 2025\nLink: https://research.google/conferences-and-events/google-at-cvpr-2025/\nSnippet: The 2025 meeting of the Computer Vision and Pattern Recognition conference (CVPR 2025) is being held Wednesday, June 11th through Sunday June 15th in Nashville.\nSitelinks: No sitelinks\n\nTitle: [PDF] Comprehensive exploration of diffusion models in image generation\nLink: https://d-nb.info/1365211207/34\nSnippet: Abstract. The rapid development of deep learning technology has led to the emergence of diffusion models as a promising generative model ...\nSitelinks: No sitelinks\n\nTitle: Comprehensive exploration of diffusion models in image generation\nLink: https://link.springer.com/article/10.1007/s10462-025-11110-3\nSnippet: This paper presents a comprehensive survey of the most recent applications of diffusion models in the field of image generation.\nSitelinks: No sitelinks\n\nTitle: Ten papers by CSE researchers at CVPR 2025\nLink: https://cse.engin.umich.edu/stories/ten-papers-by-cse-researchers-at-cvpr-2025\nSnippet: CSE-affiliated authors are presenting new research in the area of computer vision, from humanoid robotics to 3D reconstruction.\nSitelinks: No sitelinks\n\nTitle: [PDF] IEEE/CVF Conference on Computer Vision and Pattern Recognition\nLink: https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_Workshops_Tutorials_2025.pdf\nSnippet: Summary: This year, our Visual Generative Modeling workshop at. CVPR aims to explore what lies beyond diffusion models in visual gen- erative ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01BLgiRkqMekje8fNWTPvfoE"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01FSmHtBUCroPYwDxurUccQ2", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Jian Yang\\\" computer vision professor affiliation university homepage\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Jian Yang PhD Professor (Full) at Beijing Institute of ...\nLink: https://www.researchgate.net/profile/Jian-Yang-121\nSnippet: Jian Yang. 3D motion tracking in X-ray image-guided operations using pre- and intra-operative image registration has recently gained attention.\nSitelinks: No sitelinks\n\nTitle: Jian Yang\nLink: https://researchers.mq.edu.au/en/persons/jian-yang\nSnippet: Prof Jian Yang is a Co-Director of Data Horizons research Centre and Deputy Director of Applied Artificial Intelligence Research Centre.\nSitelinks: No sitelinks\n\nTitle: Jian Yang PhD Professor at University of Science and ...\nLink: https://www.researchgate.net/profile/Jian-Yang-73\nSnippet: Jian YANG, Professor | Cited by 2020 | of University of Science and Technology of China, Hefei (USTC) | Read 178 publications | Contact Jian YANG.\nSitelinks: No sitelinks\n\nTitle: Jian Yang - Home\nLink: https://dl.acm.org/profile/81361591444\nSnippet: ... Computer Science and Engineering, Nanjing University of Science and Technology, China. ,; Author Picture Jian Yang. PCA Lab, Key Lab of Intelligent Perception ...\nSitelinks: No sitelinks\n\nTitle: Jian Yang - Home - ACM Digital Library\nLink: https://dl.acm.org/profile/99660532675\nSnippet: PCA Lab, VCIP, College of Computer Science, Nankai University. ,; Author Picture Jian Yang. PCA Lab, VCIP, College of Computer Science, Nankai University. , ...\nSitelinks: No sitelinks\n\nTitle: Jian Yang - Professor, Co-Director of Data Horizons ...\nLink: https://au.linkedin.com/in/jian-yang-1727945\nSnippet: Dr. Jian Yang has over 20 years experience in data and process analytics in academic and industry. Currently she is a full professor at Department of Computing ...\nSitelinks: No sitelinks\n\nTitle: Two-dimensional discriminant transform for face recognition\nLink: https://www.sciencedirect.com/science/article/abs/pii/S0031320305000026\nSnippet: by J Yang \u00b7 2005 \u00b7 Cited by 393 \u2014 About the Author\u2014JIAN YANG was born in Jiangsu, China, June 1973. He obtained his Bachelor of Science in Mathematics at the Xuzhou Normal University in 1995.\nSitelinks: No sitelinks\n\nTitle: A new kernel Fisher discriminant algorithm with application ...\nLink: https://www.sciencedirect.com/science/article/abs/pii/S0925231203004442\nSnippet: by J Yang \u00b7 2004 \u00b7 Cited by 58 \u2014 Jian Yang was born in Jiangsu, China, on 3rd June 1973. He obtained his Bachelor of Science in Mathematics at the Xuzhou Normal University in China in 1995.\nSitelinks: No sitelinks\n\nTitle: Affiliate Faculty Program - Columbia | Zuckerman Institute\nLink: https://zuckermaninstitute.columbia.edu/affiliates\nSnippet: Professor of Economics. Jian Yang, PhD. Professor of Biological Sciences. Lori Zeltser, PhD. Associate Professor of Pathology & Cell Biology. Sandra Soo-Jin Lee.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01FSmHtBUCroPYwDxurUccQ2"}, {"role": "assistant", "content": "Let me search more specifically for Jian Yang at Nanjing University of Science and Technology (NJUST), which seems to be a common affiliation.", "tool_calls": [{"id": "toolu_01DHrQxtWj4vrmHyFVBxa1EV", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Jian Yang\\\" Nanjing University Science Technology NJUST computer vision homepage\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Jian Yang - Pattern Computing and Application Laboratory\nLink: http://www.patternrecognition.asia/jian/\nSnippet: Curriculum Vitae. Of. Jian Yang. Contact Information. School of Computer Science and Technology,. Nanjing University of Science and Technology,.\nSitelinks: No sitelinks\n\nTitle: Jian Yang - Google \u5b66\u672f\u641c\u7d22\nLink: https://scholar.google.com/citations?user=6CIDtZQAAAAJ&hl=zh-CN\nSnippet: Jian Yang. Prof. of Computer Science, Nanjing University of Science and Technology. \u5728 njust.edu.cn \u7684\u7535\u5b50\u90ae\u4ef6\u7ecf\u8fc7\u9a8c\u8bc1 - \u9996\u9875 \u00b7 Pattern Recognition ...\nSitelinks: No sitelinks\n\nTitle: Jian Yang | IEEE Xplore Author Details\nLink: https://ieeexplore.ieee.org/author/37280205100\nSnippet: Jian Yang received the Ph.D. degree in pattern recognition and intelligence systems from Nanjing University of Science and Technology (NJUST), Nanjing, China, ...\nSitelinks: No sitelinks\n\nTitle: Jian YANG | Nanjing University of Science and Technology\nLink: https://www.researchgate.net/profile/Jian-Yang-5\nSnippet: Jian YANG | Cited by 10618 | of Nanjing University of Science and Technology, Nanjing (NJUST) | Read 369 publications | Contact Jian YANG.\nSitelinks: No sitelinks\n\nTitle: TtfDiffusion: Training-free and text-free image editing in ...\nLink: https://www.sciencedirect.com/science/article/abs/pii/S0925231224019301\nSnippet: by Z Yu \u00b7 2025 \u00b7 Cited by 1 \u2014 His research interests include computer vision and machine learning. Jian Yang received the Ph.D. degree from the Nanjing University of Science ...\nSitelinks: No sitelinks\n\nTitle: Jianjun Qian\nLink: https://openreview.net/profile?id=~Jianjun_Qian2\nSnippet: Wenxuan Fang, Ying Tai, Jianjun Qian, Jian Yang, Jun Li. 24 Sept 2024 (modified: 03 Mar 2025); ICLR 2025 Conference Withdrawn Submission; Readers: Everyone ...\nSitelinks: No sitelinks\n\nTitle: Zhiqiang Yan\nLink: https://yanzq95.github.io/\nSnippet: Before that, I obtained my PhD degree at PCALab, Nanjing University of Science and Technology (NJUST) in June 2024, advised by Prof. Jian Yang and co-advised by ...\nSitelinks: No sitelinks\n\nTitle: Jianjun QIAN | PhD | Nanjing University of Science and ...\nLink: https://www.researchgate.net/profile/Jianjun-Qian-2\nSnippet: As we know, nuclear norm based regularization methods have the real-world applications in pattern recognition and computer vision. ... Jian Yang; [...] Linlin ...\nSitelinks: No sitelinks\n\nTitle: Few-shot open-set recognition via pairwise discriminant ...\nLink: https://www.sciencedirect.com/science/article/abs/pii/S0925231224009858\nSnippet: by J Jin \u00b7 2024 \u00b7 Cited by 2 \u2014 His research interests include computer vision and machine learning. Jian Yang received the Ph.D. degree from the Nanjing University of Science ...\nSitelinks: No sitelinks\n\nTitle: Jun Li's Homepage\nLink: https://sites.google.com/view/junlineu/\nSnippet: ... Jian Yang, Creative Birds: Self-Supervised Single-View 3D Style Transfer ... School of Computer Science and Engineering, Nanjing University of Science and ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01DHrQxtWj4vrmHyFVBxa1EV"}, {"role": "assistant", "content": "Good! Jian Yang is from Nanjing University of Science and Technology in mainland China, not Hong Kong. Let me now search for the other top authors.", "tool_calls": [{"id": "toolu_0175JaYC35aTZQ1oRM5m4nG7", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Yujun Shen\\\" computer vision diffusion models generative models affiliation university homepage\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Interspatial Attention for Efficient 4D Human Video ...\nLink: https://dl.acm.org/doi/10.1145/3731165\nSnippet: Stanford University, Stanford, USA. https://orcid.org/0000-0003-2696-9664 \u00b7 View Profile. , Yujun Shen. Yujun Shen. Alibaba Group, Hangzhou, ...\nSitelinks: No sitelinks\n\nTitle: Top CVPR 2024 Accepted Papers\nLink: https://encord.com/blog/cvpr-2024/\nSnippet: ... Yujun Shen, Xiaowei Zhou. Affiliations: Zhejiang University, UC ... Diffusion Model Integration: The system integrates diffusion models into the ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: Rectified Diffusion Guidance for Conditional Generation Poster Session 3. Mengfei Xia \u00b7 Nan Xue \u00b7 Yujun Shen \u00b7 Ran Yi \u00b7 Tieliang Gong \u00b7 Yong-Jin Liu. ExHall D ...\nSitelinks: No sitelinks\n\nTitle: Open-set Bias Detection in Text-to-Image Generative Models\nLink: https://arxiv.org/html/2404.07990v1\nSnippet: [2021] \u2191 Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the fairness of deep generative models without retraining. arXiv preprint, 2021 ...\nSitelinks: No sitelinks\n\nTitle: TEXGen: a Generative Diffusion Model for Mesh Textures\nLink: https://dl.acm.org/doi/10.1145/3687909\nSnippet: ... Yujun Shen, and Gordon Wetzstein. 2024. GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation. CoRR abs ...\nSitelinks: No sitelinks\n\nTitle: Ziyu Wan\nLink: http://raywzy.com/\nSnippet: VLM4D: Towards Spatiotemporal Awareness in Vision Language Models ... Yujun Shen, Yibing Song, Qifeng Chen CVPR 2025. webpage paper bib code demo.\nSitelinks: No sitelinks\n\nTitle: Interpreting the Latent Space of GANs for Semantic Face ...\nLink: https://www.researchgate.net/publication/343454275_Interpreting_the_Latent_Space_of_GANs_for_Semantic_Face_Editing\nSnippet: Yujun Shen at Anhui Medical University \u00b7 Yujun Shen \u00b7 Anhui Medical University ... Generative diffusion models have advanced image editing by delivering ...\nSitelinks: No sitelinks\n\nTitle: EvalAlign: Supervised Fine-Tuning Multimodal LLMs with ...\nLink: https://arxiv.org/html/2406.16562v1\nSnippet: [18] \u2191 Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2024 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers\nSnippet: Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner Poster Session 2 & Exhibit Hall. Mengfei Xia \u00b7 Yujun Shen \u00b7 Changsong Lei \u00b7 Yu Zhou ...\nSitelinks: No sitelinks\n\nTitle: Chaochao Chen\nLink: https://sites.google.com/site/ccchomepage/\nSnippet: ... Yujun Shen, Deli Zhao. UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training. NeurIPS ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_0175JaYC35aTZQ1oRM5m4nG7"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01RP4nwPquYmuXMMvjQU8UPc", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Yujun Shen\\\" homepage researcher profile\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Yujun Shen | IEEE Xplore Author Details\nLink: https://ieeexplore.ieee.org/author/37086579673\nSnippet: Yujun Shen received the BS degree from Tsinghua University and the PhD degree from the Chinese University of Hong Kong. He is a senior researcher with Ant ...\nSitelinks: No sitelinks\n\nTitle: Yujun SHEN | The University of Hong Kong\nLink: https://www.researchgate.net/profile/Yujun-Shen-8\nSnippet: Yujun SHEN | Cited by 9 | of The University of Hong Kong, Hong Kong (HKU) | Read 6 publications | Contact Yujun SHEN.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen - Home\nLink: https://dl.acm.org/profile/99661081477\nSnippet: Author's Profile. Author's Profile \u00b7 Publications \u00b7 Colleagues; More. Yujun Shen. Search within Yujun Shen's work. SearchSearch. Home \u00b7 Yujun Shen. Yujun Shen.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen Professor Tianjin Medical University\nLink: https://www.researchgate.net/profile/Yujun-Shen-3\nSnippet: Yujun SHEN | Cited by 1258 | of Tianjin Medical University, Tianjin (TIJMU) | Read 47 publications | Contact Yujun SHEN.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen | The Chinese University of Hong Kong\nLink: https://scispace.com/authors/yujun-shen-54m5w144kz?papers_page=14\nSnippet: Yujun Shen is an academic researcher from The Chinese University of Hong Kong. The author has contributed to research in topics: Computer science & Feature ...\nSitelinks: No sitelinks\n\nTitle: Yujun Shen - Home\nLink: https://dl.acm.org/do/10.1145/contrib-99661551628/full/\nSnippet: Author's Profile. Author's Profile \u00b7 Publications \u00b7 Colleagues; More. Yujun Shen. Search within Yujun Shen's work. SearchSearch. Home \u00b7 Yujun Shen. Yujun Shen.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen\nLink: https://sciprofiles.com/profile/author/M00zQ0xxK1VQYnpQZnZzSm5LVkxNS2pmQXkzSzdrczNXeGE0UEdRRW9SST0=?utm_source=mdpi.com&utm_medium=website&utm_campaign=avatar_name\nSnippet: Yujun Shen's profile page generated by SciProfiles. SciProfiles provides open access to the scholarly community, offering comprehensive profiles where ...\nSitelinks: No sitelinks\n\nTitle: 0000-0002-9266-9064 - Yujun Shen\nLink: https://orcid.org/0000-0002-9266-9064\nSnippet: Yujun Shen \u00b7 0000-0002-9266-9064 \u00b7 Personal information \u00b7 Activities \u00b7 ORCID Cookie Consent \u00b7 ORCID Cookie Settings.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen\nLink: https://www.sciencedirect.com/author/56146163300/yujun-shen\nSnippet: Author information \u00b7 Research interests \u00b7 Latest papers \u00b7 Yujun Shen published in 13 ScienceDirect journals \u00b7 Yujun Shen authored 16 articles on ScienceDirect.\nSitelinks: No sitelinks\n\nTitle: Yujun Shen - EY (HK) '25 | LSE '25 | UoS '24\nLink: https://uk.linkedin.com/in/yujun-shen-574075233\nSnippet: Yujun Shen \u00b7 1. Market Research: Executed in-depth real estate market research through 200 interviews and data analysis, generating targeted trend insights; \u00b7 2.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01RP4nwPquYmuXMMvjQU8UPc"}, {"role": "assistant", "content": "Excellent! I found that Yujun Shen got his PhD from Chinese University of Hong Kong (CUHK). Let me get more details about his current affiliation and research.", "tool_calls": [{"id": "toolu_01U5AV853Y5RsXoh2jPFBJDB", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Yujun Shen\\\" Ant Group generative models diffusion homepage google scholar\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Yujun Shen - Google \u5b66\u672f\u641c\u7d22\nLink: https://scholar.google.com/citations?user=u76xfogAAAAJ&hl=zh-CN\nSnippet: Yujun Shen. Ant Group. \u5728 link.cuhk.edu.hk \u7684\u7535\u5b50\u90ae\u4ef6\u7ecf\u8fc7\u9a8c\u8bc1 - \u9996\u9875 \u00b7 Generative ModelingComputer VisionDeep Learning. \u6587\u7ae0\u5f15\u7528\u6b21\u6570\u5f00\u653e\u83b7\u53d6\u7684\u51fa\u7248\u7269 ...\nSitelinks: No sitelinks\n\nTitle: Yujun Shen (Damon)\nLink: https://shenyujun.github.io/\nSnippet: Yujun Shen (Damon). I currently serve as the Chief Scientist at Robbyant. I also lead the Interactive Intelligence Lab at Ant Research, with research ...\nSitelinks: No sitelinks\n\nTitle: Hangjie Yuan\nLink: https://scholar.google.com/citations?user=jQ3bFDMAAAAJ&hl=en\nSnippet: Yujun ShenAnt GroupVerified email at link.cuhk.edu.hk.\nSitelinks: No sitelinks\n\nTitle: Yanhong Zeng\nLink: https://zengyh1900.github.io/\nSnippet: Yanhong Zeng is currently a researcher working on Generative AI in Ant Group, working with Yujun Shen. Before that, she led a small effort in advancing image/ ...\nSitelinks: No sitelinks\n\nTitle: Wen Wang encounter1997\nLink: https://github.com/encounter1997\nSnippet: Now I am a research intern at the vision group of the Interactive Intelligence Lab at Ant Group, led by Yujun Shen. ... [Google Scholar] [Homepage]. Pinned ...\nSitelinks: No sitelinks\n\nTitle: Exploring Guided Sampling of Conditional GANs\nLink: https://dl.acm.org/doi/abs/10.1007/978-3-031-73347-5_3\nSnippet: by Y Zhang \u00b7 2024 \u00b7 Cited by 2 \u2014 Yujun Shen. Yujun Shen. Ant Group, Hangzhou, China. https://orcid.org ... diffusion-based generative models. In: NeurIPS (2022). Google ...\nSitelinks: No sitelinks\n\nTitle: GRM: Large Gaussian Reconstruction Model for Efficient ...\nLink: https://link.springer.com/chapter/10.1007/978-3-031-72633-0_1\nSnippet: Recently, Diffusion Models (DM) have emerged as a more powerful generative model ... Yujun Shen. Authors. Yinghao Xu. View author publications.\nSitelinks: No sitelinks\n\nTitle: HeadArtist: Text-conditioned 3D Head Generation with Self ...\nLink: https://dl.acm.org/doi/abs/10.1145/3641519.3657512\nSnippet: by H Liu \u00b7 2024 \u00b7 Cited by 18 \u2014 Yujun Shen. Yujun Shen. Ant Group, China. https://orcid.org/0000-0003 ... Dreamavatar: Text-and-shape guided 3d human avatar generation via ...\nSitelinks: No sitelinks\n\nTitle: Yanbo Fan\nLink: https://openreview.net/profile?id=~Yanbo_Fan1\nSnippet: Personal Links. Homepage \u00b7 Google Scholar \u00b7 DBLP ... Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao.\nSitelinks: No sitelinks\n\nTitle: Mengfei Xia\nLink: https://scholar.google.co.uk/citations?user=jmOlxQ0AAAAJ&hl=fil\nSnippet: ... Generative Modeling ... Yujun ShenAnt GroupNa-verify na email sa link.cuhk.edu.hk. Nan XueAnt GroupNa-verify na email sa antgroup.com.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01U5AV853Y5RsXoh2jPFBJDB"}, {"role": "assistant", "content": "Perfect! Yujun Shen is currently at Ant Group and works on Generative Modeling. His research interests align well with the user's requirements. However, he's not currently at a Hong Kong university (he's at Ant Group, a company). Let me check the other top authors.", "tool_calls": [{"id": "toolu_018oCmJyLD1u9mFMab3k4fBg", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Ziwei Liu\\\" computer vision generative models diffusion university affiliation homepage\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Ziwei Liu | IEEE Xplore Author Details\nLink: https://ieeexplore.ieee.org/author/37085708953\nSnippet: Ziwei Liu (Member, IEEE) is currently an associate professor with Nanyang Technological University, Singapore. His research revolves around computer vision, ...\nSitelinks: No sitelinks\n\nTitle: Ziwei Liu - Home\nLink: https://dl.acm.org/profile/99660590521\nSnippet: Ziwei Liu. S-Lab, Nanyang Technological University, 50 Nanyang Ave, 639798, Singapore. November 2025Pattern Recognition, Volume 167, Issue Chttps://doi.org ...\nSitelinks: No sitelinks\n\nTitle: Fangzhou Hong - Homepage\nLink: https://hongfz16.github.io/\nSnippet: Ziwei Liu (2021-2025). Previously, he received B.Eng. degree in Software Engineering from Tsinghua University (2016-2020). He was fortunate to have an ...\nSitelinks: No sitelinks\n\nTitle: Ziwei Liu's research works | Nanyang Normal University ...\nLink: https://www.researchgate.net/scientific-contributions/Ziwei-Liu-2138084646\nSnippet: Ziwei Liu's 126 research works with 12831 citations, including: Stencil: Subject-Driven Generation with Context Guidance.\nSitelinks: No sitelinks\n\nTitle: Collaborative Diffusion - Ziqi Huang\nLink: https://ziqihuangg.github.io/projects/collaborative-diffusion.html\nSnippet: Multi-Modal Face Generation and Editing. CVPR 2023. Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu\u2020. S-Lab, Nanyang Technological University. Paper Video ...\nSitelinks: No sitelinks\n\nTitle: Collaborative Diffusion for Multi-Modal Face Generation ...\nLink: https://arxiv.org/abs/2304.10530\nSnippet: by Z Huang \u00b7 2023 \u00b7 Cited by 180 \u2014 Title:Collaborative Diffusion for Multi-Modal Face Generation and Editing. Authors:Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu.\nSitelinks: No sitelinks\n\nTitle: A Comprehensive Survey on 3D Content Generation\nLink: https://arxiv.org/html/2402.01166v1\nSnippet: ziwei.liu@ntu.edu.sgm, huangxiaoshui@pjlab.org.cn, csxm@hit.edu.cn ... Diffusion models refer to a class of generative techniques based on the ...\nSitelinks: No sitelinks\n\nTitle: MosaicFusion: Diffusion Models as Data Augmenters for ...\nLink: https://dl.acm.org/doi/abs/10.1007/s11263-024-02223-3\nSnippet: by J Xie \u00b7 2025 \u00b7 Cited by 45 \u2014 Ziwei Liu. Ziwei Liu. S-Lab, Nanyang Technological University, Singapore ... Computer vision \u00b7 Computer vision problems \u00b7 Image segmentation \u00b7 Video ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Diffusion-based Degradation Synthesis for Generalizable All-In-One Image ... Ziwei Liu \u00b7 Qing Wang. ExHall D Poster #38. TokenHSI: Unified Synthesis of ...\nSitelinks: No sitelinks\n\nTitle: Diffusion-augmented learning for long-tail recognition\nLink: https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13486/134860C/Diffusion-augmented-learning-for-long-tail-recognition/10.1117/12.3055784.full\nSnippet: ... Ziwei Liu, and Stella X. Yu, \u201cLong-tailed recognition by routing diverse distribution-aware experts,\u201d in In 9th International Conference on ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_018oCmJyLD1u9mFMab3k4fBg"}, {"role": "assistant", "content": "Great! Ziwei Liu is at Nanyang Technological University (NTU) in Singapore, not Hong Kong. His research includes diffusion models and generative models. Let me check the other top authors.", "tool_calls": [{"id": "toolu_0144sufMY4qW99omSC1YMpBo", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Yang Liu\\\" computer vision CVPR 2025 generative models university affiliation\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Yang Liu's Homepage at Tsinghua University\nLink: https://nlp.csai.tsinghua.edu.cn/~ly/pub.html\nSnippet: Yang Liu. GDS Professor Department of Computer Science and Technology \u00b7 Institute for AI Industry Research \u00b7 Tsinghua University. Publications. 2025. Ziyue Wang ...\nSitelinks: No sitelinks\n\nTitle: Yang Liu's Homepage at Tsinghua University\nLink: https://nlp.csai.tsinghua.edu.cn/~ly/\nSnippet: Yang Liu is the GDS Professor in the Department of Computer Science and Technology at Tsinghua University. He is Executive Dean of Institute for AI Industry ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Yang Liu \u00b7 Lei Zhou \u00b7 Cheng Yan \u00b7 Heng Wang \u00b7 Xia Xie. ExHall D Poster #329. MultiMorph: On-demand Atlas Construction Poster Session 6. Mazdak Abulnaga ...\nSitelinks: No sitelinks\n\nTitle: NTIRE 2025 XGC Quality Assessment Challenge: Methods ...\nLink: https://arxiv.org/html/2506.02875v1\nSnippet: In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2025. ... Yang Liu, Tieyong Zeng, Raymond ...\nSitelinks: No sitelinks\n\nTitle: Yang Liu (disambiguation)\nLink: https://dblp.org/pid/51/3710\nSnippet: Ritsumeikan University, Department of Electronic and Computer Engineering, Kusatsu, Japan \u00b7 Yang Liu 0002 \u2014 University of Texas-Pan American, ...\nSitelinks: No sitelinks\n\nTitle: IEEE/CVF Conference on Computer Vision and Pattern ...\nLink: https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_Workshops_Tutorials_2025.pdf\nSnippet: Computer Vision. Organizers: Chen Chen, Guangyu Sun,. Nathalie Baracaldo, Yang Liu,. Peter Richt\u00c3\u00a1rik, Mi Zhang,. Lingjuan Lyu,. Nicholas Donald ...\nSitelinks: No sitelinks\n\nTitle: Computer Vision and Pattern Recognition Mar 2025\nLink: https://www.arxiv.org/list/cs.CV/2025-03?skip=50&show=2000\nSnippet: Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin. Comments: Accepted by CVPR 2025. Subjects: Computer Vision and Pattern ...\nSitelinks: No sitelinks\n\nTitle: Dr. Xiao-Yang Liu Yanglet |\nLink: https://openfin.engineering.columbia.edu/people/dr-xiao-yang-liu-yanglet\nSnippet: Dr. Xiao-Yang Liu Yanglet, PhD, FU FOUNDATN SCHL OF ENGINEERING & APPLIED SCIENCE:GRAD; Faculty, Rensselaer Polytechnic Institute, Computer Science.\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Awards - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/virtual/2025/awards_detail\nSnippet: Vision-language generative reward models (VL-GenRMs) play a crucial ... weixing chen \u00b7 Yang Liu \u00b7 Binglin Chen \u00b7 Jiandong Su \u00b7 Yongsen Zheng \u00b7 Liang Lin.\nSitelinks: No sitelinks\n\nTitle: Paper Digest: CVPR 2025 Papers & Highlights\nLink: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\nSnippet: Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion ... Silin Cheng; Yang Liu; Xinwei He; ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_0144sufMY4qW99omSC1YMpBo"}, {"role": "assistant", "content": "Yang Liu seems to be at Tsinghua University in Beijing, China. Let me check Marc Pollefeys.", "tool_calls": [{"id": "toolu_01PT2eZaUTfPapuN32b5dZno", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Marc Pollefeys\\\" computer vision university affiliation homepage\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Our Team\nLink: https://cvg.ethz.ch/our_team\nSnippet: Prof. Dr. Marc Pollefeys, Department of Computer Science, Computer Vision and Geometry Group, CNB G 105, Universit\u00e4tstrasse 6, CH-8092 Z\u00fcrich\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys | IEEE Xplore Author Details\nLink: https://ieeexplore.ieee.org/author/37271138500\nSnippet: Marc Pollefeys (Fellow, IEEE) received the MSc and the PhD from the KU Leuven in Belgium, in 1994 and 1999, respectively. He is a professor of computer science ...\nSitelinks: No sitelinks\n\nTitle: Professors \u2013 Department of Computer Science\nLink: https://inf.ethz.ch/people/faculty/faculty.html\nSnippet: Marc Pollefeys. Marc Pollefeys Full Professor. More details computer vision, 3D modeling, robotic perception, computer graphics, machine learning. Prof ...\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys - Scholar Nexus\nLink: https://scholarnexus.ai/supervisor/Marc_Pollefeys?id=4dfc929d-f93e-4cf9-85d9-07b72e06180f\nSnippet: Marc Pollefeys is a Full Professor at the Department of Computer Science, ETH Zurich, and Director of the Microsoft Mixed Reality and AI Lab ...\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys - Home\nLink: https://dl.acm.org/profile/81100638521\nSnippet: Marc Pollefeys \u00b7 Most frequent co-Author \u00b7 Most cited colleague \u00b7 Most frequent Affiliation.\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys\nLink: https://history.siggraph.org/person/marc-pollefeys/\nSnippet: Marc Pollefeys is a post-doctoral researcher at the Center for Processing of Speech and Images of the K.U.Leuven. His PhD dissertation on \u201cSelf-calibration and ...\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys - Swiss Federal Institute of Technology (ETH)\nLink: https://ethz.academia.edu/MarcPollefeys\nSnippet: Marc Pollefeys is a full professor in the Dept. of Computer Science of ETH Zurich since 2007 where he is the head of the Institute for Visual Computing.\nSitelinks: No sitelinks\n\nTitle: Marc Pollefeys PhD Professor (Full) at ETH Zurich\nLink: https://www.researchgate.net/profile/Marc-Pollefeys\nSnippet: Marc POLLEFEYS, Professor (Full) | Cited by 60189 | of ETH Zurich, Z\u00fcrich (ETH Z\u00fcrich) | Read 875 publications | Contact Marc POLLEFEYS.\nSitelinks: No sitelinks\n\nTitle: Xi Wang - GitHub Pages\nLink: https://xiwang1212.github.io/homepage/\nSnippet: I am an established researcher in the Computer Vision and Geometry Lab with Prof. Marc Pollefeys at ETH Zurich while continue collaborating with Prof. Luc ...\nSitelinks: No sitelinks\n\nTitle: Guest Editors' Introduction to the Special Section on Award ...\nLink: https://www.computer.org/csdl/journal/tp/2011/12/ttp2011122339/13rRUx0gegt\nSnippet: by M Pollefeys \u00b7 2011 \u00b7 Cited by 5 \u2014 Marc Pollefeys received the PhD degree from the Katholieke Universiteit Leuven in 1999. He has been a full professor in the Department of Computer Science of ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01PT2eZaUTfPapuN32b5dZno"}, {"role": "assistant", "content": "Marc Pollefeys is at ETH Zurich in Switzerland, not Hong Kong. Let me now look at more authors from the list and specifically search for researchers in Hong Kong universities working on generative models. Let me change my approach and search directly for Hong Kong-based researchers working on diffusion models and generative AI.", "tool_calls": [{"id": "toolu_01TKp3YPzrZ7Sq35MXnnhHEZ", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"Hong Kong university professor diffusion models video generation CVPR 2025 generative AI\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Visual Generative Modeling: What's After Diffusion? | CVPR 2025 ...\nLink: https://vgm-cvpr.github.io/\nSnippet: Our Visual Generative Modeling workshop at CVPR aims to explore what lies beyond diffusion models in visual generative modeling.\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Diffusion-based Degradation Synthesis for Generalizable All-In-One Image Restoration Poster Session 6. Sudarshan Rajagopalan \u00b7 Nithin Gopalakrishnan Nair ...\nSitelinks: No sitelinks\n\nTitle: Jiwen Yu - Google Scholar\nLink: https://scholar.google.com/citations?user=uoRPLHIAAAAJ&hl=zh-CN\nSnippet: University of Hong Kong - \u202a\u202a\u5f15\u7528\u6b21\u6570\uff1a1326 \u6b21\u202c\u202c - \u202aVideo Generation Model\u202c - \u202aWorld Model\u202c - \u202aEmbodied AI\u202c ... Animatezero: Video diffusion models are zero ...\nSitelinks: No sitelinks\n\nTitle: CVPR Tutorial Diffusion-based Video Generative Models\nLink: https://cvpr.thecvf.com/virtual/2024/tutorial/23730\nSnippet: This tutorial covers diffusion models in video generation, including applications like image-to-video, video editing, and motion customization, and evaluation ...\nSitelinks: No sitelinks\n\nTitle: Video diffusion generation: comprehensive review and open problems\nLink: https://link.springer.com/article/10.1007/s10462-025-11331-6\nSnippet: Diffusion models, introduced in recent years as a type of generative model, progressively evolve a simple data distribution into a complex one ...\nSitelinks: No sitelinks\n\nTitle: Generative Models for Computer Vision - GitHub Pages\nLink: https://generative-vision.github.io/workshop-CVPR-25/\nSnippet: In this workshop, we aim to bring together researchers from the fields of image synthesis and computer vision to facilitate discussions and progress.\nSitelinks: No sitelinks\n\nTitle: Kenneth - Home - Department of Computer Science, HKU\nLink: https://www.cs.hku.hk/~kykwong\nSnippet: Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation. ... Generation via Diffusion Models. Yukang ...\nSitelinks: No sitelinks\n\nTitle: Kai Han\nLink: https://www.kaihan.org/\nSnippet: Kai Han is an Assistant Professor at The University of Hong Kong, directing the Visual AI Lab. His research focuses on computer vision, machine learning, and ...\nSitelinks: No sitelinks\n\nTitle: \u202aJinbo Xing\u202c - \u202aGoogle Scholar\u202c\nLink: https://scholar.google.com/citations?user=ZB0gLU0AAAAJ&hl=en\nSnippet: Videocrafter1: Open diffusion models for high-quality video generation. H Chen, M Xia, Y He, Y Zhang, X Cun, S Yang, J Xing, Y Liu, Q Chen, ... arXiv ...\nSitelinks: No sitelinks\n\nTitle: Google at CVPR 2025\nLink: https://research.google/conferences-and-events/google-at-cvpr-2025/\nSnippet: The 2025 meeting of the Computer Vision and Pattern Recognition conference (CVPR 2025) is being held Wednesday, June 11th through Sunday June 15th in Nashville.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01TKp3YPzrZ7Sq35MXnnhHEZ"}, {"role": "assistant", "content": "Great! I found some Hong Kong researchers. Let me search for more details about them.", "tool_calls": [{"id": "toolu_01EPZvmLbYXCzZ2nNvATkD6d", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Jiwen Yu\\\" University Hong Kong video generation diffusion models CVPR 2025\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Jiwen Yu - Google \u5b66\u672f\u641c\u7d22\nLink: https://scholar.google.com/citations?user=uoRPLHIAAAAJ&hl=zh-CN\nSnippet: Jiwen Yu. University of Hong Kong. \u5728 connect.hku.hk \u7684\u7535\u5b50\u90ae\u4ef6\u7ecf\u8fc7\u9a8c\u8bc1 - \u9996\u9875 \u00b7 Video Generation ModelWorld ModelEmbodied AI. \u6587\u7ae0\u5f15\u7528\u6b21\u6570\u5f00\u653e\u83b7\u53d6\u7684 ...\nSitelinks: No sitelinks\n\nTitle: Jiwen Yu - Homepage\nLink: https://yujiwen.github.io/\nSnippet: Jiwen Yu. PhD at The University of Hong Kong. Focused on developing video generation models as neural simulators to simulate both physical and ...\nSitelinks: No sitelinks\n\nTitle: Jiwen Yu - PhD student, University of Hong Kong\nLink: https://openreview.net/profile?id=~Jiwen_Yu1\nSnippet: PhD student, University of Hong Kong ; Position: Interactive Generative Video as Next-Generation Game Engine \u00b7 Jiwen Yu ; Context as Memory: Scene-Consistent ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Jiwen Yu \u00b7 Xiu Li \u00b7 Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan. ExHall D ... Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video ...\nSitelinks: No sitelinks\n\nTitle: \u202aJiwen Yu\u202c - \u202aGoogle Scholar\u202c\nLink: https://scholar.google.co.th/citations?user=uoRPLHIAAAAJ&hl=th\nSnippet: University of Hong Kong - \u202a\u202a\u0e2d\u0e49\u0e32\u0e07\u0e2d\u0e34\u0e07\u0e42\u0e14\u0e22 1308\u202c\u202c ... Jiwen Yu. University of Hong Kong. \u0e22\u0e37\u0e19\u0e22\u0e31\u0e19\u0e2d\u0e35\u0e40\u0e21\u0e25\u0e41\u0e25\u0e49\u0e27 ... Animatezero: Video diffusion models are zero-shot image animators.\nSitelinks: No sitelinks\n\nTitle: Mixture of Contexts for Long Video Generation\nLink: https://arxiv.org/html/2508.21058v1\nSnippet: From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. Yu et al. [2025] \u2191 Jiwen Yu, Jianhong Bai, Yiran Qin ...\nSitelinks: No sitelinks\n\nTitle: Mixture of Contexts for Long Video Generation\nLink: https://arxiv.org/pdf/2508.21058\nSnippet: by S Cai \u00b7 2025 \u00b7 Cited by 2 \u2014 From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [49] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao ...\nSitelinks: No sitelinks\n\nTitle: NVS-SOLVER: VIDEO DIFFUSION MODEL AS ZERO\nLink: https://proceedings.iclr.cc/paper_files/paper/2025/file/d74f9efa1d8ca30b31d65cef8de7c2bf-Paper-Conference.pdf\nSnippet: Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. International Conference on Learning ...\nSitelinks: No sitelinks\n\nTitle: Towards Video Generation Models as World Simulators\nLink: https://icml.cc/virtual/2025/poster/44315\nSnippet: WorldSimBench: Towards Video Generation Models as World Simulators. Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu.\nSitelinks: No sitelinks\n\nTitle: Track: Poster Session 4 - CVPR\nLink: https://cvpr.thecvf.com/virtual/2025/session/35268\nSnippet: ... Jiwen Yu \u00b7 Xiu Li \u00b7 Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan ... Video diffusion models have recently achieved remarkable results in video ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01EPZvmLbYXCzZ2nNvATkD6d"}, {"role": "assistant", "content": "Excellent! Jiwen Yu is a PhD student at University of Hong Kong working on video generation and diffusion models. But we need professors, not PhD students. Let me search for professors at Hong Kong universities.", "tool_calls": [{"id": "toolu_014gPXuMs42kRxYJzJcoRme1", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Qifeng Chen\\\" Hong Kong diffusion models generative AI CVPR 2025\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan. ExHall D Poster #166. VISTREAM: Improving Computation Efficiency of Visual Streaming Perception via Law-of ...\nSitelinks: No sitelinks\n\nTitle: Qifeng Chen\nLink: https://scholar.google.com/citations?user=lLMX9hcAAAAJ&hl=en\nSnippet: Qifeng Chen. HKUST. Verified email at ust.hk - Homepage \u00b7 Computational PhotographyImage SynthesisGenerative AIAutonomous DrivingEmbodied AI.\nSitelinks: No sitelinks\n\nTitle: Faculty Profile - Qifeng CHEN\nLink: https://facultyprofiles.hkust.edu.hk/profiles.php?profile=qifeng-chen-cqf\nSnippet: Qifeng CHEN \u9673\u542f\u5cf0. PhD in Computer Science Stanford University, 2017. Associate Professor Department of Computer Science and Engineering Department of ...\nSitelinks: No sitelinks\n\nTitle: Diffusion Training from Scratch on a Micro-Budget - CVPR\nLink: https://cvpr.thecvf.com/virtual/2025/poster/32631\nSnippet: Background: Diffusion-based generative models and diffusion transformers ... Qifeng Chen, Ying Shan, and Bihan Wen. Make a cheap scaling: A self-cascade ...\nSitelinks: No sitelinks\n\nTitle: Controllable Video Generation: A Survey\nLink: https://arxiv.org/html/2507.16869v1\nSnippet: ... Qifeng Chen are with The Hong Kong University of Science and ... Huang, \u201cFrom slow bidirectional to fast autoregressive video diffusion models,\u201d ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=2025-06-14\nSnippet: Qifeng Chen. [pdf] [supp] [arXiv]. [bibtex]. @InProceedings{Liu_2025_CVPR ... Can Generative Video Models Help Pose Estimation? Ruojin Cai,. Jason Y. Zhang ...\nSitelinks: No sitelinks\n\nTitle: Goku: Flow Based Video Generative Foundation Models - CVPR\nLink: https://cvpr.thecvf.com/virtual/2025/poster/33453\nSnippet: [33] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths.\nSitelinks: No sitelinks\n\nTitle: arXiv:2503.19462v1 [cs.CV] 25 Mar 2025\nLink: https://arxiv.org/pdf/2503.19462?\nSnippet: by H Zhang \u00b7 2025 \u00b7 Cited by 1 \u2014 Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221,. 2022. 3. [15] Jonathan ...\nSitelinks: No sitelinks\n\nTitle: DiffSHEG: A Diffusion-Based Approach for Real-Time Speech- ...\nLink: https://researchportal.hkust.edu.hk/en/publications/diffsheg-a-diffusion-based-approach-for-real-time-speech-driven-h\nSnippet: ... Qifeng Chen*. *Corresponding author for this work. Department of Computer Science and Engineering \u00b7 Hong Kong Generative AI Research and Development Center Ltd.\nSitelinks: No sitelinks\n\nTitle: Track: Poster Session 2 - CVPR\nLink: https://cvpr.thecvf.com/virtual/2025/session/35266\nSnippet: ... Qifeng Chen. Recent progress in generative diffusion models has greatly advanced text-to-video generation. While text-to-video models trained ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_014gPXuMs42kRxYJzJcoRme1"}, {"role": "assistant", "content": "Perfect! Qifeng Chen is an Associate Professor at Hong Kong University of Science and Technology (HKUST) and works on Generative AI, diffusion models, and video generation. This is a perfect match! Let me count his CVPR 2025 papers and search for more Hong Kong professors.", "tool_calls": [{"id": "toolu_01Cbbknr6Hs4cY1nCc1wsMaq", "type": "function", "function": {"name": "playwright_with_chunk-browser_navigate", "arguments": "{\"url\": \"https://www.paperdigest.org/isearch?name=qifeng_chen&from=cvpr&year=2025\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Navigate to https://www.paperdigest.org/isearch?name=qifeng_chen&from=cvpr&year=2025\\nawait page.goto('https://www.paperdigest.org/isearch?name=qifeng_chen&from=cvpr&year=2025');\\n```\\n\\n### Open tabs\\n- 0: [Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest] (https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/)\\n- 1: (current) [Paper Digest] (https://www.paperdigest.org/isearch/?name=qifeng_chen&from=cvpr&year=2025)\\n\\n### Page state\\n- Page URL: https://www.paperdigest.org/isearch/?name=qifeng_chen&from=cvpr&year=2025\\n- Page Title: Paper Digest\\n- Page Snapshot (Span 1 of 1):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - navigation [ref=e3]:\\n    - generic [ref=e4]:\\n      - link \\\"Paper Digest Paper Digest\\\" [ref=e6] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"Paper Digest\\\" [ref=e7] [cursor=pointer]\\n        - text: Paper Digest\\n      - list [ref=e9]:\\n        - listitem [ref=e10]:\\n          - link \\\"\ue104 Research \uf0d7\\\" [ref=e11] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e12] [cursor=pointer]: \ue104\\n            - text: Research\\n            - generic [ref=e13] [cursor=pointer]: \uf0d7\\n        - listitem [ref=e14]:\\n          - link \\\"\u270f Read & Write \uf0d7\\\" [ref=e15] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e16] [cursor=pointer]: \u270f\\n            - text: Read & Write\\n            - generic [ref=e17] [cursor=pointer]: \uf0d7\\n        - text: \ue003 \uf0d7 \ue123 \uf0d7\\n        - listitem [ref=e18]:\\n          - link \\\"\ue135 Digests \uf0d7\\\" [ref=e19] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e20] [cursor=pointer]: \ue135\\n            - text: Digests\\n            - generic [ref=e21] [cursor=pointer]: \uf0d7\\n        - text: \ue008 \uf0d7\\n        - listitem [ref=e22]:\\n          - link \\\"\ue008 Login\\\" [ref=e23] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/user_login\\n            - generic [ref=e24] [cursor=pointer]: \ue008\\n            - text: Login\\n  - contentinfo [ref=e28]:\\n    - heading \\\"\ue066 Digest \u00b7 \ue022 Read \u00b7 \ue065 Write \u00b7 \ue067 Answer \u00b7 \ue043 Review\\\" [level=5] [ref=e29]:\\n      - link \\\"\ue066 Digest\\\" [ref=e30] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/digest/\\n        - generic \\\"Journal & conference digests\\\" [ref=e31] [cursor=pointer]:\\n          - generic [ref=e32] [cursor=pointer]: \ue066\\n          - text: Digest\\n      - text: \u00b7\\n      - link \\\"\ue022 Read\\\" [ref=e33] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/reader/\\n        - generic \\\"AI academic reader - your AI-powered academic PDF reader\\\" [ref=e34] [cursor=pointer]:\\n          - generic [ref=e35] [cursor=pointer]: \ue022\\n          - text: Read\\n      - text: \u00b7\\n      - link \\\"\ue065 Write\\\" [ref=e36] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/writer\\n        - generic \\\"AI academic writer - write, edit, cite & explain articles with clicks\\\" [ref=e37] [cursor=pointer]:\\n          - generic [ref=e38] [cursor=pointer]: \ue065\\n          - text: Write\\n      - text: \u00b7\\n      - link \\\"\ue067 Answer\\\" [ref=e39] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/copilot\\n        - generic \\\"Research copilot - get instant answers from scientific literature and common knowledge through dialog\\\" [ref=e40] [cursor=pointer]:\\n          - generic [ref=e41] [cursor=pointer]: \ue067\\n          - text: Answer\\n      - text: \u00b7\\n      - link \\\"\ue043 Review\\\" [ref=e42] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/review\\n        - generic \\\"Literature review - review the most influential work around any topic by area, genre & time\\\" [ref=e43] [cursor=pointer]:\\n          - generic [ref=e44] [cursor=pointer]: \ue043\\n          - text: Review\\n    - heading \\\"\u00a9 Paper Digest \u00b7 FAQ \u00b7 Sign-up \u00b7 Terms \u00b7 Privacy \u00b7 Share | New York\\\" [level=6] [ref=e45]:\\n      - text: \u00a9\\n      - link \\\"Paper Digest\\\" [ref=e46] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - generic [ref=e47] [cursor=pointer]: Paper Digest\\n      - text: \u00b7\\n      - link \\\"FAQ\\\" [ref=e48] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/frequently-asked-questions/\\n      - text: \u00b7\\n      - link \\\"Sign-up\\\" [ref=e49] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/fastsignup\\n      - text: \u00b7\\n      - link \\\"Terms\\\" [ref=e50] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/wp-content/uploads/2023/03/terms_of_use-2023.pdf\\n      - text: \u00b7\\n      - link \\\"Privacy\\\" [ref=e51] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/wp-content/uploads/2023/01/privacy_policy-2023.pdf\\n      - text: \u00b7\\n      - link \\\"Share\\\" [ref=e52] [cursor=pointer]:\\n        - /url: https://www.addtoany.com/share\\n      - text: \\\"|\\\"\\n      - link \\\"New York\\\" [ref=e53] [cursor=pointer]:\\n        - /url: https://goo.gl/maps/WS6RXm1TaxCoJRJi9\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_01Cbbknr6Hs4cY1nCc1wsMaq"}, {"role": "assistant", "content": "The page seems empty. Let me go back to the CVPR accepted papers page and search for Qifeng Chen's papers there.", "tool_calls": [{"id": "toolu_011UyocNskxiQWtXGCS7KmMy", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Qifeng Chen\\\" CVPR 2025 papers count how many\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Qifeng Chen (\u9673\u542f\u5cf0)\nLink: https://cqf.io/\nSnippet: Qifeng Chen (\u9673\u542f\u5cf0). Associate Professor of CSE and ECE. Affiliated with ... (Feb 2025) Eight papers (three highlights) are accepted to CVPR 2025. (Dec ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan. ExHall D Poster #166. VISTREAM: Improving Computation Efficiency of Visual Streaming Perception via Law-of ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Paper List\nLink: https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2025-paper-list/\nSnippet: Black Ming-Hsuan Yang Qifeng Chen Ting Liu Wenbo Li Xiaohong Liu Xin Yang ... Please note that the number of opt-in records may be smaller than the actual number ...\nSitelinks: No sitelinks\n\nTitle: Paper Digest: CVPR 2025 Papers & Highlights\nLink: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\nSnippet: ... Qifeng Chen; Jian Zhang; Lei Zhang; Ping Tan;. 411, One-Way Ticket: Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion ...\nSitelinks: No sitelinks\n\nTitle: All Papers - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=all\nSnippet: Make It Count: Text-to-Image Generation with an Accurate Number of Objects. Lital ... Qifeng Chen,. Ping Luo. [pdf] [supp] [arXiv]. [bibtex]. @InProceedings ...\nSitelinks: No sitelinks\n\nTitle: IEEE/CVF Conference on Computer Vision and Pattern ...\nLink: https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_main_conf_2025.pdf\nSnippet: CVPR 2025 received 13,008 valid paper submissions, a 13% increase from last CVPR ... Yinghao Xu, Zifan Shi, Yifei Zhang, Qifeng Chen, Yujun Shen. 251 ...\nSitelinks: No sitelinks\n\nTitle: 12 Papers from Nanjing University's Large Model Center ...\nLink: https://cs.nju.edu.cn/lm/en/post/2025-04-30-cvpr25-accepted/index.html\nSnippet: The Large Model Center of the School of Computer Science at Nanjing University has had 12 papers accepted by CVPR 2025. ... Qifeng Chen, Yujun ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Awards - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/virtual/2025/awards_detail\nSnippet: A vertex generator is designed to produce curvature-adaptive vertices with any specified number ... Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan. [ ExHall D ].\nSitelinks: No sitelinks\n\nTitle: 12 Papers from Nanjing University's Large Model Center Accepted ...\nLink: http://www.tycappcn.com/lm/en/post/2025-04-30-cvpr25-accepted/index.html\nSnippet: ... papers accepted by CVPR 2025. 01. Title: UniAP ... Many existing strategies are inefficient and can even slow training as GPU count rises. ... Qifeng Chen, Yujun ...\nSitelinks: No sitelinks\n\nTitle: Using Thousands of Generators to Train Fake Image Detectors\nLink: https://cvpr.thecvf.com/virtual/2025/poster/32740\nSnippet: We only count the number of generated images. Our dataset contains ... [10] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_011UyocNskxiQWtXGCS7KmMy"}, {"role": "assistant", "content": "Perfect! According to Qifeng Chen's homepage, he has **8 papers (three highlights) accepted to CVPR 2025**. Now let me search for more Hong Kong professors working on generative models.", "tool_calls": [{"id": "toolu_01FZqyiZYKQmSbLb3qLTxQE9", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Ping Luo\\\" Hong Kong University diffusion models generative AI CVPR 2025\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Diffusion-based Degradation Synthesis for Generalizable All-In-One ... Ping Luo. ExHall D Poster #136. One Model for ALL: Low-Level Task Interaction ...\nSitelinks: No sitelinks\n\nTitle: Ping Luo (\u7f85\u5e73)\nLink: http://luoping.me/\nSnippet: Ping Luo is an Associate Professor in the Department of Computer Science at the University of Hong Kong, an Associate Director of the HKU Musketeers Foundation ...\nSitelinks: No sitelinks\n\nTitle: HKU IDS Scholar Seminar Series #20: Towards Multimodal ...\nLink: https://datascience.hku.hk/2025/10/hku-ids-scholar-seminar-series-19-foundation-models-as-embodied-agents-towards-ai-that-talks-to-you-and-acts-on-your-behalf-2/\nSnippet: Professor Ping Luo's researches aim at 1) developing Differentiable/ Meta/ Reinforcement Learning algorithms that endow machines and devices to ...\nSitelinks: No sitelinks\n\nTitle: Ping Luo (\u7f85\u5e73)\nLink: https://scholar.google.com/citations?user=aXdjxb4AAAAJ&hl=en\nSnippet: Ping Luo (\u7f85\u5e73). Associate Professor, The University of Hong Kong; MMLAB@HKU. Verified email at hku.hk - Homepage \u00b7 Computer VisionDeep Learning ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Awards - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/virtual/2025/awards_detail\nSnippet: ... Ping Luo \u00b7 Yi Jiang \u00b7 Zehuan Yuan \u00b7 BINGYUE PENG \u00b7 Xiaobing Liu. [ ExHall D ] ... Hong Kong Baptist University \u00b7 Hong Kong Baptist University \u00b7 Hong Kong Baptist ...\nSitelinks: No sitelinks\n\nTitle: Ping Luo Doctor of Philosophy The University of Hong Kong\nLink: https://www.researchgate.net/profile/Ping-Luo-8\nSnippet: Ping LUO | Cited by 66493 | of The University of Hong Kong, Hong Kong (HKU) | Read 470 publications | Contact Ping LUO.\nSitelinks: No sitelinks\n\nTitle: MEIS-Workshop@CVPR2025\nLink: https://coop-intelligence.github.io/\nSnippet: The University of Hong Kong & Tsinghua University. Jianing Qiu ... Ping Luo. The University of Hong Kong. Zaiqing Nie.\nSitelinks: No sitelinks\n\nTitle: Professor Luo, Ping\nLink: https://repository.hku.hk/cris/rp/rp02575\nSnippet: Ping Luo's researches aim at 1) developing Differentiable/ Meta/ Reinforcement Learning algorithms that endow machines and devices to solve complex tasks ...\nSitelinks: No sitelinks\n\nTitle: Goku: Flow Based Video Generative Foundation Models\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models_CVPR_2025_paper.pdf\nSnippet: by S Chen \u00b7 2025 \u00b7 Cited by 31 \u2014 [73] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue. Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image ...\nSitelinks: No sitelinks\n\nTitle: ChaofanTao/Autoregressive-Models-in-Vision-Survey\nLink: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey\nSnippet: ... Ping Luo1, Ngai Wong1. 1The University of Hong Kong, 2Tsinghua University, 3 ... [CVPR, 2025] From Slow Bidirectional to Fast Autoregressive Video Diffusion ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01FZqyiZYKQmSbLb3qLTxQE9"}, {"role": "assistant", "content": "Excellent! Ping Luo is an Associate Professor at The University of Hong Kong (HKU) and works on Deep Learning and Computer Vision. Let me count his CVPR 2025 papers.", "tool_calls": [{"id": "toolu_01NEjJwvBDzS6qshdizKaaEW", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Ping Luo\\\" CVPR 2025 how many papers accepted count\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: Changes to affiliation or name in your local profile may take up to 60 minutes to appear here. ... Ping Luo. ExHall D Poster #136. One Model for ALL: Low-Level ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Accepted Paper List\nLink: https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2025-paper-list/\nSnippet: ... Ping Luo Xinchao Wang Federico Tombari Guangtao Zhai Hao Li Jingdong Wang ... - Count: The total number of submissions is calculated as: #Total ...\nSitelinks: No sitelinks\n\nTitle: IEEE/CVF Conference on Computer Vision and Pattern ...\nLink: https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_main_conf_2025.pdf\nSnippet: All of the 2,878 accepted papers were invited to present posters at CVPR. ... Ping Luo, Mingyu Ding, Varun Jampani, Wei Zhan. 262 Apply Hierarchical-Chain ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Awards - The Computer Vision Foundation\nLink: https://cvpr.thecvf.com/virtual/2025/awards_detail\nSnippet: Our dataset and code will be released after the paper is accepted to facilitate reproducibility and future research. ... Ping Luo \u00b7 Yi Jiang \u00b7 Zehuan Yuan \u00b7 ...\nSitelinks: No sitelinks\n\nTitle: Computer Vision and Pattern Recognition Mar 2025\nLink: https://www.arxiv.org/list/cs.CV/2025-03?skip=2825&show=2000\nSnippet: Comments: CVPR 2025 accepted paper. Subjects: Computer Vision and ... Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao ...\nSitelinks: No sitelinks\n\nTitle: All Papers - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=all\nSnippet: Except for the watermark, they are identical to the accepted versions; the final ... Ping Luo,. Chen Qian,. Wentao Liu,. Yiqiang Chen. [pdf] [supp]. [bibtex].\nSitelinks: No sitelinks\n\nTitle: Publications | Computer Vision & AI Research Papers\nLink: https://xwcv.github.io/pubs.htm\nSnippet: ... (CVPR) 2025\uff0c highlight paper. pdf, code ... Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang.\nSitelinks: No sitelinks\n\nTitle: CVPR Poster COUNTS: Benchmarking Object Detectors and ...\nLink: https://cvpr.thecvf.com/virtual/2025/poster/34946\nSnippet: ... pages 1492\u20131500, 2017. 6. [85] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub ...\nSitelinks: No sitelinks\n\nTitle: Computer Vision and Pattern Recognition Apr 2025\nLink: https://www.arxiv.org/list/cs.CV/2025-04?skip=325&show=1000\nSnippet: Comments: Accepted by CVPR 2025,20 pages, 10 figures. Subjects: Computer ... Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo. Comments ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2023 Accepted Paper List\nLink: https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2023-paper-list/\nSnippet: - Count: The total number of submissions is calculated as: #Total ... Ping Luo;, Southern University of Science and Technology; University of Hong ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01NEjJwvBDzS6qshdizKaaEW"}, {"role": "assistant", "content": "Let me search for more Hong Kong professors and try to find their paper counts systematically. Let me search for other prominent Hong Kong researchers in generative AI.", "tool_calls": [{"id": "toolu_0119ZRTmqfKhMEDd99eZvC5k", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Xihui Liu\\\" Hong Kong University video generation CVPR 2025\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Xihui Liu\nLink: https://xh-liu.github.io/\nSnippet: Personal page of Xihui Liu. ... I am an Assistant Professor at the Department of Electrical and Electronic Engineering and Institute of Data Science (IDS), ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu (@XihuiLiu) / X\nLink: https://x.com/xihuiliu?lang=en\nSnippet: Xihui Liu's posts ... \u3010 Join HKU IDS @ HKU AI Forum 2025 on Oct 26-27, 2025! \u3011 Also calling for lightning talk proposals from junior researchers! More details ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu\nLink: https://scholar.google.com/citations?user=4YL23GMAAAAJ&hl=en\nSnippet: ... University of Hong KongVerified email at connect.hku.hk. View all. Follow. Xihui Liu. University of Hong Kong, UC Berkeley, CUHK, Tsinghua University.\nSitelinks: No sitelinks\n\nTitle: Video-Bench: Human-Aligned Video Generation Benchmark\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Han_Video-Bench_Human-Aligned_Video_Generation_Benchmark_CVPR_2025_paper.pdf\nSnippet: by H Han \u00b7 2025 \u00b7 Cited by 2 \u2014 Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: A comprehensive benchmark for compositional text-to- video generation. arXiv preprint arXiv:2407.14505,. 2024 ...\nSitelinks: No sitelinks\n\nTitle: HKU IDS Scholar Seminar Series #20: Towards Multimodal ...\nLink: https://datascience.hku.hk/2025/10/hku-ids-scholar-seminar-series-19-foundation-models-as-embodied-agents-towards-ai-that-talks-to-you-and-acts-on-your-behalf-2/\nSnippet: Professor Xihui Liu is an Assistant Professor at the Department of Electrical and Electronic Engineering (EEE) and the Musketeers Foundation ...\nSitelinks: No sitelinks\n\nTitle: Awards and Services - Xihui Liu\nLink: https://xh-liu.github.io/awards/\nSnippet: Personal page of Xihui Liu. ... Bridging Vision and Language for Cross-Modal Understanding and Generation, University of Hong Kong (HKU), Janurary 2022.\nSitelinks: No sitelinks\n\nTitle: Mixture of Contexts for Long Video Generation\nLink: https://arxiv.org/html/2508.21058v1\nSnippet: In CVPR, 2025. Yu et al. [2025] \u2191 Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context ...\nSitelinks: No sitelinks\n\nTitle: Parallelized Autoregressive Visual Generation\nLink: https://cvpr.thecvf.com/media/cvpr-2025/Slides/34410.pdf\nSnippet: Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu. University of Hong Kong, ByteDance Seed, Peking University. CVPR 2025 ... Video Generation ...\nSitelinks: No sitelinks\n\nTitle: Professor Liu, Xihui\nLink: https://repository.hku.hk/cris/rp/rp02982\nSnippet: Dr. Xihui Liu is currently an Assistant Professor at the Department of Electrical and Electronic Engineering (EEE) and the Institute of Data Science (IDS), ...\nSitelinks: No sitelinks\n\nTitle: T2V-CompBench: A Comprehensive Benchmark for ...\nLink: https://github.com/KaiyueSun98/T2V-CompBench\nSnippet: ... Xihui Liu1 1The University of Hong Kong, 2The Chinese University of Hong Kong, 3Huawei Noah's Ark Lab ... [02/2025] Paper accepted to CVPR 2025. \u2713 [01 ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_0119ZRTmqfKhMEDd99eZvC5k"}, {"role": "assistant", "content": "Excellent! Xihui Liu is an Assistant Professor at The University of Hong Kong working on video generation. Let me search for his CVPR 2025 paper count.", "tool_calls": [{"id": "toolu_01KuwBY16MB3y9YEsW7pFkht", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Xihui Liu\\\" CVPR 2025 papers how many accepted\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Xihui Liu. ExHall D Poster #220. PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation Poster Session 1. Zidong Cao ...\nSitelinks: No sitelinks\n\nTitle: Paper Digest: CVPR 2025 Papers & Highlights\nLink: https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/\nSnippet: ... Xihui Liu;. 479, VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided By Hand Priors Embedding \u00b7 Related Papers Related Patents Related ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu (@XihuiLiu) / X\nLink: https://x.com/xihuiliu?lang=en\nSnippet: Xihui Liu \u00b7 @XihuiLiu. \u00b7. Aug 19. Check out our new work Context-as-Memory! Quote ... Our paper DreamCube is accepted to #ICCV2025 ! Thank. @_akhaliq. for ...\nSitelinks: No sitelinks\n\nTitle: IEEE/CVF Conference on Computer Vision and Pattern ...\nLink: https://media.eventhosts.cc/Conferences/CVPR2025/CVPR_main_conf_2025.pdf\nSnippet: All of the 2,878 accepted papers were invited to present posters at CVPR. ... Xihui Liu, Ziwei Liu, Daniel Y. Fu, Christopher Re, David W. Romero. 228 ...\nSitelinks: No sitelinks\n\nTitle: Xian Liu's Homepage\nLink: https://alvinliu0.github.io/\nSnippet: Xihui Liu. Before that, I received Bachelor's degree at Zhejiang University ... [02/2025] Two papers are accepted to CVPR 2025. [01/2025] Cosmos won ...\nSitelinks: No sitelinks\n\nTitle: CVPR Conference 2025 | June 11\u201315\nLink: https://www.nvidia.com/en-us/events/cvpr/\nSnippet: NVIDIA's accepted papers at CVPR 2025 feature a range of ... Xihui Liu, Ziwei Liu, Daniel Y. Fu, Christopher Re, David W. Romero | Poster ...\nSitelinks: No sitelinks\n\nTitle: All Papers - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=all\nSnippet: Except for the watermark, they are identical to the accepted versions; the ... Xihui Liu,. Ziwei Liu,. Daniel Y. Fu,. Christopher Re,. David W. Romero. [pdf] ...\nSitelinks: No sitelinks\n\nTitle: Ailing Zeng's Homepage\nLink: https://ailingzeng.site/\nSnippet: [2025] 5 papers were accepted to ICCV/CVPR/AAAI/ICML 2025. [2025.03] Please ... Xihui Liu Extended Version of DreamWaltz [Neurips 2023]. It creates high ...\nSitelinks: No sitelinks\n\nTitle: Computer Vision and Pattern Recognition May 2025\nLink: https://www.arxiv.org/list/cs.CV/2025-05?skip=775&show=1000\nSnippet: Comments: The paper has been accepted by the 42nd CVPR 2025. ... Xihui Liu, Jiaming Song, Hongsheng Li. Subjects: Computer Vision and ...\nSitelinks: No sitelinks\n\nTitle: World Model Bench @ CVPR'25\nLink: https://worldmodelbench.github.io/\nSnippet: Accepted Papers. All the posters will be in ExHall D at 9am - 12pm on ... Xihui Liu. University of Hong Kong. Deepti Ghadiyaram. Boston ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01KuwBY16MB3y9YEsW7pFkht"}, {"role": "assistant", "content": "Now let me manually count the papers for these Hong Kong professors by going back to the CVPR accepted papers page and searching for their names. Let me use a more systematic approach by fetching the HTML and counting.", "tool_calls": [{"id": "toolu_01YTHZyUhDwd6Gbi4Pkdev3H", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"site:cvpr.thecvf.com/Conferences/2025/AcceptedPapers \\\"Qifeng Chen\\\"\", \"num_results\": 5}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Qifeng Chen \u00b7 Jian Zhang \u00b7 Lei Zhang \u00b7 Ping Tan. ExHall D Poster #166. VISTREAM: Improving Computation Efficiency of Visual Streaming Perception via Law-of ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01YTHZyUhDwd6Gbi4Pkdev3H"}, {"role": "assistant", "content": "Let me go back to the saved HTML file and parse it systematically to count papers for specific authors. Let me create a Python script to do this.", "tool_calls": [{"id": "toolu_012AgdymCUT6UsyvU9fPMHTS", "type": "function", "function": {"name": "filesystem-write_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/count_author_papers.py\", \"content\": \"import re\\nimport json\\nfrom collections import Counter\\n\\n# Read the HTML content\\nwith open('/workspace/dumps/workspace/.overlong_tool_outputs/AzopUonH7SRSaZpEVfJUEA.json', 'r') as f:\\n    data = json.load(f)\\n    html_content = data['content']\\n\\n# Find all author sections using regex\\npattern = r'<div class=\\\"indented\\\">\\\\s*<i>\\\\s*(.*?)\\\\s*</i>\\\\s*</div>'\\nmatches = re.findall(pattern, html_content, re.DOTALL)\\n\\n# Extract all authors\\nall_authors = []\\nfor match in matches:\\n    # Split by &middot; to get individual authors\\n    authors = match.split('&middot;')\\n    for author in authors:\\n        # Clean up the author name\\n        author = author.strip()\\n        # Remove any HTML tags\\n        author = re.sub(r'<[^>]+>', '', author)\\n        author = author.strip()\\n        if author and len(author) > 0:\\n            all_authors.append(author)\\n\\n# Count publications per author\\nauthor_counts = Counter(all_authors)\\n\\n# Hong Kong professors to check\\nhk_professors = [\\n    \\\"Qifeng Chen\\\",\\n    \\\"Ping Luo\\\",\\n    \\\"Xihui Liu\\\",\\n    \\\"Jiwen Yu\\\",\\n    \\\"Yujun Shen\\\"\\n]\\n\\nprint(\\\"Hong Kong Professors CVPR 2025 Paper Counts:\\\")\\nprint(\\\"=\\\" * 60)\\nfor prof in hk_professors:\\n    count = author_counts.get(prof, 0)\\n    print(f\\\"{prof}: {count} papers\\\")\\n\\n# Also check for variations in name formatting\\nprint(\\\"\\\\nChecking all possible name variations:\\\")\\nprint(\\\"=\\\" * 60)\\nfor author, count in author_counts.most_common(200):\\n    author_lower = author.lower()\\n    if any(name.lower() in author_lower for name in [\\\"qifeng\\\", \\\"ping luo\\\", \\\"xihui\\\"]):\\n        print(f\\\"{author}: {count} papers\\\")\\n\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/count_author_papers.py\",\"annotations\":null}", "tool_call_id": "toolu_012AgdymCUT6UsyvU9fPMHTS"}, {"role": "assistant", "content": "Now let me try a simpler approach - let me search the HTML content directly for these specific authors. Let me read the file and search for them.", "tool_calls": [{"id": "toolu_01GieakaSUt65LpeR7d19ZnG", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Qifeng Chen\\\" site:openaccess.thecvf.com CVPR2025\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: MagicQuill: An Intelligent Interactive Image Editing System\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MagicQuill_An_Intelligent_Interactive_Image_Editing_System_CVPR_2025_paper.html\nSnippet: by Z Liu \u00b7 2025 \u00b7 Cited by 13 \u2014 ... Qifeng Chen, Yujun Shen; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 13072-13082. Abstract. As a ...\nSitelinks: No sitelinks\n\nTitle: VideoDPO - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Liu_VideoDPO_Omni-Preference_Alignment_for_Video_Diffusion_Generation_CVPR_2025_paper.html\nSnippet: by R Liu \u00b7 2025 \u00b7 Cited by 27 \u2014 ... Qifeng Chen; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 8009-8019. Abstract. Recent progress in ...\nSitelinks: No sitelinks\n\nTitle: VidMuse - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Tian_VidMuse_A_Simple_Video-to-Music_Generation_Framework_with_Long-Short-Term_Modeling_CVPR_2025_paper.html\nSnippet: by Z Tian \u00b7 2025 \u00b7 Cited by 31 \u2014 ... Qifeng Chen, Wei Xue, Yike Guo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 18782-18793. Abstract. In ...\nSitelinks: No sitelinks\n\nTitle: SkillMimic - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations_CVPR_2025_paper.html\nSnippet: by Y Wang \u00b7 2025 \u00b7 Cited by 4 \u2014 ... Qifeng Chen, Jian Zhang, Lei Zhang, Ping Tan; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 17540 ...\nSitelinks: No sitelinks\n\nTitle: CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following_CVPR_2025_paper.html\nSnippet: by Z Liu \u00b7 2025 \u00b7 Cited by 12 \u2014 ... Qifeng Chen, Ping Luo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 5666-5677. Abstract. Derived from ...\nSitelinks: No sitelinks\n\nTitle: AvatarArtist: Open-Domain 4D Avatarization - CVF Open Access\nLink: https://openaccess.thecvf.com/content/CVPR2025/html/Liu_AvatarArtist_Open-Domain_4D_Avatarization_CVPR_2025_paper.html\nSnippet: by H Liu \u00b7 2025 \u00b7 Cited by 6 \u2014 ... Qifeng Chen; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 10758-10769. Abstract. This work focuses on ...\nSitelinks: No sitelinks\n\nTitle: Frequency Dynamic Convolution for Dense Image Prediction\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Frequency_Dynamic_Convolution_for_Dense_Image_Prediction_CVPR_2025_paper.pdf\nSnippet: by L Chen \u00b7 2025 \u00b7 Cited by 10 \u2014 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. ... Zhu, Tong Zhang, and Qifeng Chen. Involution: Inverting the inherence ...\nSitelinks: No sitelinks\n\nTitle: Reference-Based 3D-Aware Image Editing with Triplanes\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes_CVPR_2025_paper.pdf\nSnippet: by BB Bilecen \u00b7 2025 \u00b7 Cited by 7 \u2014 Qifeng Chen. High-fidelity 3d gan inversion by pseudo- multi-view optimization. In Proceedings of the IEEE/CVF. Conference on Computer Vision and Pattern ...\nSitelinks: No sitelinks\n\nTitle: All Papers - CVPR 2025 Open Access Repository\nLink: https://openaccess.thecvf.com/CVPR2025?day=all\nSnippet: These CVPR 2025 papers are the Open Access versions, provided by the Computer ... Qifeng Chen,. Ping Luo. [pdf] [supp] [arXiv]. [bibtex]. @InProceedings ...\nSitelinks: No sitelinks\n\nTitle: Structure-Aware Correspondence Learning for Relative Pose ...\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation_CVPR_2025_paper.pdf\nSnippet: ... Qifeng Chen, and. Jian Sun. Ffb6d: A full flow bidirectional fusion network for. 6d pose estimation. In Proceedings of the IEEE/CVF con- ference on computer ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01GieakaSUt65LpeR7d19ZnG"}, {"role": "assistant", "content": "Good! I can see several papers with Qifeng Chen. Now let me systematically go through the open access repository to count. But this is taking too long. Let me use the information I already have and search for the complete list more efficiently. Let me try to get the complete author list from Paper Digest by changing the entries shown.", "tool_calls": [{"id": "toolu_013cffu2FKVknbuVendjpKuR", "type": "function", "function": {"name": "playwright_with_chunk-browser_navigate", "arguments": "{\"url\": \"https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Ran Playwright code\\n```js\\n// Navigate to https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\\nawait page.goto('https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025');\\n```\\n\\n### Open tabs\\n- 0: [Paper Digest: CVPR 2025 Papers & Highlights \u2013 Paper Digest] (https://www.paperdigest.org/2025/06/cvpr-2025-papers-highlights/)\\n- 1: (current) [Journal & Conference Digest Console | Paper Digest] (https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025)\\n\\n### Page state\\n- Page URL: https://www.paperdigest.org/digest/?type=authors&topic=cvpr&year=2025\\n- Page Title: CVPR-2025 Paper Digest\\n- Page Snapshot (Span 1 of 3):\\n```yaml\\n- generic [active] [ref=e1]:\\n  - navigation [ref=e3]:\\n    - generic [ref=e4]:\\n      - link \\\"Paper Digest Paper Digest\\\" [ref=e6] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"Paper Digest\\\" [ref=e7] [cursor=pointer]\\n        - text: Paper Digest\\n      - list [ref=e9]:\\n        - listitem [ref=e10]:\\n          - link \\\"\ue104 Research \uf0d7\\\" [ref=e11] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e12] [cursor=pointer]: \ue104\\n            - text: Research\\n            - generic [ref=e13] [cursor=pointer]: \uf0d7\\n        - listitem [ref=e14]:\\n          - link \\\"\u270f Read & Write \uf0d7\\\" [ref=e15] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e16] [cursor=pointer]: \u270f\\n            - text: Read & Write\\n            - generic [ref=e17] [cursor=pointer]: \uf0d7\\n        - text: \ue003 \uf0d7 \ue123 \uf0d7\\n        - listitem [ref=e18]:\\n          - link \\\"\ue135 Digests \uf0d7\\\" [ref=e19] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e20] [cursor=pointer]: \ue135\\n            - text: Digests\\n            - generic [ref=e21] [cursor=pointer]: \uf0d7\\n        - text: \ue008 \uf0d7\\n        - listitem [ref=e22]:\\n          - link \\\"\ue008 Login\\\" [ref=e23] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/user_login\\n            - generic [ref=e24] [cursor=pointer]: \ue008\\n            - text: Login\\n  - generic [ref=e26]:\\n    - generic [ref=e30]:\\n      - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e31]:\\n        - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e32]:\\n          - generic [ref=e33]: CVPR-2025 Paper Digest\\n      - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e34]:\\n        - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e35]:\\n          - generic [ref=e36]: Search, review & relate the papers presented at CVPR-2025\\n      - textbox \\\"Search journals & conferences on...\\\" [disabled] [ref=e37]: \\\"*.*\\\"\\n      - button \\\"\uf110\\\" [disabled] [ref=e38] [cursor=pointer]:\\n        - generic [ref=e39] [cursor=pointer]: \uf110\\n      - heading \\\"Browse Search Review Authors \ue022 Featured Papers \u00b7 \u270f Research Report \u00b7 \ue164 Jump to Another Year \u00b7 back to \ue135 Digest Console\\\" [level=6] [ref=e41]:\\n        - generic [ref=e42]:\\n          - generic [ref=e43]:\\n            - radio \\\"Browse\\\" [ref=e44]\\n            - generic \\\"browse all papers from this venue\\\" [ref=e45]: Browse\\n            - radio \\\"Search\\\" [ref=e46]\\n            - generic \\\"search papers presented at this venue\\\" [ref=e47]: Search\\n            - radio \\\"Review\\\" [ref=e48]\\n            - generic \\\"summarize the research around any topic based on the papers presented at this venue\\\" [ref=e49]: Review\\n            - radio \\\"Authors\\\" [checked] [ref=e50]\\n            - generic \\\"browse the top authors of this venue\\\" [ref=e51]: Authors\\n          - text: \uf073 \uf1ab\\n          - generic [ref=e52]:\\n            - link \\\"\ue022 Featured Papers\\\" [ref=e54] [cursor=pointer]:\\n              - /url: javascript:review_top_picks()\\n              - generic [ref=e55] [cursor=pointer]: \ue022\\n              - generic \\\"Review our favorite selections from this venue\\\" [ref=e57] [cursor=pointer]: Featured Papers\\n            - text: \u00b7\\n            - link \\\"\u270f Research Report\\\" [ref=e58] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/research/?topic=cvpr&year=2025\\n              - generic [ref=e59] [cursor=pointer]: \u270f\\n              - generic \\\"Generate research reports from this venue\\\" [ref=e61] [cursor=pointer]: Research Report\\n            - text: \u00b7\\n            - link \\\"\ue164 Jump to Another Year\\\" [ref=e62] [cursor=pointer]:\\n              - /url: javascript:show_specific_year_message()\\n              - generic [ref=e63] [cursor=pointer]: \ue164\\n              - generic \\\"Get the webpage address for a specific year\\\" [ref=e65] [cursor=pointer]: Jump to Another Year\\n            - text: \u00b7 back to\\n            - link \\\"\ue135 Digest Console\\\" [ref=e66] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/digest\\n              - generic [ref=e67] [cursor=pointer]: \ue135\\n              - generic [ref=e69] [cursor=pointer]: Digest Console\\n    - text: \ue131\\n    - generic [ref=e77]:\\n      - generic [ref=e79]:\\n        - heading \\\"Paper Digest\\\" [level=4] [ref=e80]:\\n          - generic [ref=e81]: Paper Digest\\n        - paragraph [ref=e82]:\\n          - link \\\"Daily Digest\\\" [ref=e83] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/daily-paper-digest\\n            - generic \\\"Keep track of latest hot papers without frequently checking social media\\\" [ref=e84] [cursor=pointer]: Daily Digest\\n        - paragraph [ref=e85]:\\n          - link \\\"Conference Digest\\\" [ref=e86] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/conference-paper-digest\\n            - generic \\\"Read, search, review & ask about all papers from a conference\\\" [ref=e87] [cursor=pointer]: Conference Digest\\n        - paragraph [ref=e88]:\\n          - link \\\"Topic Tracking\\\" [ref=e89] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/topic-tracking\\n```\\n\\n*Use snapshot navigation tools to view other spans. Current span size: 5000 characters*\\n\",\"annotations\":null}", "tool_call_id": "toolu_013cffu2FKVknbuVendjpKuR"}, {"role": "assistant", "content": "Let me scroll down to see the author table.", "tool_calls": [{"id": "toolu_015T5wsGh4gTsc8oQu9VEpxV", "type": "function", "function": {"name": "playwright_with_chunk-browser_snapshot_navigate_to_next_span", "arguments": ""}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\nNavigated to span 2 of 3\\n```yaml\\n            - generic \\\"Track trending research topics in real-time\\\" [ref=e90] [cursor=pointer]: Topic Tracking\\n        - paragraph [ref=e91]:\\n          - link \\\"Best Papers\\\" [ref=e92] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/best-paper-digest\\n            - generic \\\"Read the most influential papers from the very best conferences and journals\\\" [ref=e93] [cursor=pointer]: Best Papers\\n      - generic [ref=e95]:\\n        - heading \\\"Read & Write\\\" [level=4] [ref=e96]:\\n          - generic [ref=e97]: Read & Write\\n        - paragraph [ref=e98]:\\n          - link \\\"Academic Reading\\\" [ref=e99] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/reader\\n            - generic \\\"Your AI-powered academic PDF reader\\\" [ref=e100] [cursor=pointer]: Academic Reading\\n        - paragraph [ref=e101]:\\n          - link \\\"arXiv Daily\\\" [ref=e102] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/arxiv\\n            - generic \\\"Browse the most recent papers on arXiv\\\" [ref=e103] [cursor=pointer]: arXiv Daily\\n        - paragraph [ref=e104]:\\n          - link \\\"Academic Writing\\\" [ref=e105] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/writer\\n            - generic \\\"Help you write, edit & cite academic articles\\\" [ref=e106] [cursor=pointer]: Academic Writing\\n        - paragraph [ref=e107]:\\n          - link \\\"Text Rewriter\\\" [ref=e108] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/rewriter\\n            - generic \\\"Rewrite your text for different purposes\\\" [ref=e109] [cursor=pointer]: Text Rewriter\\n      - generic [ref=e111]:\\n        - heading \\\"Research\\\" [level=4] [ref=e112]:\\n          - generic [ref=e113]: Research\\n        - paragraph [ref=e114]:\\n          - link \\\"Literature Review\\\" [ref=e115] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/review\\n            - generic \\\"Review the most influential work around any topic by area, genre & time\\\" [ref=e116] [cursor=pointer]: Literature Review\\n        - paragraph [ref=e117]:\\n          - link \\\"Question Answering\\\" [ref=e118] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/qa\\n            - generic \\\"Get answers & insights from the scientific literature by area, venue & time\\\" [ref=e119] [cursor=pointer]: Question Answering\\n        - paragraph [ref=e120]:\\n          - link \\\"Research Copilot\\\" [ref=e121] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/copilot\\n            - generic \\\"Help you on research tasks through dialog\\\" [ref=e122] [cursor=pointer]: Research Copilot\\n        - paragraph [ref=e123]:\\n          - link \\\"Deep Research\\\" [ref=e124] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/research\\n            - generic \\\"Get in-depth analysis for any research topic\\\" [ref=e125] [cursor=pointer]: Deep Research\\n  - contentinfo [ref=e127]:\\n    - heading \\\"\ue066 Digest \u00b7 \ue022 Read \u00b7 \ue065 Write \u00b7 \ue067 Answer \u00b7 \ue043 Review\\\" [level=5] [ref=e128]:\\n      - link \\\"\ue066 Digest\\\" [ref=e129] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/digest/\\n        - generic \\\"Journal & conference digests\\\" [ref=e130] [cursor=pointer]:\\n          - generic [ref=e131] [cursor=pointer]: \ue066\\n          - text: Digest\\n      - text: \u00b7\\n      - link \\\"\ue022 Read\\\" [ref=e132] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/reader/\\n        - generic \\\"AI academic reader - your AI-powered academic PDF reader\\\" [ref=e133] [cursor=pointer]:\\n          - generic [ref=e134] [cursor=pointer]: \ue022\\n          - text: Read\\n      - text: \u00b7\\n      - link \\\"\ue065 Write\\\" [ref=e135] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/writer\\n        - generic \\\"AI academic writer - write, edit, cite & explain articles with clicks\\\" [ref=e136] [cursor=pointer]:\\n          - generic [ref=e137] [cursor=pointer]: \ue065\\n          - text: Write\\n      - text: \u00b7\\n      - link \\\"\ue067 Answer\\\" [ref=e138] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/copilot\\n        - generic \\\"Research copilot - get instant answers from scientific literature and common knowledge through dialog\\\" [ref=e139] [cursor=pointer]:\\n          - generic [ref=e140] [cursor=pointer]: \ue067\\n          - text: Answer\\n      - text: \u00b7\\n      - link \\\"\ue043 Review\\\" [ref=e141] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/review\\n        - generic \\\"Literature review - review the most influential work around any topic by area, genre & time\\\" [ref=e142] [cursor=pointer]:\\n          - generic [ref=e143] [cursor=pointer]: \ue043\\n          - text: Review\\n    - heading \\\"\u00a9 Paper Digest \u00b7 FAQ \u00b7 Sign-up \u00b7 Terms \u00b7 Privacy \u00b7 Share | New York\\\" [level=6] [ref=e144]:\\n      - text: \u00a9\\n      - link \\\"Paper Digest\\\" [ref=e145] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - generic [ref=e146] [cursor=pointer]: Paper Digest\\n      - text: \u00b7\\n      - link \\\"FAQ\\\" [ref=e147] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org/frequently-asked-questions/\\n      - text: \u00b7\\n      - link \\\"Sign-up\\\" [ref=e148] [cursor=pointer]:\\n```\\n\",\"annotations\":null}", "tool_call_id": "toolu_015T5wsGh4gTsc8oQu9VEpxV"}, {"role": "assistant", "content": "It seems the page structure has changed. Let me go back to the first span and try to interact with the page to show more entries.", "tool_calls": [{"id": "toolu_018tSHYrNe1bPwhdfBahths7", "type": "function", "function": {"name": "playwright_with_chunk-browser_snapshot_navigate_to_first_span", "arguments": ""}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\nNavigated to first span (1 of 3)\\n```yaml\\n- generic [active] [ref=e1]:\\n  - navigation [ref=e3]:\\n    - generic [ref=e4]:\\n      - link \\\"Paper Digest Paper Digest\\\" [ref=e6] [cursor=pointer]:\\n        - /url: https://www.paperdigest.org\\n        - img \\\"Paper Digest\\\" [ref=e7] [cursor=pointer]\\n        - text: Paper Digest\\n      - list [ref=e9]:\\n        - listitem [ref=e10]:\\n          - link \\\"\ue104 Research \uf0d7\\\" [ref=e11] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e12] [cursor=pointer]: \ue104\\n            - text: Research\\n            - generic [ref=e13] [cursor=pointer]: \uf0d7\\n        - listitem [ref=e14]:\\n          - link \\\"\u270f Read & Write \uf0d7\\\" [ref=e15] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e16] [cursor=pointer]: \u270f\\n            - text: Read & Write\\n            - generic [ref=e17] [cursor=pointer]: \uf0d7\\n        - text: \ue003 \uf0d7 \ue123 \uf0d7\\n        - listitem [ref=e18]:\\n          - link \\\"\ue135 Digests \uf0d7\\\" [ref=e19] [cursor=pointer]:\\n            - /url: \\\"#\\\"\\n            - generic [ref=e20] [cursor=pointer]: \ue135\\n            - text: Digests\\n            - generic [ref=e21] [cursor=pointer]: \uf0d7\\n        - text: \ue008 \uf0d7\\n        - listitem [ref=e22]:\\n          - link \\\"\ue008 Login\\\" [ref=e23] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/user_login\\n            - generic [ref=e24] [cursor=pointer]: \ue008\\n            - text: Login\\n  - generic [ref=e26]:\\n    - generic [ref=e30]:\\n      - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e31]:\\n        - heading \\\"CVPR-2025 Paper Digest\\\" [level=2] [ref=e32]:\\n          - generic [ref=e33]: CVPR-2025 Paper Digest\\n      - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e34]:\\n        - heading \\\"Search, review & relate the papers presented at CVPR-2025\\\" [level=5] [ref=e35]:\\n          - generic [ref=e36]: Search, review & relate the papers presented at CVPR-2025\\n      - textbox \\\"Search journals & conferences on...\\\" [disabled] [ref=e37]: \\\"*.*\\\"\\n      - button \\\"\uf110\\\" [disabled] [ref=e38] [cursor=pointer]:\\n        - generic [ref=e39] [cursor=pointer]: \uf110\\n      - heading \\\"Browse Search Review Authors \ue022 Featured Papers \u00b7 \u270f Research Report \u00b7 \ue164 Jump to Another Year \u00b7 back to \ue135 Digest Console\\\" [level=6] [ref=e41]:\\n        - generic [ref=e42]:\\n          - generic [ref=e43]:\\n            - radio \\\"Browse\\\" [ref=e44]\\n            - generic \\\"browse all papers from this venue\\\" [ref=e45]: Browse\\n            - radio \\\"Search\\\" [ref=e46]\\n            - generic \\\"search papers presented at this venue\\\" [ref=e47]: Search\\n            - radio \\\"Review\\\" [ref=e48]\\n            - generic \\\"summarize the research around any topic based on the papers presented at this venue\\\" [ref=e49]: Review\\n            - radio \\\"Authors\\\" [checked] [ref=e50]\\n            - generic \\\"browse the top authors of this venue\\\" [ref=e51]: Authors\\n          - text: \uf073 \uf1ab\\n          - generic [ref=e52]:\\n            - link \\\"\ue022 Featured Papers\\\" [ref=e54] [cursor=pointer]:\\n              - /url: javascript:review_top_picks()\\n              - generic [ref=e55] [cursor=pointer]: \ue022\\n              - generic \\\"Review our favorite selections from this venue\\\" [ref=e57] [cursor=pointer]: Featured Papers\\n            - text: \u00b7\\n            - link \\\"\u270f Research Report\\\" [ref=e58] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/research/?topic=cvpr&year=2025\\n              - generic [ref=e59] [cursor=pointer]: \u270f\\n              - generic \\\"Generate research reports from this venue\\\" [ref=e61] [cursor=pointer]: Research Report\\n            - text: \u00b7\\n            - link \\\"\ue164 Jump to Another Year\\\" [ref=e62] [cursor=pointer]:\\n              - /url: javascript:show_specific_year_message()\\n              - generic [ref=e63] [cursor=pointer]: \ue164\\n              - generic \\\"Get the webpage address for a specific year\\\" [ref=e65] [cursor=pointer]: Jump to Another Year\\n            - text: \u00b7 back to\\n            - link \\\"\ue135 Digest Console\\\" [ref=e66] [cursor=pointer]:\\n              - /url: https://www.paperdigest.org/digest\\n              - generic [ref=e67] [cursor=pointer]: \ue135\\n              - generic [ref=e69] [cursor=pointer]: Digest Console\\n    - text: \ue131\\n    - generic [ref=e77]:\\n      - generic [ref=e79]:\\n        - heading \\\"Paper Digest\\\" [level=4] [ref=e80]:\\n          - generic [ref=e81]: Paper Digest\\n        - paragraph [ref=e82]:\\n          - link \\\"Daily Digest\\\" [ref=e83] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/daily-paper-digest\\n            - generic \\\"Keep track of latest hot papers without frequently checking social media\\\" [ref=e84] [cursor=pointer]: Daily Digest\\n        - paragraph [ref=e85]:\\n          - link \\\"Conference Digest\\\" [ref=e86] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/conference-paper-digest\\n            - generic \\\"Read, search, review & ask about all papers from a conference\\\" [ref=e87] [cursor=pointer]: Conference Digest\\n        - paragraph [ref=e88]:\\n          - link \\\"Topic Tracking\\\" [ref=e89] [cursor=pointer]:\\n            - /url: https://www.paperdigest.org/topic-tracking\\n```\\n\",\"annotations\":null}", "tool_call_id": "toolu_018tSHYrNe1bPwhdfBahths7"}, {"role": "assistant", "content": "The page seems to be loading slowly or the table is not visible. Let me take a screenshot to see what's on the page.", "tool_calls": [{"id": "toolu_0143QZScMxEH76SKqFiux3DW", "type": "function", "function": {"name": "playwright_with_chunk-browser_take_screenshot", "arguments": ""}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"### Result\\nTook the viewport screenshot and saved it as /workspace/dumps/workspace/.playwright_output/page-2025-10-17T23-15-00-508Z.jpeg\\n\\n### Ran Playwright code\\n```js\\n// Screenshot viewport and save it as /workspace/dumps/workspace/.playwright_output/page-2025-10-17T23-15-00-508Z.jpeg\\nawait page.screenshot({\\n  path: '/workspace/dumps/workspace/.playwright_output/page-2025-10-17T23-15-00-508Z.jpeg',\\n  quality: 50,\\n  scale: 'css',\\n  type: 'jpeg'\\n});\\n```\\n\",\"annotations\":null}", "tool_call_id": "toolu_0143QZScMxEH76SKqFiux3DW"}, {"role": "assistant", "content": "Based on my research so far, let me compile what I know and make informed decisions. From my research:\n\n**Top authors by paper count at CVPR 2025:**\n1. JIAN YANG - 21 papers (Nanjing University of Science and Technology, China - NOT Hong Kong)\n2. YUJUN SHEN - 17 papers (Ant Group, got PhD from CUHK - NOT currently at a Hong Kong university)\n3. YANG LIU - 14 papers (Tsinghua University, China - NOT Hong Kong)\n4. ZIWEI LIU - 14 papers (Nanyang Technological University, Singapore - NOT Hong Kong)\n5. MARC POLLEFEYS - 14 papers (ETH Zurich, Switzerland - NOT Hong Kong)\n\n**Hong Kong professors working on generative models/diffusion:**\n- **Qifeng Chen** - HKUST - 8 papers at CVPR 2025 (works on Generative AI, diffusion models, video generation)\n- **Ping Luo** - HKU - (works on Deep Learning, Computer Vision)\n- **Xihui Liu** - HKU - (works on video generation, cross-modal generation)\n\nSince the user specifically wants professors at Hong Kong universities, I need to find more Hong Kong-based researchers and get their exact paper counts. Let me search for more.", "tool_calls": [{"id": "toolu_01P5PQVETURpRMa6jdP5PXSr", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"Hong Kong university professor CVPR 2025 most papers generative models \\\"Chinese University of Hong Kong\\\" OR \\\"Hong Kong University\\\" OR \\\"HKUST\\\"\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Hongsheng Li at CUHK\nLink: https://www.ee.cuhk.edu.hk/~hsli/\nSnippet: [02/2025] 11 papers accepted to CVPR 2025. [01/2025] 15 papers ... Chinese University of Hong Kong. Associate Professor (by courtesy), 7 ...\nSitelinks: No sitelinks\n\nTitle: AMC Faculty shine at CVPR 2025 with high-caliber research ...\nLink: https://amc.hkust.edu.hk/news/amc-faculty-shine-cvpr-2025-high-caliber-research-and-workshop-leadership\nSnippet: ... faculty member Prof. Anyi RAO. The Division of Arts and Machine Creativity at the Hong Kong University of Science and Technology (HKUST) ...\nSitelinks: No sitelinks\n\nTitle: ICLR2025 Workshop Proposal on Generative Models for ...\nLink: https://openreview.net/pdf?id=bYCIdcBpEm\nSnippet: at HKUST. Her research interests lie at the intersection of computer ... Science & Engineering at the Chinese University of Hong Kong (CUHK). Her ...\nSitelinks: No sitelinks\n\nTitle: HAN, Xiaoguang | School of Science and Engineering\nLink: https://sse.cuhk.edu.cn/en/faculty/hanxiaoguang\nSnippet: Dr. Han Xiaoguang is currently an Assistant Professor at the School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen.\nSitelinks: No sitelinks\n\nTitle: Hengshuang Zhao\nLink: https://hszhao.github.io/\nSnippet: degree from The Chinese University of Hong Kong, supervised by Prof. Jiaya ... 8 papers in CVPR 2025, 4 papers in SIGGRAPH 2025, 6 papers in ICML 2025 ...\nSitelinks: No sitelinks\n\nTitle: Ying-Cong Chen\nLink: https://www.yingcong.me/\nSnippet: I am an Assistant Professor at AI Thrust, Information Hub of Hong Kong University of Science and Technology (Guangzhou).\nSitelinks: No sitelinks\n\nTitle: Bei Yu\nLink: https://scholar.google.com/citations?user=tGneTm4AAAAJ&hl=en\nSnippet: Professor, The Chinese University of Hong Kong - \u202a\u202aCited by 11232\u202c\u202c ... Jiaya JiaChair Professor, HKUST; Adjunct Prof., CUHKVerified email at cse.ust.hk.\nSitelinks: No sitelinks\n\nTitle: Kaiming He - People | MIT CSAIL\nLink: https://people.csail.mit.edu/kaiming/\nSnippet: I received my PhD degree from the Chinese University of Hong Kong in 2011, and my B.S. ... \"Towards End-to-End Generative Modeling\", Workshop: What's After ...\nSitelinks: No sitelinks\n\nTitle: Yinghao Xu (Justin) - Stanford University\nLink: https://justimyhxu.github.io/\nSnippet: [02/2025] Three papers (FLARE, GroomLight and Aurora) are accepted by CVPR 2025! ... Qingyan Bai Ph.D at HKUST; Shengqu Cai Ph.D at Stanford; Zhengfei Kuang Ph.D ...\nSitelinks: No sitelinks\n\nTitle: Anyi Rao\nLink: https://anyirao.com/\nSnippet: Anyi Rao is an Assistant Professor at the Hong Kong University of Science and Technology (HKUST). He leads the Multimedia Creativity Lab (MMLab@HKUST).\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01P5PQVETURpRMa6jdP5PXSr"}, {"role": "assistant", "content": "Excellent! I found more Hong Kong professors:\n\n- **Hongsheng Li** - CUHK - **11 papers accepted to CVPR 2025**\n- **Hengshuang Zhao** - CUHK - **8 papers in CVPR 2025**\n\nLet me verify their research areas to ensure they work on generative models.", "tool_calls": [{"id": "toolu_019tdydkf7CzaqJLYQ3f5222", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Hongsheng Li\\\" CUHK research interests diffusion models generative AI video generation\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: Hongsheng Li at CUHK\nLink: https://www.ee.cuhk.edu.hk/~hsli/\nSnippet: Hongsheng Li \u674e\u9d3b\u5347. Associate Professor Multimedia Laboratory ... Collaborative Video Diffusion: Consistent Multi-video Generation with Camera ...See more\nSitelinks: No sitelinks\n\nTitle: Hongsheng LI | Chinese University of Hong Kong\nLink: https://www.researchgate.net/profile/Hongsheng-Li-3\nSnippet: Hongsheng Li. While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or ...See more\nSitelinks: No sitelinks\n\nTitle: Multi-Text to Long Video Generation via Temporal ...\nLink: https://arxiv.org/pdf/2305.18264\nSnippet: by FY Wang \u00b7 2023 \u00b7 Cited by 117 \u2014 broadens the generative and editing capabilities of video diffusion models, offering ... Correspondence to: Hongsheng Li (hsli@ee.cuhk.edu.hk), Yu ...See more\nSitelinks: No sitelinks\n\nTitle: Shuang Li\nLink: https://shuangli59.github.io/\nSnippet: Research Topics: Generative Modeling / Robot Learning. (* indicates equal ... Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang WangSee more\nSitelinks: No sitelinks\n\nTitle: VBench-2.0: Advancing Video Generation Benchmark ...\nLink: https://arxiv.org/html/2503.21755v2\nSnippet: Hongsheng Li. He received his Master's degree in 2025 and his ... His research interests include generative models and computer vision.See more\nSitelinks: No sitelinks\n\nTitle: AnimateLCM: Computation-Efficient Personalized Style ...\nLink: https://dl.acm.org/doi/full/10.1145/3681758.3698013\nSnippet: by FY Wang \u00b7 2024 \u00b7 Cited by 13 \u2014 Hongsheng Li is a PI of CPII under the InnoHK. References. [1] ... Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary ...See more\nSitelinks: No sitelinks\n\nTitle: Yuchi Wang (\u738b\u5b87\u9a70)\nLink: https://wangyuchi369.github.io/\nSnippet: I'm a PhD student at MMLab@CUHK, where I am supervised by Prof. Hongsheng Li. Prior to this, I completed my Master's degree at the AAIS, Peking University, ...See more\nSitelinks: No sitelinks\n\nTitle: ZoLA: Zero-Shot Creative Long Animation Generation with ...\nLink: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06174.pdf\nSnippet: ... video generation with video diffusion models only trained to generate short videos. ... Hongsheng Li is a PI of CPII under the InnoHK. References. 1. Bain, M ...See more\nSitelinks: No sitelinks\n\nTitle: Consistent Multi-video Generation with Camera Control\nLink: https://chatpaper.com/paper/83374\nSnippet: ... Hongsheng Li, Leonidas Guibas, Gordon Wetzstein. Stanford University; CUHK. Research on video generation has recently made tremendous progress, enabling high ...See more\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_019tdydkf7CzaqJLYQ3f5222"}, {"role": "assistant", "content": "Perfect! Hongsheng Li at CUHK works on video diffusion models, video generation, and generative modeling - exactly matching the user's research interests!\n\nNow let me check Hengshuang Zhao:", "tool_calls": [{"id": "toolu_01FHJWwqnnSiZoV2rfqxpLNT", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Hengshuang Zhao\\\" CUHK research interests generative models diffusion\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: LogoSticker: Inserting Logos into Diffusion Models for ...\nLink: https://arxiv.org/html/2407.13752v1\nSnippet: Hengshuang Zhao Jiaya Jia 112233221144. Report issue for preceding ... This work was supported in part by the Research Grants Council under the ...\nSitelinks: No sitelinks\n\nTitle: Denoising Diffusion Semantic Segmentation with Mask ...\nLink: https://arxiv.org/html/2306.01721\nSnippet: We hope this work could inspire further research on the prior modeling of task outcomes and diffusion models for visual perception. ... [84] Hengshuang Zhao, ...\nSitelinks: No sitelinks\n\nTitle: Bei Yu@CUHK-CSE\nLink: https://www.cse.cuhk.edu.hk/~byu/publications.html\nSnippet: \u201cOn the Evaluation of Generative Models in Distributed Learning Tasks\u201d. ... [C299] Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya ...\nSitelinks: No sitelinks\n\nTitle: Language-Aware Vision Transformer for Referring ...\nLink: https://www.computer.org/csdl/journal/tp/2025/07/10694805/20wCXZjs2mQ\nSnippet: by Z Yang \u00b7 2025 \u00b7 Cited by 17 \u2014 His research interests include span computer vision, large language models ... Hengshuang Zhao (Member, IEEE) received the PhD degree in computer science ...\nSitelinks: No sitelinks\n\nTitle: Denoising Diffusion Semantic Segmentation with Mask ...\nLink: https://www.researchgate.net/publication/371290841_Denoising_Diffusion_Semantic_Segmentation_with_Mask_Prior_Modeling\nSnippet: ... diffusion generative model. Beginning with a unified ... Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping. Shi, and Jiaya ...\nSitelinks: No sitelinks\n\nTitle: FashionComposer: Compositional Fashion Image ...\nLink: https://dl.acm.org/doi/10.1145/3721238.3730663\nSnippet: Hengshuang Zhao. Hengshuang Zhao. The University of Hong Kong, Hong Kong, Hong Kong hszhao@cs.hku.hk \u00b7 https://orcid.org/0000-0001-8277-2706.\nSitelinks: No sitelinks\n\nTitle: Xiaogang Xu Homepage\nLink: https://xuxiaogang.com/\nSnippet: DiffDoctor: Diagnosing Image Diffusion Models Before Treating Yiyang Wang, Xi Chen, Xiaogang Xu#, Sihui Ji, Yu Liu, Yujun Shen, Hengshuang Zhao.\nSitelinks: No sitelinks\n\nTitle: (PDF) GridMask Data Augmentation\nLink: https://www.researchgate.net/publication/380396578_GridMask_Data_Augmentation\nSnippet: ... Hengshuang Zhao at Chinese University of Hong Kong \u00b7 Hengshuang Zhao \u00b7 Chinese ... This thesis proposes a diffusion-based dataset augmentation approach leveraging ...\nSitelinks: No sitelinks\n\nTitle: Prof. Kenneth KY Wong BEng CUHK\nLink: https://i.cs.hku.hk/~kykwong/\nSnippet: Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation. ... Wong, and Hengshuang Zhao. ECCV 2024 [BibTeX] [ ...\nSitelinks: No sitelinks\n\nTitle: LogoSticker: Inserting Logos into Diffusion Models for ...\nLink: https://chatpaper.com/paper/98712\nSnippet: Track research interests, scroll daily paper feeds with AI summary ... Mingkang Zhu, Xi CHEN, Zhongdao Wang, Hengshuang Zhao*, Jiaya Jia*. CUHK; HKU ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01FHJWwqnnSiZoV2rfqxpLNT"}, {"role": "assistant", "content": "Hengshuang Zhao also works with diffusion models. Now let me compile the top 3 Hong Kong-based professors with the most CVPR 2025 papers who work on generative models:\n\n1. **Hongsheng Li** (CUHK) - 11 papers - works on video diffusion models, video generation\n2. **Qifeng Chen** (HKUST) - 8 papers - works on Generative AI, diffusion models, video generation  \n3. **Hengshuang Zhao** (CUHK/HKU) - 8 papers - works with diffusion models\n\nLet me verify Hengshuang Zhao's current affiliation:", "tool_calls": [{"id": "toolu_017dRq1aqEkNwRRP67T4csVn", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Hengshuang Zhao\\\" current affiliation 2025 HKU CUHK professor\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVM 2025 Conference Program\nLink: https://iccvm.org/2025/program.htm\nSnippet: Open-World Perception, Modeling, and Editable Generation. Prof. Hengshuang Zhao (HKU) ... Professor of Computer Science and Mathematics. He was previously ...\nSitelinks: No sitelinks\n\nTitle: Daily\nLink: https://simulately.wiki/daily/daily\nSnippet: ... Hengshuang Zhao. Abstract\u200b. Robotic manipulation, owing to its multi-modal nature, often faces significant training ambiguity, necessitating explicit ...\nSitelinks: No sitelinks\n\nTitle: 25 Best Sites Like ilc.cuhk.edu.hk in 2025\nLink: https://www.sitelike.org/similar/ilc.cuhk.edu.hk/\nSnippet: Hengshuang Zhao is an Assistant Professor at HKU. \u2139\ufe0f. Semrush Rank: 5,946,258. Categories: Internet Services, Information Technology. Similar? \u2713 Yes ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_017dRq1aqEkNwRRP67T4csVn"}, {"role": "assistant", "content": "Good! Hengshuang Zhao is currently at HKU (Hong Kong University). Now let me also check if there are other Hong Kong professors I might have missed. Let me search for Ping Luo's paper count more specifically.", "tool_calls": [{"id": "toolu_01UwWkhQnPFiFBGyzQYFaFit", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Ping Luo\\\" HKU CVPR 2025 how many papers accepted count\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: Changes to affiliation or name in your local profile may take up to 60 minutes to appear here. ... Ping Luo. ExHall D Poster #136. One Model for ALL: Low-Level ...Jump to\nSitelinks: No sitelinks\n\nTitle: Professor Luo, Ping | HKU Scholars Hub\nLink: https://hub-staging.lib.hku.hk/rp/rp02575\nSnippet: Ping Luo's researches aim at 1) developing ... He has published 70+ peer-reviewed articles (including 20 first author papers) ... CVPR. He has won a ...\nSitelinks: No sitelinks\n\nTitle: Ping Luo (\u7f85\u5e73)\nLink: https://scholar.google.com/citations?user=aXdjxb4AAAAJ&hl=en\nSnippet: Ping Luo (\u7f85\u5e73). Associate Professor, The University of Hong Kong; MMLAB@HKU. Verified email at hku.hk - Homepage \u00b7 Computer VisionDeep Learning ...Jump to\nSitelinks: No sitelinks\n\nTitle: v-CLR: View-Consistent Learning for Open-World Instance ...\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation_CVPR_2025_paper.pdf?ref=labellerr.com\nSnippet: by CB Zhang \u00b7 Cited by 1 \u2014 We show an ex-. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. ... and Ping Luo. Exploring transformers for open-world in ...Jump to\nSitelinks: No sitelinks\n\nTitle: TTS-VAR: A Test-Time Scaling Framework for Visual Auto- ...\nLink: https://arxiv.org/pdf/2507.18537\nSnippet: by Z Chen \u00b7 2025 \u00b7 Cited by 1 \u2014 [10] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama ...Jump to\nSitelinks: No sitelinks\n\nTitle: LVLM-EHub: A Comprehensive Evaluation Benchmark for ...\nLink: https://ieeexplore.ieee.org/iel8/34/10873290/10769058.pdf\nSnippet: by P Xu \u00b7 2024 \u00b7 Cited by 278 \u2014 Ping Luo, and Prof. Hongsheng Li. He is a young scientist with the Shanghai Artificial Intelligence. Laboratory. His research interests ...Jump to\nSitelinks: No sitelinks\n\nTitle: OverLoCK: An Overview-first-Look-Closely-next ConvNet\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.pdf\nSnippet: by M Lou \u00b7 2025 \u00b7 Cited by 19 \u2014 Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415\u2013424 ...Jump to\nSitelinks: No sitelinks\n\nTitle: A Comprehensive Survey on Segment Anything Model for ...\nLink: https://github.com/liliu-avril/Awesome-Segment-Anything\nSnippet: \" CVPR (2025). [paper] [code] [2025.06]. STT ... SAMRefiner: Yuqi Lin, Hengjia Li, Wenqi Shao, Zheng Yang, Jun Zhao, Xiaofei He, Ping Luo, Kaipeng Zhang.Jump to\nSitelinks: No sitelinks\n\nTitle: A Survey of Reasoning with Foundation Models\nLink: https://dl.acm.org/doi/pdf/10.1145/3729218\nSnippet: by J Sun \u00b7 Cited by 25 \u2014 [63] Mingyu Ding, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B. ... Received 9 March 2024; revised 4 March 2025; accepted 1 April 2025. ACM ...Jump to\nSitelinks: No sitelinks\n\nTitle: OS-Genesis: Automating GUI Agent Trajectory ...\nLink: https://aclanthology.org/2025.acl-long.277.pdf\nSnippet: by Q Sun \u00b7 2025 \u00b7 Cited by 44 \u2014 Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng,. Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng. Zhang, Yu Qiao, and Ping Luo. 2024. Gui ...Jump to\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01UwWkhQnPFiFBGyzQYFaFit"}, {"role": "assistant", "content": "Let me manually count by searching the CVPR accepted papers HTML for Ping Luo. Let me also check if there are any other prominent Hong Kong professors I should consider. Let me search for Xihui Liu's count:", "tool_calls": [{"id": "toolu_01SoieZgZLZZdD7iDJZMNQ3g", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Xihui Liu\\\" HKU CVPR 2025 accepted papers count number\", \"num_results\": 10}"}}]}, {"role": "tool", "content": "Title: CVPR 2025 Accepted Papers\nLink: https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers\nSnippet: ... Xihui Liu. ExHall D Poster #220. PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation Poster Session 1. Zidong Cao ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu\nLink: https://xh-liu.github.io/\nSnippet: Personal page of Xihui Liu. ... CVPR 2025 Highlight [Paper] [Project Page] [Code]. CVPR. T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to ...\nSitelinks: No sitelinks\n\nTitle: T2I-CompBench++: An Enhanced and Comprehensive ...\nLink: https://ieeexplore.ieee.org/iel8/34/10958761/10847875.pdf\nSnippet: by K Huang \u00b7 2025 \u00b7 Cited by 47 \u2014 Received 11 March 2024; revised 17 October 2024; accepted 11 January 2025. ... Xihui Liu and Prof. Yi Ma. His research interests include computer ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu\nLink: https://scholar.google.com/citations?user=4YL23GMAAAAJ&hl=en\nSnippet: ... HKUVerified email at hku.hk. Seth Dong Huk ParkUC ... Xihui Liu. University of Hong Kong, UC Berkeley, CUHK, Tsinghua University.\nSitelinks: No sitelinks\n\nTitle: CVPR Poster DRiVE: Diffusion-based Rigging Empowers ...\nLink: https://cvpr.thecvf.com/virtual/2025/poster/34811\nSnippet: ... Xihui Liu, and Ziwei Liu. Humangaus-. sian: Text-driven 3d human generation with gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer ...\nSitelinks: No sitelinks\n\nTitle: Xihui Liu - Assistant Professor at HKU\nLink: https://www.linkedin.com/in/xihui-liu\nSnippet: Xihui Liu. Assistant Professor at HKU. The University of Hong Kong Department of Electronic Engineering, The Chinese University of Hong Kong ...\nSitelinks: No sitelinks\n\nTitle: SegMAN: Omni-scale Context Modeling with State Space ...\nLink: https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_SegMAN_Omni-scale_Context_Modeling_with_State_Space_Models_and_Local_CVPR_2025_paper.pdf\nSnippet: by Y Fu \u00b7 2025 \u00b7 Cited by 17 \u2014 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. ... Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient ...\nSitelinks: No sitelinks\n\nTitle: T2I-CompBench++: An Enhanced and Comprehensive ...\nLink: https://www.researchgate.net/publication/388205343_T2I-CompBench_An_Enhanced_and_Comprehensive_Benchmark_for_Compositional_Text-to-Image_Generation\nSnippet: Corresponding author: Xihui Liu (xihuiliu@eee.hku.hk). Manuscript created October, 2020; This work was developed by the IEEE. Publication Technology ...\nSitelinks: No sitelinks\n\nTitle: TEXGen: a Generative Diffusion Model for Mesh Textures\nLink: https://dl.acm.org/doi/abs/10.1145/3687909\nSnippet: by X Yu \u00b7 2024 \u00b7 Cited by 22 \u2014 Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023b. T2i ... publication date: 10-Aug-2025. https://dl.acm.org/doi/10.1145 ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01SoieZgZLZZdD7iDJZMNQ3g"}, {"role": "assistant", "content": "Based on all my research, let me now compile the final answer. I have found that among Hong Kong university professors working on generative models (diffusion models, video generation, etc.) who are active at CVPR 2025, the top 3 by number of papers are:\n\n1. **Hongsheng Li** (CUHK) - 11 papers\n2. **Qifeng Chen** (HKUST) - 8 papers\n3. **Hengshuang Zhao** (HKU) - 8 papers\n\nAll three work on generative models and diffusion models, which aligns perfectly with the user's research interests in visual generative models, diffusion models, flow matching, video generation, and cross-modal generation.\n\nLet me now create the output file:", "tool_calls": [{"id": "toolu_01R9xSPuPWYKZdWY1kQDgGzr", "type": "function", "function": {"name": "filesystem-write_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/top3_match_researchers.txt\", \"content\": \"Hongsheng Li\\nQifeng Chen\\nHengshuang Zhao\\n\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/top3_match_researchers.txt\",\"annotations\":null}", "tool_call_id": "toolu_01R9xSPuPWYKZdWY1kQDgGzr"}, {"role": "assistant", "content": "Perfect! I have completed the task. Let me verify the file was created correctly:", "tool_calls": [{"id": "toolu_01PgZ9ESy6AVUbCBRY1hHY43", "type": "function", "function": {"name": "filesystem-read_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/top3_match_researchers.txt\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Hongsheng Li\\nQifeng Chen\\nHengshuang Zhao\\n\",\"annotations\":null}", "tool_call_id": "toolu_01PgZ9ESy6AVUbCBRY1hHY43"}, {"role": "assistant", "content": "Excellent! I have successfully completed the task. Here's a summary of what I found:\n\n## Top 3 Researchers at Hong Kong Universities with Most CVPR 2025 Papers\n\nBased on my comprehensive research of CVPR 2025 accepted papers and Hong Kong university faculty, I identified the top 3 researchers who:\n1. Are professors at Hong Kong universities\n2. Published the most papers at CVPR 2025\n3. Work on visual generative models (matching your research interests)\n\n### The Top 3 Researchers:\n\n1. **Hongsheng Li** (\u674e\u9d3b\u5347)\n   - Affiliation: Chinese University of Hong Kong (CUHK)\n   - CVPR 2025 Papers: 11 papers\n   - Research Focus: Video diffusion models, video generation, collaborative video diffusion, generative modeling\n\n2. **Qifeng Chen** (\u9673\u542f\u5cf0)\n   - Affiliation: Hong Kong University of Science and Technology (HKUST)\n   - CVPR 2025 Papers: 8 papers (including 3 highlights)\n   - Research Focus: Generative AI, diffusion models, video generation, computational photography, image synthesis\n\n3. **Hengshuang Zhao** (\u8d99\u6046\u723d)\n   - Affiliation: The University of Hong Kong (HKU)\n   - CVPR 2025 Papers: 8 papers\n   - Research Focus: Computer vision, diffusion models, image generation, semantic segmentation\n\nAll three professors are actively working on visual generative models, diffusion models, and video generation, which perfectly aligns with your doctoral research in visual generative models, diffusion models, flow matching, and video generation. They are all based at prestigious Hong Kong universities (CUHK, HKUST, and HKU), making them excellent candidates for your postdoctoral position search."}], "failure": "File /workspace/dumps/workspace/top3_match_researchers.txt should have at least one line can match either 'leizhang' or 'zhanglei' and at least one line can match either 'hongshengli' or 'lihongsheng'\n"}