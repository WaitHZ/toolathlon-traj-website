{"pass": true, "messages": [{"role": "user", "content": "Hello! I am a PhD student engaged in deep learning research. Recently, my collaborators and I have proposed a logical reasoning dataset and are writing a paper to introduce this new dataset to the community. To demonstrate the differences between our work and previous work, I need you to help me summarize a latex table and save it in the workspace with name as `datasets.tex`, the tex file should only contain table content (no commented lines), without any other content. The table needs to include four columns: Dataset (dataset name), Tasks (number of tasks included in the dataset), Trainable (whether it includes a training set, filled with \\ding{55} or \\ding{51}), and Adjustable Difficulty (whether it includes different difficulty levels, filled with \\ding{55} or \\ding{51}). The table format is as follows:\n\n```tex\n\\begin{table}[!ht]\n    \\begin{center}\n    \\begin{tabular}{lccc}\n        \\toprule\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\n        \\midrule\n        % content\n        \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table}\n```\n\nThe names of the datasets we need to include are (from top to bottom):\n- BBH\n- Zebra Logic\n- KOR-Bench (# of tasks should based the broader categorization in the paper.)\n- K&K (https://arxiv.org/abs/2410.23123)\n- BBEH"}, {"role": "assistant", "content": "I'll help you create a LaTeX table summarizing these logical reasoning datasets. Let me search for information about each dataset to fill in the details.", "tool_calls": [{"id": "toolu_0133q6DTqiAGPabBWe9MPeJf", "type": "function", "function": {"name": "arxiv_local-search_papers", "arguments": "{\"query\": \"BBH Big-Bench Hard logical reasoning\", \"max_results\": 5}"}}, {"id": "toolu_016aV7vagYEBDo36GhYVNENU", "type": "function", "function": {"name": "arxiv_local-search_papers", "arguments": "{\"query\": \"Zebra Logic puzzle reasoning dataset\", \"max_results\": 5}"}}, {"id": "toolu_01UKqpdNVvciQqvKUvzGfod9", "type": "function", "function": {"name": "arxiv_local-search_papers", "arguments": "{\"query\": \"KOR-Bench knowledge reasoning benchmark\", \"max_results\": 5}"}}, {"id": "toolu_01URmR82cv3uCxF4UkcVj5YT", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\"}"}}, {"id": "toolu_016bZ9TKczNHRdJEqWivxSxM", "type": "function", "function": {"name": "arxiv_local-search_papers", "arguments": "{\"query\": \"BBEH benchmark reasoning\", \"max_results\": 5}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"total_results\\\": 5,\\n  \\\"papers\\\": [\\n    {\\n      \\\"id\\\": \\\"2510.14979v1\\\",\\n      \\\"title\\\": \\\"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale\\\",\\n      \\\"authors\\\": [\\n        \\\"Haiwen Diao\\\",\\n        \\\"Mingxuan Li\\\",\\n        \\\"Silei Wu\\\",\\n        \\\"Linjun Dai\\\",\\n        \\\"Xiaohua Wang\\\",\\n        \\\"Hanming Deng\\\",\\n        \\\"Lewei Lu\\\",\\n        \\\"Dahua Lin\\\",\\n        \\\"Ziwei Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\\\\ncontender to typical modular VLMs, shaped by evolving model architectures and\\\\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\\\\nexploration and promotion: (-) What fundamental constraints set native VLMs\\\\napart from modular ones, and to what extent can these barriers be overcome? (-)\\\\nHow to make research in native VLMs more accessible and democratized, thereby\\\\naccelerating progress in the field. In this paper, we clarify these challenges\\\\nand outline guiding principles for constructing native VLMs. Specifically, one\\\\nnative VLM primitive should: (i) effectively align pixel and word\\\\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\\\\nstrengths of formerly separate vision and language modules; (iii) inherently\\\\nembody various cross-modal properties that support unified vision-language\\\\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\\\\nnative VLMs built from first principles, capable of rivaling top-tier modular\\\\ncounterparts across diverse real-world scenarios. With only 390M image-text\\\\nexamples, NEO efficiently develops visual perception from scratch while\\\\nmitigating vision-language conflicts inside a dense and monolithic model\\\\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\\\\nscalable and powerful native VLMs, paired with a rich set of reusable\\\\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\\\\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14979v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14979v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14980v1\\\",\\n      \\\"title\\\": \\\"Agentic Design of Compositional Machines\\\",\\n      \\\"authors\\\": [\\n        \\\"Wenqian Zhang\\\",\\n        \\\"Weiyang Liu\\\",\\n        \\\"Zhen Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The design of complex machines stands as both a marker of human intelligence\\\\nand a foundation of engineering practice. Given recent advances in large\\\\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\\\\napproach this question through the lens of compositional machine design: a task\\\\nin which machines are assembled from standardized components to meet functional\\\\ndemands like locomotion or manipulation in a simulated physical environment. To\\\\nsupport this investigation, we introduce BesiegeField, a testbed built on the\\\\nmachine-building game Besiege, which enables part-based construction, physical\\\\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\\\\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\\\\nrequired for success, including spatial reasoning, strategic assembly, and\\\\ninstruction-following. As current open-source models fall short, we explore\\\\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\\\\ndataset, conduct RL finetuning experiments, and highlight open challenges at\\\\nthe intersection of language, machine design, and physical reasoning.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.AI\\\",\\n        \\\"cs.CL\\\",\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14980v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14980v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14978v1\\\",\\n      \\\"title\\\": \\\"Learning an Image Editing Model without Image Editing Pairs\\\",\\n      \\\"authors\\\": [\\n        \\\"Nupur Kumari\\\",\\n        \\\"Sheng-Yu Wang\\\",\\n        \\\"Nanxuan Zhao\\\",\\n        \\\"Yotam Nitzan\\\",\\n        \\\"Yuheng Li\\\",\\n        \\\"Krishna Kumar Singh\\\",\\n        \\\"Richard Zhang\\\",\\n        \\\"Eli Shechtman\\\",\\n        \\\"Jun-Yan Zhu\\\",\\n        \\\"Xun Huang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Recent image editing models have achieved impressive results while following\\\\nnatural language editing instructions, but they rely on supervised fine-tuning\\\\nwith large datasets of input-target pairs. This is a critical bottleneck, as\\\\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\\\\nuse synthetic training pairs that leverage the zero-shot capabilities of\\\\nexisting models. However, this can propagate and magnify the artifacts of the\\\\npretrained model into the final trained model. In this work, we present a new\\\\ntraining paradigm that eliminates the need for paired data entirely. Our\\\\napproach directly optimizes a few-step diffusion model by unrolling it during\\\\ntraining and leveraging feedback from vision-language models (VLMs). For each\\\\ninput and editing instruction, the VLM evaluates if an edit follows the\\\\ninstruction and preserves unchanged content, providing direct gradients for\\\\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\\\\nmatching loss (DMD), which constrains generated images to remain within the\\\\nimage manifold learned by pretrained models. We evaluate our method on standard\\\\nbenchmarks and include an extensive ablation study. Without any paired data,\\\\nour method performs on par with various image editing diffusion models trained\\\\non extensive supervised paired data, under the few-step setting. Given the same\\\\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:57+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14978v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14978v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14973v1\\\",\\n      \\\"title\\\": \\\"Attention Is All You Need for KV Cache in Diffusion LLMs\\\",\\n      \\\"authors\\\": [\\n        \\\"Quan Nguyen-Tri\\\",\\n        \\\"Mukul Ranjan\\\",\\n        \\\"Zhiqiang Shen\\\"\\n      ],\\n      \\\"abstract\\\": \\\"This work studies how to adaptively recompute key-value (KV) caches for\\\\ndiffusion large language models (DLMs) to maximize prediction accuracy while\\\\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\\\\ntokens at every denoising step and layer, despite KV states changing little\\\\nacross most steps, especially in shallow layers, leading to substantial\\\\nredundancy. We make three observations: (1) distant ${\\\\\\\\bf MASK}$ tokens\\\\nprimarily act as a length-bias and can be cached block-wise beyond the active\\\\nprediction window; (2) KV dynamics increase with depth, suggesting that\\\\nselective refresh starting from deeper layers is sufficient; and (3) the\\\\nmost-attended token exhibits the smallest KV drift, providing a conservative\\\\nlower bound on cache change for other tokens. Building on these, we propose\\\\n${\\\\\\\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\\\\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\\\\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\\\\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\\\\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\\\\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\\\\ncomputation and accelerating decoding with negligible loss in generation\\\\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\\\\nmathematical reasoning and code generation tasks demonstrate consistent\\\\nspeedups: $8.7\\\\\\\\times$ on GSM8K (256 tokens), $45.1\\\\\\\\times$ on longer sequences,\\\\nand $4.8\\\\\\\\times$ on HumanEval, while consistently maintaining higher accuracy\\\\nthan the baseline. Our method achieves significantly higher throughput\\\\n($6.8\\\\\\\\times$ on GSM8K) than existing confidence-based approaches while\\\\npreserving generation quality, enabling practical deployment of diffusion LLMs.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CL\\\",\\n        \\\"cs.AI\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:48+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14973v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14973v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14967v1\\\",\\n      \\\"title\\\": \\\"Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents\\\",\\n      \\\"authors\\\": [\\n        \\\"Guoqing Wang\\\",\\n        \\\"Sunhao Dai\\\",\\n        \\\"Guangze Ye\\\",\\n        \\\"Zeyu Gan\\\",\\n        \\\"Wei Yao\\\",\\n        \\\"Yong Deng\\\",\\n        \\\"Xiaofeng Wu\\\",\\n        \\\"Zhenzhe Ying\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Large language model (LLM)-based agents are increasingly trained with\\\\nreinforcement learning (RL) to enhance their ability to interact with external\\\\nenvironments through tool use, particularly in search-based settings that\\\\nrequire multi-turn reasoning and knowledge acquisition. However, existing\\\\napproaches typically rely on outcome-based rewards that are only provided at\\\\nthe final answer. This reward sparsity becomes particularly problematic in\\\\nmulti-turn settings, where long trajectories exacerbate two critical issues:\\\\n(i) advantage collapse, where all rollouts receive identical rewards and\\\\nprovide no useful learning signals, and (ii) lack of fine-grained credit\\\\nassignment, where dependencies between turns are obscured, especially in\\\\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\\\\nOptimization (IGPO), a simple yet effective RL framework that provides dense\\\\nand intrinsic supervision for multi-turn agent training. IGPO models each\\\\ninteraction turn as an incremental process of acquiring information about the\\\\nground truth, and defines turn-level rewards as the marginal increase in the\\\\npolicy's probability of producing the correct answer. Unlike prior\\\\nprocess-level reward approaches that depend on external reward models or costly\\\\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\\\\nmodel's own belief updates. These intrinsic turn-level rewards are combined\\\\nwith outcome-level supervision to form dense reward trajectories. Extensive\\\\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\\\\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\\\\nachieving higher accuracy and improved sample efficiency.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CL\\\",\\n        \\\"cs.AI\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:32+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14967v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14967v1\\\"\\n    }\\n  ]\\n}\",\"annotations\":null}", "tool_call_id": "toolu_0133q6DTqiAGPabBWe9MPeJf"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"total_results\\\": 5,\\n  \\\"papers\\\": [\\n    {\\n      \\\"id\\\": \\\"2510.14979v1\\\",\\n      \\\"title\\\": \\\"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale\\\",\\n      \\\"authors\\\": [\\n        \\\"Haiwen Diao\\\",\\n        \\\"Mingxuan Li\\\",\\n        \\\"Silei Wu\\\",\\n        \\\"Linjun Dai\\\",\\n        \\\"Xiaohua Wang\\\",\\n        \\\"Hanming Deng\\\",\\n        \\\"Lewei Lu\\\",\\n        \\\"Dahua Lin\\\",\\n        \\\"Ziwei Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\\\\ncontender to typical modular VLMs, shaped by evolving model architectures and\\\\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\\\\nexploration and promotion: (-) What fundamental constraints set native VLMs\\\\napart from modular ones, and to what extent can these barriers be overcome? (-)\\\\nHow to make research in native VLMs more accessible and democratized, thereby\\\\naccelerating progress in the field. In this paper, we clarify these challenges\\\\nand outline guiding principles for constructing native VLMs. Specifically, one\\\\nnative VLM primitive should: (i) effectively align pixel and word\\\\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\\\\nstrengths of formerly separate vision and language modules; (iii) inherently\\\\nembody various cross-modal properties that support unified vision-language\\\\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\\\\nnative VLMs built from first principles, capable of rivaling top-tier modular\\\\ncounterparts across diverse real-world scenarios. With only 390M image-text\\\\nexamples, NEO efficiently develops visual perception from scratch while\\\\nmitigating vision-language conflicts inside a dense and monolithic model\\\\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\\\\nscalable and powerful native VLMs, paired with a rich set of reusable\\\\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\\\\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14979v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14979v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14980v1\\\",\\n      \\\"title\\\": \\\"Agentic Design of Compositional Machines\\\",\\n      \\\"authors\\\": [\\n        \\\"Wenqian Zhang\\\",\\n        \\\"Weiyang Liu\\\",\\n        \\\"Zhen Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The design of complex machines stands as both a marker of human intelligence\\\\nand a foundation of engineering practice. Given recent advances in large\\\\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\\\\napproach this question through the lens of compositional machine design: a task\\\\nin which machines are assembled from standardized components to meet functional\\\\ndemands like locomotion or manipulation in a simulated physical environment. To\\\\nsupport this investigation, we introduce BesiegeField, a testbed built on the\\\\nmachine-building game Besiege, which enables part-based construction, physical\\\\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\\\\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\\\\nrequired for success, including spatial reasoning, strategic assembly, and\\\\ninstruction-following. As current open-source models fall short, we explore\\\\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\\\\ndataset, conduct RL finetuning experiments, and highlight open challenges at\\\\nthe intersection of language, machine design, and physical reasoning.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.AI\\\",\\n        \\\"cs.CL\\\",\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14980v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14980v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14978v1\\\",\\n      \\\"title\\\": \\\"Learning an Image Editing Model without Image Editing Pairs\\\",\\n      \\\"authors\\\": [\\n        \\\"Nupur Kumari\\\",\\n        \\\"Sheng-Yu Wang\\\",\\n        \\\"Nanxuan Zhao\\\",\\n        \\\"Yotam Nitzan\\\",\\n        \\\"Yuheng Li\\\",\\n        \\\"Krishna Kumar Singh\\\",\\n        \\\"Richard Zhang\\\",\\n        \\\"Eli Shechtman\\\",\\n        \\\"Jun-Yan Zhu\\\",\\n        \\\"Xun Huang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Recent image editing models have achieved impressive results while following\\\\nnatural language editing instructions, but they rely on supervised fine-tuning\\\\nwith large datasets of input-target pairs. This is a critical bottleneck, as\\\\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\\\\nuse synthetic training pairs that leverage the zero-shot capabilities of\\\\nexisting models. However, this can propagate and magnify the artifacts of the\\\\npretrained model into the final trained model. In this work, we present a new\\\\ntraining paradigm that eliminates the need for paired data entirely. Our\\\\napproach directly optimizes a few-step diffusion model by unrolling it during\\\\ntraining and leveraging feedback from vision-language models (VLMs). For each\\\\ninput and editing instruction, the VLM evaluates if an edit follows the\\\\ninstruction and preserves unchanged content, providing direct gradients for\\\\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\\\\nmatching loss (DMD), which constrains generated images to remain within the\\\\nimage manifold learned by pretrained models. We evaluate our method on standard\\\\nbenchmarks and include an extensive ablation study. Without any paired data,\\\\nour method performs on par with various image editing diffusion models trained\\\\non extensive supervised paired data, under the few-step setting. Given the same\\\\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:57+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14978v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14978v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14976v1\\\",\\n      \\\"title\\\": \\\"Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation\\\",\\n      \\\"authors\\\": [\\n        \\\"Shaowei Liu\\\",\\n        \\\"Chuan Guo\\\",\\n        \\\"Bing Zhou\\\",\\n        \\\"Jian Wang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Close-proximity human-human interactive poses convey rich contextual\\\\ninformation about interaction dynamics. Given such poses, humans can\\\\nintuitively infer the context and anticipate possible past and future dynamics,\\\\ndrawing on strong priors of human behavior. Inspired by this observation, we\\\\npropose Ponimator, a simple framework anchored on proximal interactive poses\\\\nfor versatile interaction animation. Our training data consists of\\\\nclose-contact two-person poses and their surrounding temporal context from\\\\nmotion-capture interaction datasets. Leveraging interactive pose priors,\\\\nPonimator employs two conditional diffusion models: (1) a pose animator that\\\\nuses the temporal prior to generate dynamic motion sequences from interactive\\\\nposes, and (2) a pose generator that applies the spatial prior to synthesize\\\\ninteractive poses from a single pose, text, or both when interactive poses are\\\\nunavailable. Collectively, Ponimator supports diverse tasks, including\\\\nimage-based interaction animation, reaction animation, and text-to-interaction\\\\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\\\\nmocap data to open-world scenarios. Empirical experiments across diverse\\\\ndatasets and applications demonstrate the universality of the pose prior and\\\\nthe effectiveness and robustness of our framework.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.RO\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:56+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14976v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14976v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14975v1\\\",\\n      \\\"title\\\": \\\"WithAnyone: Towards Controllable and ID Consistent Image Generation\\\",\\n      \\\"authors\\\": [\\n        \\\"Hengyuan Xu\\\",\\n        \\\"Wei Cheng\\\",\\n        \\\"Peng Xing\\\",\\n        \\\"Yixiao Fang\\\",\\n        \\\"Shuhan Wu\\\",\\n        \\\"Rui Wang\\\",\\n        \\\"Xianfang Zeng\\\",\\n        \\\"Daxin Jiang\\\",\\n        \\\"Gang Yu\\\",\\n        \\\"Xingjun Ma\\\",\\n        \\\"Yu-Gang Jiang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Identity-consistent generation has become an important focus in text-to-image\\\\nresearch, with recent models achieving notable success in producing images\\\\naligned with a reference identity. Yet, the scarcity of large-scale paired\\\\ndatasets containing multiple images of the same individual forces most\\\\napproaches to adopt reconstruction-based training. This reliance often leads to\\\\na failure mode we term copy-paste, where the model directly replicates the\\\\nreference face rather than preserving identity across natural variations in\\\\npose, expression, or lighting. Such over-similarity undermines controllability\\\\nand limits the expressive power of generation. To address these limitations, we\\\\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\\\\nmulti-person scenarios, providing diverse references for each identity; (2)\\\\nintroduce a benchmark that quantifies both copy-paste artifacts and the\\\\ntrade-off between identity fidelity and variation; and (3) propose a novel\\\\ntraining paradigm with a contrastive identity loss that leverages paired data\\\\nto balance fidelity with diversity. These contributions culminate in\\\\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\\\\npreserving high identity similarity. Extensive qualitative and quantitative\\\\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\\\\nartifacts, improves controllability over pose and expression, and maintains\\\\nstrong perceptual quality. User studies further validate that our method\\\\nachieves high identity fidelity while enabling expressive controllable\\\\ngeneration.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:54+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14975v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14975v1\\\"\\n    }\\n  ]\\n}\",\"annotations\":null}", "tool_call_id": "toolu_016aV7vagYEBDo36GhYVNENU"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"total_results\\\": 5,\\n  \\\"papers\\\": [\\n    {\\n      \\\"id\\\": \\\"2510.14979v1\\\",\\n      \\\"title\\\": \\\"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale\\\",\\n      \\\"authors\\\": [\\n        \\\"Haiwen Diao\\\",\\n        \\\"Mingxuan Li\\\",\\n        \\\"Silei Wu\\\",\\n        \\\"Linjun Dai\\\",\\n        \\\"Xiaohua Wang\\\",\\n        \\\"Hanming Deng\\\",\\n        \\\"Lewei Lu\\\",\\n        \\\"Dahua Lin\\\",\\n        \\\"Ziwei Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\\\\ncontender to typical modular VLMs, shaped by evolving model architectures and\\\\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\\\\nexploration and promotion: (-) What fundamental constraints set native VLMs\\\\napart from modular ones, and to what extent can these barriers be overcome? (-)\\\\nHow to make research in native VLMs more accessible and democratized, thereby\\\\naccelerating progress in the field. In this paper, we clarify these challenges\\\\nand outline guiding principles for constructing native VLMs. Specifically, one\\\\nnative VLM primitive should: (i) effectively align pixel and word\\\\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\\\\nstrengths of formerly separate vision and language modules; (iii) inherently\\\\nembody various cross-modal properties that support unified vision-language\\\\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\\\\nnative VLMs built from first principles, capable of rivaling top-tier modular\\\\ncounterparts across diverse real-world scenarios. With only 390M image-text\\\\nexamples, NEO efficiently develops visual perception from scratch while\\\\nmitigating vision-language conflicts inside a dense and monolithic model\\\\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\\\\nscalable and powerful native VLMs, paired with a rich set of reusable\\\\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\\\\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14979v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14979v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14980v1\\\",\\n      \\\"title\\\": \\\"Agentic Design of Compositional Machines\\\",\\n      \\\"authors\\\": [\\n        \\\"Wenqian Zhang\\\",\\n        \\\"Weiyang Liu\\\",\\n        \\\"Zhen Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The design of complex machines stands as both a marker of human intelligence\\\\nand a foundation of engineering practice. Given recent advances in large\\\\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\\\\napproach this question through the lens of compositional machine design: a task\\\\nin which machines are assembled from standardized components to meet functional\\\\ndemands like locomotion or manipulation in a simulated physical environment. To\\\\nsupport this investigation, we introduce BesiegeField, a testbed built on the\\\\nmachine-building game Besiege, which enables part-based construction, physical\\\\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\\\\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\\\\nrequired for success, including spatial reasoning, strategic assembly, and\\\\ninstruction-following. As current open-source models fall short, we explore\\\\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\\\\ndataset, conduct RL finetuning experiments, and highlight open challenges at\\\\nthe intersection of language, machine design, and physical reasoning.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.AI\\\",\\n        \\\"cs.CL\\\",\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14980v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14980v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14978v1\\\",\\n      \\\"title\\\": \\\"Learning an Image Editing Model without Image Editing Pairs\\\",\\n      \\\"authors\\\": [\\n        \\\"Nupur Kumari\\\",\\n        \\\"Sheng-Yu Wang\\\",\\n        \\\"Nanxuan Zhao\\\",\\n        \\\"Yotam Nitzan\\\",\\n        \\\"Yuheng Li\\\",\\n        \\\"Krishna Kumar Singh\\\",\\n        \\\"Richard Zhang\\\",\\n        \\\"Eli Shechtman\\\",\\n        \\\"Jun-Yan Zhu\\\",\\n        \\\"Xun Huang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Recent image editing models have achieved impressive results while following\\\\nnatural language editing instructions, but they rely on supervised fine-tuning\\\\nwith large datasets of input-target pairs. This is a critical bottleneck, as\\\\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\\\\nuse synthetic training pairs that leverage the zero-shot capabilities of\\\\nexisting models. However, this can propagate and magnify the artifacts of the\\\\npretrained model into the final trained model. In this work, we present a new\\\\ntraining paradigm that eliminates the need for paired data entirely. Our\\\\napproach directly optimizes a few-step diffusion model by unrolling it during\\\\ntraining and leveraging feedback from vision-language models (VLMs). For each\\\\ninput and editing instruction, the VLM evaluates if an edit follows the\\\\ninstruction and preserves unchanged content, providing direct gradients for\\\\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\\\\nmatching loss (DMD), which constrains generated images to remain within the\\\\nimage manifold learned by pretrained models. We evaluate our method on standard\\\\nbenchmarks and include an extensive ablation study. Without any paired data,\\\\nour method performs on par with various image editing diffusion models trained\\\\non extensive supervised paired data, under the few-step setting. Given the same\\\\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:57+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14978v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14978v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14976v1\\\",\\n      \\\"title\\\": \\\"Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation\\\",\\n      \\\"authors\\\": [\\n        \\\"Shaowei Liu\\\",\\n        \\\"Chuan Guo\\\",\\n        \\\"Bing Zhou\\\",\\n        \\\"Jian Wang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Close-proximity human-human interactive poses convey rich contextual\\\\ninformation about interaction dynamics. Given such poses, humans can\\\\nintuitively infer the context and anticipate possible past and future dynamics,\\\\ndrawing on strong priors of human behavior. Inspired by this observation, we\\\\npropose Ponimator, a simple framework anchored on proximal interactive poses\\\\nfor versatile interaction animation. Our training data consists of\\\\nclose-contact two-person poses and their surrounding temporal context from\\\\nmotion-capture interaction datasets. Leveraging interactive pose priors,\\\\nPonimator employs two conditional diffusion models: (1) a pose animator that\\\\nuses the temporal prior to generate dynamic motion sequences from interactive\\\\nposes, and (2) a pose generator that applies the spatial prior to synthesize\\\\ninteractive poses from a single pose, text, or both when interactive poses are\\\\nunavailable. Collectively, Ponimator supports diverse tasks, including\\\\nimage-based interaction animation, reaction animation, and text-to-interaction\\\\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\\\\nmocap data to open-world scenarios. Empirical experiments across diverse\\\\ndatasets and applications demonstrate the universality of the pose prior and\\\\nthe effectiveness and robustness of our framework.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.RO\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:56+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14976v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14976v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14975v1\\\",\\n      \\\"title\\\": \\\"WithAnyone: Towards Controllable and ID Consistent Image Generation\\\",\\n      \\\"authors\\\": [\\n        \\\"Hengyuan Xu\\\",\\n        \\\"Wei Cheng\\\",\\n        \\\"Peng Xing\\\",\\n        \\\"Yixiao Fang\\\",\\n        \\\"Shuhan Wu\\\",\\n        \\\"Rui Wang\\\",\\n        \\\"Xianfang Zeng\\\",\\n        \\\"Daxin Jiang\\\",\\n        \\\"Gang Yu\\\",\\n        \\\"Xingjun Ma\\\",\\n        \\\"Yu-Gang Jiang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Identity-consistent generation has become an important focus in text-to-image\\\\nresearch, with recent models achieving notable success in producing images\\\\naligned with a reference identity. Yet, the scarcity of large-scale paired\\\\ndatasets containing multiple images of the same individual forces most\\\\napproaches to adopt reconstruction-based training. This reliance often leads to\\\\na failure mode we term copy-paste, where the model directly replicates the\\\\nreference face rather than preserving identity across natural variations in\\\\npose, expression, or lighting. Such over-similarity undermines controllability\\\\nand limits the expressive power of generation. To address these limitations, we\\\\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\\\\nmulti-person scenarios, providing diverse references for each identity; (2)\\\\nintroduce a benchmark that quantifies both copy-paste artifacts and the\\\\ntrade-off between identity fidelity and variation; and (3) propose a novel\\\\ntraining paradigm with a contrastive identity loss that leverages paired data\\\\nto balance fidelity with diversity. These contributions culminate in\\\\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\\\\npreserving high identity similarity. Extensive qualitative and quantitative\\\\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\\\\nartifacts, improves controllability over pose and expression, and maintains\\\\nstrong perceptual quality. User studies further validate that our method\\\\nachieves high identity fidelity while enabling expressive controllable\\\\ngeneration.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:54+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14975v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14975v1\\\"\\n    }\\n  ]\\n}\",\"annotations\":null}", "tool_call_id": "toolu_01UKqpdNVvciQqvKUvzGfod9"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"message\\\": \\\"Paper downloaded, conversion started\\\", \\\"started_at\\\": \\\"2025-10-17T23:00:13.017483\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01URmR82cv3uCxF4UkcVj5YT"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"total_results\\\": 5,\\n  \\\"papers\\\": [\\n    {\\n      \\\"id\\\": \\\"2510.14979v1\\\",\\n      \\\"title\\\": \\\"From Pixels to Words -- Towards Native Vision-Language Primitives at Scale\\\",\\n      \\\"authors\\\": [\\n        \\\"Haiwen Diao\\\",\\n        \\\"Mingxuan Li\\\",\\n        \\\"Silei Wu\\\",\\n        \\\"Linjun Dai\\\",\\n        \\\"Xiaohua Wang\\\",\\n        \\\"Hanming Deng\\\",\\n        \\\"Lewei Lu\\\",\\n        \\\"Dahua Lin\\\",\\n        \\\"Ziwei Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\\\\ncontender to typical modular VLMs, shaped by evolving model architectures and\\\\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\\\\nexploration and promotion: (-) What fundamental constraints set native VLMs\\\\napart from modular ones, and to what extent can these barriers be overcome? (-)\\\\nHow to make research in native VLMs more accessible and democratized, thereby\\\\naccelerating progress in the field. In this paper, we clarify these challenges\\\\nand outline guiding principles for constructing native VLMs. Specifically, one\\\\nnative VLM primitive should: (i) effectively align pixel and word\\\\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\\\\nstrengths of formerly separate vision and language modules; (iii) inherently\\\\nembody various cross-modal properties that support unified vision-language\\\\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\\\\nnative VLMs built from first principles, capable of rivaling top-tier modular\\\\ncounterparts across diverse real-world scenarios. With only 390M image-text\\\\nexamples, NEO efficiently develops visual perception from scratch while\\\\nmitigating vision-language conflicts inside a dense and monolithic model\\\\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\\\\nscalable and powerful native VLMs, paired with a rich set of reusable\\\\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\\\\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14979v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14979v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14980v1\\\",\\n      \\\"title\\\": \\\"Agentic Design of Compositional Machines\\\",\\n      \\\"authors\\\": [\\n        \\\"Wenqian Zhang\\\",\\n        \\\"Weiyang Liu\\\",\\n        \\\"Zhen Liu\\\"\\n      ],\\n      \\\"abstract\\\": \\\"The design of complex machines stands as both a marker of human intelligence\\\\nand a foundation of engineering practice. Given recent advances in large\\\\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\\\\napproach this question through the lens of compositional machine design: a task\\\\nin which machines are assembled from standardized components to meet functional\\\\ndemands like locomotion or manipulation in a simulated physical environment. To\\\\nsupport this investigation, we introduce BesiegeField, a testbed built on the\\\\nmachine-building game Besiege, which enables part-based construction, physical\\\\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\\\\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\\\\nrequired for success, including spatial reasoning, strategic assembly, and\\\\ninstruction-following. As current open-source models fall short, we explore\\\\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\\\\ndataset, conduct RL finetuning experiments, and highlight open challenges at\\\\nthe intersection of language, machine design, and physical reasoning.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.AI\\\",\\n        \\\"cs.CL\\\",\\n        \\\"cs.CV\\\",\\n        \\\"cs.GR\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:58+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14980v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14980v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14978v1\\\",\\n      \\\"title\\\": \\\"Learning an Image Editing Model without Image Editing Pairs\\\",\\n      \\\"authors\\\": [\\n        \\\"Nupur Kumari\\\",\\n        \\\"Sheng-Yu Wang\\\",\\n        \\\"Nanxuan Zhao\\\",\\n        \\\"Yotam Nitzan\\\",\\n        \\\"Yuheng Li\\\",\\n        \\\"Krishna Kumar Singh\\\",\\n        \\\"Richard Zhang\\\",\\n        \\\"Eli Shechtman\\\",\\n        \\\"Jun-Yan Zhu\\\",\\n        \\\"Xun Huang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Recent image editing models have achieved impressive results while following\\\\nnatural language editing instructions, but they rely on supervised fine-tuning\\\\nwith large datasets of input-target pairs. This is a critical bottleneck, as\\\\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\\\\nuse synthetic training pairs that leverage the zero-shot capabilities of\\\\nexisting models. However, this can propagate and magnify the artifacts of the\\\\npretrained model into the final trained model. In this work, we present a new\\\\ntraining paradigm that eliminates the need for paired data entirely. Our\\\\napproach directly optimizes a few-step diffusion model by unrolling it during\\\\ntraining and leveraging feedback from vision-language models (VLMs). For each\\\\ninput and editing instruction, the VLM evaluates if an edit follows the\\\\ninstruction and preserves unchanged content, providing direct gradients for\\\\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\\\\nmatching loss (DMD), which constrains generated images to remain within the\\\\nimage manifold learned by pretrained models. We evaluate our method on standard\\\\nbenchmarks and include an extensive ablation study. Without any paired data,\\\\nour method performs on par with various image editing diffusion models trained\\\\non extensive supervised paired data, under the few-step setting. Given the same\\\\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:57+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14978v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14978v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14975v1\\\",\\n      \\\"title\\\": \\\"WithAnyone: Towards Controllable and ID Consistent Image Generation\\\",\\n      \\\"authors\\\": [\\n        \\\"Hengyuan Xu\\\",\\n        \\\"Wei Cheng\\\",\\n        \\\"Peng Xing\\\",\\n        \\\"Yixiao Fang\\\",\\n        \\\"Shuhan Wu\\\",\\n        \\\"Rui Wang\\\",\\n        \\\"Xianfang Zeng\\\",\\n        \\\"Daxin Jiang\\\",\\n        \\\"Gang Yu\\\",\\n        \\\"Xingjun Ma\\\",\\n        \\\"Yu-Gang Jiang\\\"\\n      ],\\n      \\\"abstract\\\": \\\"Identity-consistent generation has become an important focus in text-to-image\\\\nresearch, with recent models achieving notable success in producing images\\\\naligned with a reference identity. Yet, the scarcity of large-scale paired\\\\ndatasets containing multiple images of the same individual forces most\\\\napproaches to adopt reconstruction-based training. This reliance often leads to\\\\na failure mode we term copy-paste, where the model directly replicates the\\\\nreference face rather than preserving identity across natural variations in\\\\npose, expression, or lighting. Such over-similarity undermines controllability\\\\nand limits the expressive power of generation. To address these limitations, we\\\\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\\\\nmulti-person scenarios, providing diverse references for each identity; (2)\\\\nintroduce a benchmark that quantifies both copy-paste artifacts and the\\\\ntrade-off between identity fidelity and variation; and (3) propose a novel\\\\ntraining paradigm with a contrastive identity loss that leverages paired data\\\\nto balance fidelity with diversity. These contributions culminate in\\\\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\\\\npreserving high identity similarity. Extensive qualitative and quantitative\\\\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\\\\nartifacts, improves controllability over pose and expression, and maintains\\\\nstrong perceptual quality. User studies further validate that our method\\\\nachieves high identity fidelity while enabling expressive controllable\\\\ngeneration.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CV\\\",\\n        \\\"cs.AI\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:54+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14975v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14975v1\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"2510.14973v1\\\",\\n      \\\"title\\\": \\\"Attention Is All You Need for KV Cache in Diffusion LLMs\\\",\\n      \\\"authors\\\": [\\n        \\\"Quan Nguyen-Tri\\\",\\n        \\\"Mukul Ranjan\\\",\\n        \\\"Zhiqiang Shen\\\"\\n      ],\\n      \\\"abstract\\\": \\\"This work studies how to adaptively recompute key-value (KV) caches for\\\\ndiffusion large language models (DLMs) to maximize prediction accuracy while\\\\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\\\\ntokens at every denoising step and layer, despite KV states changing little\\\\nacross most steps, especially in shallow layers, leading to substantial\\\\nredundancy. We make three observations: (1) distant ${\\\\\\\\bf MASK}$ tokens\\\\nprimarily act as a length-bias and can be cached block-wise beyond the active\\\\nprediction window; (2) KV dynamics increase with depth, suggesting that\\\\nselective refresh starting from deeper layers is sufficient; and (3) the\\\\nmost-attended token exhibits the smallest KV drift, providing a conservative\\\\nlower bound on cache change for other tokens. Building on these, we propose\\\\n${\\\\\\\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\\\\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\\\\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\\\\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\\\\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\\\\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\\\\ncomputation and accelerating decoding with negligible loss in generation\\\\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\\\\nmathematical reasoning and code generation tasks demonstrate consistent\\\\nspeedups: $8.7\\\\\\\\times$ on GSM8K (256 tokens), $45.1\\\\\\\\times$ on longer sequences,\\\\nand $4.8\\\\\\\\times$ on HumanEval, while consistently maintaining higher accuracy\\\\nthan the baseline. Our method achieves significantly higher throughput\\\\n($6.8\\\\\\\\times$ on GSM8K) than existing confidence-based approaches while\\\\npreserving generation quality, enabling practical deployment of diffusion LLMs.\\\",\\n      \\\"categories\\\": [\\n        \\\"cs.CL\\\",\\n        \\\"cs.AI\\\",\\n        \\\"cs.LG\\\"\\n      ],\\n      \\\"published\\\": \\\"2025-10-16T17:59:48+00:00\\\",\\n      \\\"url\\\": \\\"http://arxiv.org/pdf/2510.14973v1\\\",\\n      \\\"resource_uri\\\": \\\"arxiv://2510.14973v1\\\"\\n    }\\n  ]\\n}\",\"annotations\":null}", "tool_call_id": "toolu_016bZ9TKczNHRdJEqWivxSxM"}, {"role": "assistant", "content": "Let me search more specifically for these datasets and also check on the K&K paper:", "tool_calls": [{"id": "toolu_01RjQSBahBMMcu3D1bAgkV1a", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"BBH Big-Bench Hard dataset tasks training set\", \"num_results\": 5}"}}, {"id": "toolu_01DomXYG1eck7k8qmWqjtS9A", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"Zebra Logic puzzle dataset training difficulty levels\", \"num_results\": 5}"}}, {"id": "toolu_01A5NExiDk2XMBDoK1VAHAKp", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"KOR-Bench knowledge reasoning benchmark tasks categories\", \"num_results\": 5}"}}, {"id": "toolu_01EgdtVQwkCTid3Mh5sVcz9V", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"BBEH benchmark dataset reasoning tasks\", \"num_results\": 5}"}}, {"id": "toolu_01Tvp5ubgEZtUzvr4kUYdLZe", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\", \"check_status\": true}"}}]}, {"role": "tool", "content": "Title: suzgunmirac/BIG-Bench-Hard - GitHub\nLink: https://github.com/suzgunmirac/BIG-Bench-Hard\nSnippet: We focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not ...\nSitelinks: No sitelinks\n\nTitle: maveriq/bigbenchhard \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/maveriq/bigbenchhard\nSnippet: This is a subset of BIG Bench dataset and consists of 23 tasks that are particularly hard for current generation of language models.\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Extra Hard - arXiv\nLink: https://arxiv.org/html/2502.19187v1\nSnippet: We create BIG-Bench Extra Hard (BBEH), a dataset that tests the general reasoning capability of models on a wide array of reasoning skills. To ...\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Hard - The Open-Source LLM Evaluation Framework\nLink: https://deepeval.com/docs/benchmarks-big-bench-hard\nSnippet: The BIG-Bench Hard (BBH) benchmark comprises 23 challenging BIG-Bench tasks where prior language model evaluations have not outperformed the average human ...\nSitelinks: No sitelinks\n\nTitle: \"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can ...\nLink: https://www.reddit.com/r/mlscaling/comments/y6tzck/challenging_bigbench_tasks_and_whether/\nSnippet: I feel like BB tasks are supposed to be used as an evaluation set, and this is pretty much tuning to the test set. A lot of the BBH tasks are ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01RjQSBahBMMcu3D1bAgkV1a"}, {"role": "tool", "content": "Title: ZebraLogic: Benchmarking the Logical Reasoning Ability of ...\nLink: https://huggingface.co/blog/yuchenlin/zebra-logic\nSnippet: We set a threshold for the logarithmic value and put all puzzles easier than 3x3 are considered easy, while the others are considered hard.\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\nLink: https://arxiv.org/html/2502.01100v1\nSnippet: We create the ZebraLogic dataset, a benchmark of 1,000 logic grid puzzles spanning multiple complexity levels, designed to evaluate LLMs' ...\nSitelinks: No sitelinks\n\nTitle: Challenging Large Reasoning Models with Long-tail Logic Puzzle ...\nLink: https://arxiv.org/html/2510.12563v1\nSnippet: We introduced various puzzles, increased the difficulty limit of logical puzzles, performed long-tail transformation on puzzles from multiple ...\nSitelinks: No sitelinks\n\nTitle: Meet ZebraLogic: A Comprehensive AI Evaluation Framework for ...\nLink: https://www.reddit.com/r/machinelearningnews/comments/1ikwikd/meet_zebralogic_a_comprehensive_ai_evaluation/\nSnippet: The ZebraLogic framework constructs logic puzzles with varying difficulty levels based on two primary complexity measures: search space size ...\nSitelinks: No sitelinks\n\nTitle: [PDF] Probing the Limits of Reasoning Abilities in Large Language Models\nLink: https://aclanthology.org/2025.acl-long.664.pdf\nSnippet: It includes 4,250 newly generated puzzles in 12 for- mal difficulty classes and 42 sizes. While existing datasets control difficulty mainly based on the grid.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01DomXYG1eck7k8qmWqjtS9A"}, {"role": "tool", "content": "Title: KOR-Bench\nLink: https://kor-bench.github.io/\nSnippet: KOR-Bench consists of five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. Leading models, such as Claude-3.5-Sonnet and GPT-4o, score ...\nSitelinks: No sitelinks\n\nTitle: [2410.06526] KOR-Bench: Benchmarking Language Models ... - arXiv\nLink: https://arxiv.org/abs/2410.06526\nSnippet: We propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual.\nSitelinks: No sitelinks\n\nTitle: KOR-Bench: Benchmarking Language Models on Knowledge ...\nLink: https://huggingface.co/papers/2410.06526\nSnippet: Based on this concept, we propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, ...\nSitelinks: No sitelinks\n\nTitle: [PDF] kor-bench: benchmarking language models - ICLR Proceedings\nLink: https://proceedings.iclr.cc/paper_files/paper/2025/file/c6f5851a0d9cb435ed8b50e87bd6a257-Paper-Conference.pdf\nSnippet: These tasks are divided into five categories, each based on one of the following new elements: new symbols, new concepts, new execution rules, new problem-.\nSitelinks: No sitelinks\n\nTitle: Korean Reasoning Benchmarks: HRM8K & HRMCR\nLink: https://onelineai.com/blog/korean-reasoning-benchmarks\nSnippet: The following table describes highly-used benchmarks to evaluate LLMs' Korean capabilities with 3 primary categories: Knowledge, Fluency, and ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01A5NExiDk2XMBDoK1VAHAKp"}, {"role": "tool", "content": "Title: [2502.19187] BIG-Bench Extra Hard - arXiv\nLink: https://arxiv.org/abs/2502.19187\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: google-deepmind/bbeh - GitHub\nLink: https://github.com/google-deepmind/bbeh\nSnippet: A new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning ...\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Extra Hard - arXiv\nLink: https://arxiv.org/html/2502.19187v1\nSnippet: We create BIG-Bench Extra Hard (BBEH), a dataset that tests the general reasoning capability of models on a wide array of reasoning skills. To ...\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Extra Hard - ACL Anthology\nLink: https://aclanthology.org/2025.acl-long.1285/\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Extra Hard (BBEH) \u2014 Paper Review | by Sulbha Jain\nLink: https://medium.com/@sulbha.jindal/big-bench-extra-hard-bbeh-paper-review-a61d667c3871\nSnippet: The introduction of BIG-Bench Extra Hard (BBEH) marks a pivotal step forward in assessing the reasoning capabilities of large language models (LLMs).\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01EgdtVQwkCTid3Mh5sVcz9V"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"started_at\\\": \\\"2025-10-17T23:00:13.017483\\\", \\\"completed_at\\\": null, \\\"error\\\": null, \\\"message\\\": \\\"Paper conversion converting\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01Tvp5ubgEZtUzvr4kUYdLZe"}, {"role": "assistant", "content": "Let me read the K&K paper and get more details about the datasets:", "tool_calls": [{"id": "toolu_01FEM51GzJdNxJs8eKW3bNiK", "type": "function", "function": {"name": "fetch-fetch_markdown", "arguments": "{\"url\": \"https://arxiv.org/abs/2410.23123\"}"}}, {"id": "toolu_013wyVf8u3FFMPpX2Cn5fRYm", "type": "function", "function": {"name": "fetch-fetch_markdown", "arguments": "{\"url\": \"https://arxiv.org/abs/2410.06526\"}"}}, {"id": "toolu_01GvC66RE56KzrbDh3YBE6aQ", "type": "function", "function": {"name": "fetch-fetch_markdown", "arguments": "{\"url\": \"https://arxiv.org/abs/2502.19187\"}"}}, {"id": "toolu_01JF6V9W65BQBaxA9jijdp3y", "type": "function", "function": {"name": "fetch-fetch_markdown", "arguments": "{\"url\": \"https://arxiv.org/abs/2502.01100\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\\\[2410.23123\\\\] On Memorization of Large Language Models in Logical Reasoning                              \\n\\n[Skip to main content](#content)\\n\\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\\n\\n[](/IgnoreMe)\\n\\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) \\\\> [cs](/list/cs/recent) \\\\> arXiv:2410.23123\\n\\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\\n\\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\\n\\n Search\\n\\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\\n\\n  [![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nopen search\\n\\n   GO\\n\\nopen navigation menu\\n\\nquick links\\n-----------\\n\\n*   [Login](https://arxiv.org/login)\\n*   [Help Pages](https://info.arxiv.org/help)\\n*   [About](https://info.arxiv.org/about)\\n\\nComputer Science > Computation and Language\\n===========================================\\n\\n**arXiv:2410.23123** (cs)\\n\\n\\\\[Submitted on 30 Oct 2024 ([v1](https://arxiv.org/abs/2410.23123v1)), last revised 4 Mar 2025 (this version, v2)\\\\]\\n\\nTitle:On Memorization of Large Language Models in Logical Reasoning\\n===================================================================\\n\\nAuthors:[Chulin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+C), [Yangsibo Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+Y), [Chiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C), [Da Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+D), [Xinyun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Bill Yuchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+B+Y), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Badih Ghazi](https://arxiv.org/search/cs?searchtype=author&query=Ghazi,+B), [Ravi Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar,+R)\\n\\nView a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors\\n\\n[View PDF](/pdf/2410.23123) [HTML (experimental)](https://arxiv.org/html/2410.23123v2)\\n\\n> Abstract:Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at [this https URL](https://memkklogic.github.io).\\n\\nSubjects:\\n\\nComputation and Language (cs.CL)\\n\\nCite as:\\n\\n[arXiv:2410.23123](https://arxiv.org/abs/2410.23123) \\\\[cs.CL\\\\]\\n\\n\u00a0\\n\\n(or [arXiv:2410.23123v2](https://arxiv.org/abs/2410.23123v2) \\\\[cs.CL\\\\] for this version)\\n\\n\u00a0\\n\\n[https://doi.org/10.48550/arXiv.2410.23123](https://doi.org/10.48550/arXiv.2410.23123)\\n\\nFocus to learn more\\n\\narXiv-issued DOI via DataCite\\n\\nSubmission history\\n------------------\\n\\nFrom: Chulin Xie \\\\[[view email](/show-email/d1541218/2410.23123)\\\\]  \\n**[\\\\[v1\\\\]](/abs/2410.23123v1)** Wed, 30 Oct 2024 15:31:54 UTC (8,530 KB)  \\n**\\\\[v2\\\\]** Tue, 4 Mar 2025 06:22:40 UTC (9,623 KB)  \\n\\nFull-text links:\\n\\nAccess Paper:\\n-------------\\n\\nView a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors\\n\\n*   [View PDF](/pdf/2410.23123)\\n*   [HTML (experimental)](https://arxiv.org/html/2410.23123v2)\\n*   [TeX Source](/src/2410.23123)\\n*   [Other Formats](/format/2410.23123)\\n\\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\\"Rights to this article\\\")\\n\\nCurrent browse context:\\n\\ncs.CL\\n\\n[<\u00a0prev](/prevnext?id=2410.23123&function=prev&context=cs.CL \\\"previous in cs.CL (accesskey p)\\\") \u00a0 | \u00a0 [next\u00a0>](/prevnext?id=2410.23123&function=next&context=cs.CL \\\"next in cs.CL (accesskey n)\\\")  \\n\\n[new](/list/cs.CL/new) | [recent](/list/cs.CL/recent) | [2024-10](/list/cs.CL/2024-10)\\n\\nChange to browse by:\\n\\n[cs](/abs/2410.23123?context=cs)  \\n\\n### References & Citations\\n\\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.23123)\\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.23123)\\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.23123)\\n\\nexport BibTeX citation Loading...\\n\\nBibTeX formatted citation\\n-------------------------\\n\\n\u00d7\\n\\nloading...\\n\\nData provided by:\\n\\n### Bookmark\\n\\n [![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.23123&description=On Memorization of Large Language Models in Logical Reasoning \\\"Bookmark on BibSonomy\\\")[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.23123&title=On Memorization of Large Language Models in Logical Reasoning \\\"Bookmark on Reddit\\\")\\n\\n Bibliographic Tools\\n\\nBibliographic and Citation Tools\\n================================\\n\\n Bibliographic Explorer Toggle\\n\\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\\n\\n Connected Papers Toggle\\n\\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\\n\\n Litmaps Toggle\\n\\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\\n\\n scite.ai Toggle\\n\\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\\n\\n Code, Data, Media\\n\\nCode, Data and Media Associated with this Article\\n=================================================\\n\\n alphaXiv Toggle\\n\\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\\n\\n Links to Code Toggle\\n\\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\\n\\n DagsHub Toggle\\n\\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\\n\\n GotitPub Toggle\\n\\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\\n\\n Huggingface Toggle\\n\\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\\n\\n Links to Code Toggle\\n\\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\\n\\n ScienceCast Toggle\\n\\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\\n\\n Demos\\n\\nDemos\\n=====\\n\\n Replicate Toggle\\n\\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\\n\\n Spaces Toggle\\n\\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\\n\\n Spaces Toggle\\n\\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\\n\\n Related Papers\\n\\nRecommenders and Search Tools\\n=============================\\n\\n Link to Influence Flower\\n\\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\\n\\n Core recommender toggle\\n\\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\\n\\n*   Author\\n*   Venue\\n*   Institution\\n*   Topic\\n\\n About arXivLabs\\n\\narXivLabs: experimental projects with community collaborators\\n=============================================================\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\\n\\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2410.23123) | [Disable MathJax](javascript:setMathjaxCookie\\\\(\\\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))\\n\\nmathjaxToggle();\\n\\n*   [About](https://info.arxiv.org/about)\\n*   [Help](https://info.arxiv.org/help)\\n\\n*   contact arXivClick here to contact arXiv [Contact](https://info.arxiv.org/help/contact.html)\\n*   subscribe to arXiv mailingsClick here to subscribe [Subscribe](https://info.arxiv.org/help/subscribe)\\n\\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\\n\\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\\n*   [arXiv Operational Status](https://status.arxiv.org)  \\n    Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\",\"annotations\":null}", "tool_call_id": "toolu_01FEM51GzJdNxJs8eKW3bNiK"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\\\[2410.06526\\\\] KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks                              \\n\\n[Skip to main content](#content)\\n\\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\\n\\n[](/IgnoreMe)\\n\\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) \\\\> [cs](/list/cs/recent) \\\\> arXiv:2410.06526\\n\\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\\n\\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\\n\\n Search\\n\\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\\n\\n  [![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nopen search\\n\\n   GO\\n\\nopen navigation menu\\n\\nquick links\\n-----------\\n\\n*   [Login](https://arxiv.org/login)\\n*   [Help Pages](https://info.arxiv.org/help)\\n*   [About](https://info.arxiv.org/about)\\n\\nComputer Science > Databases\\n============================\\n\\n**arXiv:2410.06526** (cs)\\n\\n\\\\[Submitted on 9 Oct 2024 ([v1](https://arxiv.org/abs/2410.06526v1)), last revised 1 Mar 2025 (this version, v3)\\\\]\\n\\nTitle:KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks\\n=====================================================================================\\n\\nAuthors:[Kaijing Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+K), [Xinrun Du](https://arxiv.org/search/cs?searchtype=author&query=Du,+X), [Yunran Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Haoran Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Zhoufutu Wen](https://arxiv.org/search/cs?searchtype=author&query=Wen,+Z), [Xingwei Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu,+X), [Jian Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+J), [Jiaheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+J), [Minghao Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+M), [Xiang Yue](https://arxiv.org/search/cs?searchtype=author&query=Yue,+X), [Wenhao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+W), [Ge Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+G)\\n\\nView a PDF of the paper titled KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks, by Kaijing Ma and 11 other authors\\n\\n[View PDF](/pdf/2410.06526)\\n\\n> Abstract:In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), a concept aimed at minimizing reliance on domain-specific knowledge, enabling more accurate evaluation of models' reasoning abilities in out-of-distribution settings. Based on this concept, we propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes models' effectiveness in applying new rule descriptions to solve novel rule-driven questions. O1-Preview and O1-Mini achieve accuracies of 72.88% and 70.16%, surpassing Claude-3.5-Sonnet and GPT-4o (58.96% and 58.00%), highlighting the effectiveness of KOR-Bench. We perform detailed analyses, identifying bottlenecks in the Cipher task with Stepwise Prompting, where two rounds of Self-Correction yield optimal results. We evaluate performance across three integrated tasks, explore the impact of Tricks on the Puzzle task, and visualize rule-focused attention. Additionally, we conduct an ablation study on dataset size, benchmark correlations, and zero-shot and three-shot \\\"only questions\\\" experiments. KOR-Bench aims to enhance reasoning evaluation and support further research in this area.\\n\\nSubjects:\\n\\nDatabases (cs.DB)\\n\\nCite as:\\n\\n[arXiv:2410.06526](https://arxiv.org/abs/2410.06526) \\\\[cs.DB\\\\]\\n\\n\u00a0\\n\\n(or [arXiv:2410.06526v3](https://arxiv.org/abs/2410.06526v3) \\\\[cs.DB\\\\] for this version)\\n\\n\u00a0\\n\\n[https://doi.org/10.48550/arXiv.2410.06526](https://doi.org/10.48550/arXiv.2410.06526)\\n\\nFocus to learn more\\n\\narXiv-issued DOI via DataCite\\n\\nSubmission history\\n------------------\\n\\nFrom: Kaijing Ma \\\\[[view email](/show-email/539e195e/2410.06526)\\\\]  \\n**[\\\\[v1\\\\]](/abs/2410.06526v1)** Wed, 9 Oct 2024 03:56:50 UTC (2,109 KB)  \\n**[\\\\[v2\\\\]](/abs/2410.06526v2)** Fri, 18 Oct 2024 03:51:29 UTC (2,110 KB)  \\n**\\\\[v3\\\\]** Sat, 1 Mar 2025 12:34:10 UTC (6,535 KB)  \\n\\nFull-text links:\\n\\nAccess Paper:\\n-------------\\n\\nView a PDF of the paper titled KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks, by Kaijing Ma and 11 other authors\\n\\n*   [View PDF](/pdf/2410.06526)\\n*   [TeX Source](/src/2410.06526)\\n\\n [![license icon](https://arxiv.org/icons/licenses/by-4.0.png) view license](http://creativecommons.org/licenses/by/4.0/ \\\"Rights to this article\\\")\\n\\nCurrent browse context:\\n\\ncs.DB\\n\\n[<\u00a0prev](/prevnext?id=2410.06526&function=prev&context=cs.DB \\\"previous in cs.DB (accesskey p)\\\") \u00a0 | \u00a0 [next\u00a0>](/prevnext?id=2410.06526&function=next&context=cs.DB \\\"next in cs.DB (accesskey n)\\\")  \\n\\n[new](/list/cs.DB/new) | [recent](/list/cs.DB/recent) | [2024-10](/list/cs.DB/2024-10)\\n\\nChange to browse by:\\n\\n[cs](/abs/2410.06526?context=cs)  \\n\\n### References & Citations\\n\\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.06526)\\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.06526)\\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.06526)\\n\\nexport BibTeX citation Loading...\\n\\nBibTeX formatted citation\\n-------------------------\\n\\n\u00d7\\n\\nloading...\\n\\nData provided by:\\n\\n### Bookmark\\n\\n [![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.06526&description=KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks \\\"Bookmark on BibSonomy\\\")[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.06526&title=KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks \\\"Bookmark on Reddit\\\")\\n\\n Bibliographic Tools\\n\\nBibliographic and Citation Tools\\n================================\\n\\n Bibliographic Explorer Toggle\\n\\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\\n\\n Connected Papers Toggle\\n\\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\\n\\n Litmaps Toggle\\n\\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\\n\\n scite.ai Toggle\\n\\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\\n\\n Code, Data, Media\\n\\nCode, Data and Media Associated with this Article\\n=================================================\\n\\n alphaXiv Toggle\\n\\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\\n\\n Links to Code Toggle\\n\\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\\n\\n DagsHub Toggle\\n\\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\\n\\n GotitPub Toggle\\n\\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\\n\\n Huggingface Toggle\\n\\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\\n\\n Links to Code Toggle\\n\\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\\n\\n ScienceCast Toggle\\n\\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\\n\\n Demos\\n\\nDemos\\n=====\\n\\n Replicate Toggle\\n\\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\\n\\n Spaces Toggle\\n\\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\\n\\n Spaces Toggle\\n\\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\\n\\n Related Papers\\n\\nRecommenders and Search Tools\\n=============================\\n\\n Link to Influence Flower\\n\\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\\n\\n Core recommender toggle\\n\\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\\n\\n*   Author\\n*   Venue\\n*   Institution\\n*   Topic\\n\\n About arXivLabs\\n\\narXivLabs: experimental projects with community collaborators\\n=============================================================\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\\n\\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2410.06526) | [Disable MathJax](javascript:setMathjaxCookie\\\\(\\\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))\\n\\nmathjaxToggle();\\n\\n*   [About](https://info.arxiv.org/about)\\n*   [Help](https://info.arxiv.org/help)\\n\\n*   contact arXivClick here to contact arXiv [Contact](https://info.arxiv.org/help/contact.html)\\n*   subscribe to arXiv mailingsClick here to subscribe [Subscribe](https://info.arxiv.org/help/subscribe)\\n\\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\\n\\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\\n*   [arXiv Operational Status](https://status.arxiv.org)  \\n    Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\",\"annotations\":null}", "tool_call_id": "toolu_013wyVf8u3FFMPpX2Cn5fRYm"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\\\[2502.19187\\\\] BIG-Bench Extra Hard            </script> <link rel=\\\"canonical\\\" href=\\\"https://arxiv.org/abs/2502.19187\\\"/> <meta name=\\\"description\\\" content=\\\"Abstract page for arXiv paper 2502.19187: BIG-Bench Extra Hard\\\"><meta property=\\\"og:type\\\" content=\\\"website\\\" /> <meta property=\\\"og:site\\\\_name\\\" content=\\\"arXiv.org\\\" /> <meta property=\\\"og:title\\\" content=\\\"BIG-Bench Extra Hard\\\" /> <meta property=\\\"og:url\\\" content=\\\"https://arxiv.org/abs/2502.19187v2\\\" /> <meta property=\\\"og:image\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" /> <meta property=\\\"og:image:secure\\\\_url\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" /> <meta property=\\\"og:image:width\\\" content=\\\"1200\\\" /> <meta property=\\\"og:image:height\\\" content=\\\"700\\\" /> <meta property=\\\"og:image:alt\\\" content=\\\"arXiv logo\\\"/> <meta property=\\\"og:description\\\" content=\\\"Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\\\\\\\% for the best general-purpose model and 44.8\\\\\\\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.\\\"/> <meta name=\\\"twitter:site\\\" content=\\\"@arxiv\\\"/> <meta name=\\\"twitter:card\\\" content=\\\"summary\\\"/> <meta name=\\\"twitter:title\\\" content=\\\"BIG-Bench Extra Hard\\\"/> <meta name=\\\"twitter:description\\\" content=\\\"Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning...\\\"/> <meta name=\\\"twitter:image\\\" content=\\\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\\\"/> <meta name=\\\"twitter:image:alt\\\" content=\\\"arXiv logo\\\"/> <link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"/static/browse/0.3.4/css/tooltip.css\\\"/><link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\\\"/> <script src=\\\"/static/browse/0.3.4/js/mathjaxToggle.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//code.jquery.com/jquery-latest.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\\\"></script> <script src=\\\"/static/browse/0.3.4/js/toggle-labs.js?20241022\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"/static/browse/0.3.4/js/cite.js\\\" type=\\\"text/javascript\\\"></script><meta name=\\\"citation\\\\_title\\\" content=\\\"BIG-Bench Extra Hard\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Kazemi, Mehran\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Fatemi, Bahare\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Bansal, Hritik\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Palowitch, John\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Anastasiou, Chrysovalantis\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Mehta, Sanket Vaibhav\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Jain, Lalit K.\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Aglietti, Virginia\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Jindal, Disha\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Chen, Peter\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Dikkala, Nishanth\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Tyen, Gladys\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Liu, Xin\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Shalit, Uri\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Chiappa, Silvia\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Olszewska, Kate\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Tay, Yi\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Tran, Vinh Q.\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Le, Quoc V.\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Firat, Orhan\\\" /><meta name=\\\"citation\\\\_date\\\" content=\\\"2025/02/26\\\" /><meta name=\\\"citation\\\\_online\\\\_date\\\" content=\\\"2025/05/06\\\" /><meta name=\\\"citation\\\\_pdf\\\\_url\\\" content=\\\"http://arxiv.org/pdf/2502.19187\\\" /><meta name=\\\"citation\\\\_arxiv\\\\_id\\\" content=\\\"2502.19187\\\" /><meta name=\\\"citation\\\\_abstract\\\" content=\\\"Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\\\\\\\% for the best general-purpose model and 44.8\\\\\\\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.\\\" /> </head> <body class=\\\"with-cu-identity\\\"> <div class=\\\"flex-wrap-footer\\\"> <header> <a href=\\\"#content\\\" class=\\\"is-sr-only\\\">Skip to main content</a> <!-- start desktop header --> <div class=\\\"columns is-vcentered is-hidden-mobile\\\" id=\\\"cu-identity\\\"> <div class=\\\"column\\\" id=\\\"cu-logo\\\"> <a href=\\\"https://www.cornell.edu/\\\"><img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University\\\" /></a> </div> <!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\\\"column\\\" id=\\\"support-ack\\\"> <span id=\\\"support-ack-url\\\">We gratefully acknowledge support from the Simons Foundation, <a href=\\\"https://info.arxiv.org/about/ourmembers.html\\\">member institutions</a>, and all contributors.</span> <a href=\\\"https://info.arxiv.org/about/donate.html\\\" class=\\\"btn-header-donate\\\">Donate</a> </div> </div> <div id=\\\"header\\\" class=\\\"is-hidden-mobile\\\"> <a aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"/IgnoreMe\\\"></a> <div class=\\\"header-breadcrumbs is-hidden-mobile\\\"> <a href=\\\"/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\\\" alt=\\\"arxiv logo\\\" style=\\\"height:40px;\\\"/></a> <span>&gt;</span> <a href=\\\"/list/cs/recent\\\">cs</a> <span>&gt;</span> arXiv:2502.19187 </div> <div class=\\\"search-block level-right\\\"> <form class=\\\"level-item mini-search\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\"> <div class=\\\"field has-addons\\\"> <div class=\\\"control\\\"> <input class=\\\"input is-small\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" /> <p class=\\\"help\\\"><a href=\\\"https://info.arxiv.org/help\\\">Help</a> | <a href=\\\"https://arxiv.org/search/advanced\\\">Advanced Search</a></p> </div> <div class=\\\"control\\\"> <div class=\\\"select is-small\\\"> <select name=\\\"searchtype\\\" aria-label=\\\"Field to search\\\"> <option value=\\\"all\\\" selected=\\\"selected\\\">All fields</option> <option value=\\\"title\\\">Title</option> <option value=\\\"author\\\">Author</option> <option value=\\\"abstract\\\">Abstract</option> <option value=\\\"comments\\\">Comments</option> <option value=\\\"journal\\\\_ref\\\">Journal reference</option> <option value=\\\"acm\\\\_class\\\">ACM classification</option> <option value=\\\"msc\\\\_class\\\">MSC classification</option> <option value=\\\"report\\\\_num\\\">Report number</option> <option value=\\\"paper\\\\_id\\\">arXiv identifier</option> <option value=\\\"doi\\\">DOI</option> <option value=\\\"orcid\\\">ORCID</option> <option value=\\\"author\\\\_id\\\">arXiv author ID</option> <option value=\\\"help\\\">Help pages</option> <option value=\\\"full\\\\_text\\\">Full text</option> </select> </div> </div> <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\"> <button class=\\\"button is-small is-cul-darker\\\">Search</button> </div> </form> </div> </div><!-- /end desktop header --> <div class=\\\"mobile-header\\\"> <div class=\\\"columns is-mobile\\\"> <div class=\\\"column logo-arxiv\\\"><a href=\\\"https://arxiv.org/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\\\" alt=\\\"arXiv logo\\\" style=\\\"height:60px;\\\" /></a></div> <div class=\\\"column logo-cornell\\\"><a href=\\\"https://www.cornell.edu/\\\"> <picture> <source media=\\\"(min-width: 501px)\\\" srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg 400w\\\" sizes=\\\"400w\\\" /> <source srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell\\\\_seal\\\\_simple\\\\_black.svg 2x\\\" /> <img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University Logo\\\" /> </picture> </a></div> <div class=\\\"column nav\\\" id=\\\"toggle-container\\\" role=\\\"menubar\\\"> <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-white\\\"><title>open search</title><path d=\\\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\\\"/></svg></button> <div class=\\\"mobile-toggle-block toggle-target\\\"> <form class=\\\"mobile-search-form\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\"> <div class=\\\"field has-addons\\\"> <input class=\\\"input\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" /> <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\"> <input type=\\\"hidden\\\" name=\\\"searchtype\\\" value=\\\"all\\\"> <button class=\\\"button\\\">GO</button> </div> </form> </div> <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-white\\\" role=\\\"menu\\\"><title>open navigation menu</title><path d=\\\"M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z\\\"/ ></svg></button> <div class=\\\"mobile-toggle-block toggle-target\\\"> <nav class=\\\"mobile-menu\\\" aria-labelledby=\\\"mobilemenulabel\\\"> <h2 id=\\\"mobilemenulabel\\\">quick links</h2> <ul> <li><a href=\\\"https://arxiv.org/login\\\">Login</a></li> <li><a href=\\\"https://info.arxiv.org/help\\\">Help Pages</a></li> <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li> </ul> </nav> </div> </div> </div> </div><!-- /end mobile-header --> </header> <main> <div id=\\\"content\\\"> <!-- rdf:RDF xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\" xmlns:dc=\\\"http://purl.org/dc/elements/1.1/\\\" xmlns:trackback=\\\"http://madskills.com/public/xml/rss/module/trackback/\\\"> <rdf:Description rdf:about=\\\"/abs/2502.19187\\\" dc:identifier=\\\"/abs/2502.19187\\\" dc:title=\\\"BIG-Bench Extra Hard\\\" trackback:ping=\\\"/trackback/2502.19187\\\" /> </rdf:RDF> --><div id=\\\"abs-outer\\\"> <div class=\\\"leftcolumn\\\"> <div class=\\\"subheader\\\"> <h1>Computer Science > Computation and Language</h1> </div> <div class=\\\"header-breadcrumbs-mobile\\\"> <strong>arXiv:2502.19187</strong> (cs) </div> <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"/static/base/1.0.1/css/abs.css\\\"> <div id=\\\"content-inner\\\"> <div id=\\\"abs\\\"> <div class=\\\"dateline\\\"> \\\\[Submitted on 26 Feb 2025 (<a href=\\\"https://arxiv.org/abs/2502.19187v1\\\">v1</a>), last revised 6 May 2025 (this version, v2)\\\\]</div> <h1 class=\\\"title mathjax\\\"><span class=\\\"descriptor\\\">Title:</span>BIG-Bench Extra Hard</h1> <div class=\\\"authors\\\"><span class=\\\"descriptor\\\">Authors:</span><a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Kazemi,+M\\\" rel=\\\"nofollow\\\">Mehran Kazemi</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Fatemi,+B\\\" rel=\\\"nofollow\\\">Bahare Fatemi</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bansal,+H\\\" rel=\\\"nofollow\\\">Hritik Bansal</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Palowitch,+J\\\" rel=\\\"nofollow\\\">John Palowitch</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Anastasiou,+C\\\" rel=\\\"nofollow\\\">Chrysovalantis Anastasiou</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Mehta,+S+V\\\" rel=\\\"nofollow\\\">Sanket Vaibhav Mehta</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jain,+L+K\\\" rel=\\\"nofollow\\\">Lalit K. Jain</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Aglietti,+V\\\" rel=\\\"nofollow\\\">Virginia Aglietti</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Jindal,+D\\\" rel=\\\"nofollow\\\">Disha Jindal</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+P\\\" rel=\\\"nofollow\\\">Peter Chen</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Dikkala,+N\\\" rel=\\\"nofollow\\\">Nishanth Dikkala</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Tyen,+G\\\" rel=\\\"nofollow\\\">Gladys Tyen</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X\\\" rel=\\\"nofollow\\\">Xin Liu</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Shalit,+U\\\" rel=\\\"nofollow\\\">Uri Shalit</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chiappa,+S\\\" rel=\\\"nofollow\\\">Silvia Chiappa</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Olszewska,+K\\\" rel=\\\"nofollow\\\">Kate Olszewska</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Tay,+Y\\\" rel=\\\"nofollow\\\">Yi Tay</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Tran,+V+Q\\\" rel=\\\"nofollow\\\">Vinh Q. Tran</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Le,+Q+V\\\" rel=\\\"nofollow\\\">Quoc V. Le</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Firat,+O\\\" rel=\\\"nofollow\\\">Orhan Firat</a></div> <div id=\\\"download-button-info\\\" hidden>View a PDF of the paper titled BIG-Bench Extra Hard, by Mehran Kazemi and 19 other authors</div> <a class=\\\"mobile-submission-download\\\" href=\\\"/pdf/2502.19187\\\">View PDF</a> <a class=\\\"mobile-submission-download\\\" href=\\\"https://arxiv.org/html/2502.19187v2\\\">HTML (experimental)</a> <blockquote class=\\\"abstract mathjax\\\"> <span class=\\\"descriptor\\\">Abstract:</span>Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\\\\\\\% for the best general-purpose model and 44.8\\\\\\\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: <a href=\\\"https://github.com/google-deepmind/bbeh\\\" rel=\\\"external noopener nofollow\\\" class=\\\"link-external link-https\\\">this https URL</a>. </blockquote> <!--CONTEXT--> <div class=\\\"metatable\\\"> <table summary=\\\"Additional metadata\\\"><tr> <td class=\\\"tablecell label\\\">Subjects:</td> <td class=\\\"tablecell subjects\\\"> <span class=\\\"primary-subject\\\">Computation and Language (cs.CL)</span></td> </tr><tr> <td class=\\\"tablecell label\\\">Cite as:</td> <td class=\\\"tablecell arxivid\\\"><span class=\\\"arxivid\\\"><a href=\\\"https://arxiv.org/abs/2502.19187\\\">arXiv:2502.19187</a> \\\\[cs.CL\\\\]</span></td> </tr> <tr> <td class=\\\"tablecell label\\\">&nbsp;</td> <td class=\\\"tablecell arxividv\\\">(or <span class=\\\"arxivid\\\"> <a href=\\\"https://arxiv.org/abs/2502.19187v2\\\">arXiv:2502.19187v2</a> \\\\[cs.CL\\\\]</span> for this version) </td> </tr> <tr> <td class=\\\"tablecell label\\\">&nbsp;</td> <td class=\\\"tablecell arxivdoi\\\"> <a href=\\\"https://doi.org/10.48550/arXiv.2502.19187\\\" id=\\\"arxiv-doi-link\\\">https://doi.org/10.48550/arXiv.2502.19187</a><div class=\\\"button-and-tooltip\\\"> <button class=\\\"more-info\\\" aria-describedby=\\\"more-info-desc-1\\\"> <svg height=\\\"15\\\" role=\\\"presentation\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><path fill=\\\"currentColor\\\" d=\\\"M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z\\\" class=\\\"\\\"></path></svg> <span class=\\\"visually-hidden\\\">Focus to learn more</span> </button> <!-- tooltip description --> <div role=\\\"tooltip\\\" id=\\\"more-info-desc-1\\\"> <span class=\\\"left-corner\\\"></span> arXiv-issued DOI via DataCite</div> </div> </td> </tr></table> </div> </div> </div> <div class=\\\"submission-history\\\"> <h2>Submission history</h2> From: Mehran Kazemi \\\\[<a href=\\\"/show-email/70675538/2502.19187\\\" rel=\\\"nofollow\\\">view email</a>\\\\] <br/> <strong><a href=\\\"/abs/2502.19187v1\\\" rel=\\\"nofollow\\\">\\\\[v1\\\\]</a></strong> Wed, 26 Feb 2025 14:50:50 UTC (136 KB)<br/> <strong>\\\\[v2\\\\]</strong> Tue, 6 May 2025 15:11:32 UTC (137 KB)<br/> </div> </div> <!--end leftcolumn--> <div class=\\\"extra-services\\\"> <div class=\\\"full-text\\\"> <a name=\\\"other\\\"></a> <span class=\\\"descriptor\\\">Full-text links:</span> <h2>Access Paper:</h2> <ul> <div id=\\\"download-button-info\\\" hidden> View a PDF of the paper titled BIG-Bench Extra Hard, by Mehran Kazemi and 19 other authors</div><li><a href=\\\"/pdf/2502.19187\\\" aria-describedby=\\\"download-button-info\\\" accesskey=\\\"f\\\" class=\\\"abs-button download-pdf\\\">View PDF</a></li><li><a href=\\\"https://arxiv.org/html/2502.19187v2\\\" class=\\\"abs-button\\\" id=\\\"latexml-download-link\\\">HTML (experimental)</a></li><li><a href=\\\"/src/2502.19187\\\" class=\\\"abs-button download-eprint\\\">TeX Source</a></li><li><a href=\\\"/format/2502.19187\\\" class=\\\"abs-button download-format\\\">Other Formats</a></li></ul> <div class=\\\"abs-license\\\"><a href=\\\"http://creativecommons.org/licenses/by/4.0/\\\" title=\\\"Rights to this article\\\" class=\\\"has\\\\_license\\\"> <img alt=\\\"license icon\\\" role=\\\"presentation\\\" src=\\\"https://arxiv.org/icons/licenses/by-4.0.png\\\"/> <span>view license</span> </a></div> </div> <!--end full-text--> <div class=\\\"browse\\\"> Current browse context: <div class=\\\"current\\\">cs.CL</div> <div class=\\\"prevnext\\\"> <span class=\\\"arrow\\\"> <a class=\\\"abs-button prev-url\\\" href=\\\"/prevnext?id=2502.19187&amp;function=prev&amp;context=cs.CL\\\" accesskey=\\\"p\\\" title=\\\"previous in cs.CL (accesskey p)\\\" rel=\\\"nofollow\\\">&lt;&nbsp;prev</a> </span> <span class=\\\"is-hidden-mobile\\\">&nbsp; | &nbsp;</span> <span class=\\\"arrow\\\"> <a class=\\\"abs-button next-url\\\" href=\\\"/prevnext?id=2502.19187&amp;function=next&amp;context=cs.CL\\\" accesskey=\\\"n\\\" title=\\\"next in cs.CL (accesskey n)\\\" rel=\\\"nofollow\\\">next&nbsp;&gt;</a> </span><br/> </div><div class=\\\"list\\\"> <a class=\\\"abs-button abs-button-grey abs-button-small context-new\\\" href=\\\"/list/cs.CL/new\\\" rel=\\\"nofollow\\\">new</a> <span class=\\\"is-hidden-mobile\\\"> | </span> <a class=\\\"abs-button abs-button-grey abs-button-small context-recent\\\" href=\\\"/list/cs.CL/recent\\\" rel=\\\"nofollow\\\">recent</a> <span class=\\\"is-hidden-mobile\\\"> | </span><a class=\\\"abs-button abs-button-grey abs-button-small context-id\\\" href=\\\"/list/cs.CL/2025-02\\\" rel=\\\"nofollow\\\">2025-02</a> </div><div class=\\\"abs-switch-cat\\\"> Change to browse by: <div class=\\\"switch context-change\\\"> <a href=\\\"/abs/2502.19187?context=cs\\\" rel=\\\"nofollow\\\">cs</a><br class=\\\"is-hidden-mobile\\\"> </div> </div> </div> <div class=\\\"extra-ref-cite\\\"> <h3>References &amp; Citations</h3> <ul> <li><a class=\\\"abs-button abs-button-small cite-ads\\\" href=\\\"https://ui.adsabs.harvard.edu/abs/arXiv:2502.19187\\\">NASA ADS</a></li><li><a class=\\\"abs-button abs-button-small cite-google-scholar\\\" href=\\\"https://scholar.google.com/scholar\\\\_lookup?arxiv\\\\_id=2502.19187\\\" target=\\\"\\\\_blank\\\" rel=\\\"noopener\\\">Google Scholar</a></li> <li><a class=\\\"abs-button abs-button-small cite-semantic-scholar\\\" href=\\\"https://api.semanticscholar.org/arXiv:2502.19187\\\" target=\\\"\\\\_blank\\\" rel=\\\"noopener\\\">Semantic Scholar</a></li> </ul> <div style=\\\"clear:both;\\\"></div> </div> <div class='extra-ref-cite'> <a id='bib-cite-css' hidden='true' href='/static/browse/0.3.4/css/cite.css'>a</a> <span id='bib-cite-trigger' class=\\\"bib-cite-button abs-button\\\">export BibTeX citation</span> <span id='bib-cite-loading' hidden='true'>Loading...</span> </div> <div id='bib-cite-modal' class='bib-modal' hidden='true'> <div class='bib-modal-content'> <div class='bib-modal-title'> <h2>BibTeX formatted citation</h2> <span class='bib-modal-close' >&times;</span> </div> <div> <textarea id='bib-cite-target' class=\\\"bib-citation-content\\\" aria-label=\\\"loading the citation\\\">loading...</textarea> </div> <div> <span>Data provided by: </span> <a id='bib-cite-source-api'></a> </div> </div> </div><div class=\\\"bookmarks\\\"> <div><h3>Bookmark</h3></div><a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2502.19187&amp;description=BIG-Bench Extra Hard\\\" title=\\\"Bookmark on BibSonomy\\\"> <img src=\\\"/static/browse/0.3.4/images/icons/social/bibsonomy.png\\\" alt=\\\"BibSonomy logo\\\"/> </a> <a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"https://reddit.com/submit?url=https://arxiv.org/abs/2502.19187&amp;title=BIG-Bench Extra Hard\\\" title=\\\"Bookmark on Reddit\\\"> <img src=\\\"/static/browse/0.3.4/images/icons/social/reddit.png\\\" alt=\\\"Reddit logo\\\"/> </a> </div> </div> <!--end extra-services--> <!-- LABS AREA --> <div id=\\\"labstabs\\\"> <div class=\\\"labstabs\\\"><input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabone\\\"checked=\\\"checked\\\"> <label for=\\\"tabone\\\">Bibliographic Tools</label> <div class=\\\"tab labs-display-bib\\\"> <h1>Bibliographic and Citation Tools</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"bibex-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/bibex/bibex.js?20241202\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Bibliographic Explorer Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-bibex\\\">Bibliographic Explorer</span> <em>(<a href=\\\"https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer\\\">What is the Explorer?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"connectedpapers-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/connectedpapers.js\\\" aria-labelledby=\\\"label-for-connected-papers\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Connected Papers Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-connected-papers\\\">Connected Papers</span> <em>(<a href=\\\"https://www.connectedpapers.com/about\\\" target=\\\"\\\\_blank\\\">What is Connected Papers?</a>)</em> </div> </div><div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"litmaps-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/litmaps.js?20210617\\\" aria-labelledby=\\\"label-for-litmaps\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Litmaps Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-litmaps\\\">Litmaps</span> <em>(<a href=\\\"https://www.litmaps.co/\\\" target=\\\"\\\\_blank\\\">What is Litmaps?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"scite-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/scite.js?20210617\\\" aria-labelledby=\\\"label-for-scite\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">scite.ai Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-scite\\\">scite Smart Citations</span> <em>(<a href=\\\"https://www.scite.ai/\\\" target=\\\"\\\\_blank\\\">What are Smart Citations?</a>)</em> </div> </div> </div> <div class=\\\"labs-content-placeholder labs-display\\\" style=\\\"display: none;\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"connectedpapers-output\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"litmaps-open-in\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"scite-open-in\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabtwo\\\"> <label for=\\\"tabtwo\\\">Code, Data, Media</label> <div class=\\\"tab\\\"> <h1>Code, Data and Media Associated with this Article</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"alphaxiv-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/alphaxiv.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-alphaxiv\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">alphaXiv Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-alphaxiv\\\">alphaXiv</span> <em>(<a href=\\\"https://alphaxiv.org/\\\" target=\\\"\\\\_blank\\\">What is alphaXiv?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"catalyzex-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/catalyzex.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-cx\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Links to Code Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-cx\\\">CatalyzeX Code Finder for Papers</span> <em>(<a href=\\\"https://www.catalyzex.com\\\" target=\\\"\\\\_blank\\\">What is CatalyzeX?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"dagshub-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/dagshub.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-dagshub\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">DagsHub Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-dagshub\\\">DagsHub</span> <em>(<a href=\\\"https://dagshub.com/\\\" target=\\\"\\\\_blank\\\">What is DagsHub?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"gotitpub-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/gotitpub.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-gotitpub\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">GotitPub Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-gotitpub\\\">Gotit.pub</span> <em>(<a href=\\\"http://gotit.pub/faq\\\" target=\\\"\\\\_blank\\\">What is GotitPub?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"huggingface-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/huggingface.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-huggingface\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Huggingface Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-huggingface\\\">Hugging Face</span> <em>(<a href=\\\"https://huggingface.co/huggingface\\\" target=\\\"\\\\_blank\\\">What is Huggingface?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"paperwithcode-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/paperswithcode.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-pwc\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Links to Code Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-pwc\\\">Papers with Code</span> <em>(<a href=\\\"https://paperswithcode.com/\\\" target=\\\"\\\\_blank\\\">What is Papers with Code?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"sciencecast-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/sciencecast.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-sciencecast\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">ScienceCast Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-sciencecast\\\">ScienceCast</span> <em>(<a href=\\\"https://sciencecast.org/welcome\\\" target=\\\"\\\\_blank\\\">What is ScienceCast?</a>)</em> </div> </div> </div> <div id=\\\"alphaxiv-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"catalyzex-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"dagshub-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"gotitpub-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"pwc-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"pwc-data-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"sciencecast-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"huggingface-output\\\" style=\\\"display:none\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"labstabs-demos-input\\\"> <label for=\\\"labstabs-demos-input\\\" id=\\\"labstabs-demos-label\\\">Demos</label> <div class=\\\"tab\\\"> <h1>Demos</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"replicate-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/replicate.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-replicate\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Replicate Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-replicate\\\">Replicate</span> <em>(<a href=\\\"https://replicate.com/docs/arxiv/about\\\" target=\\\"\\\\_blank\\\">What is Replicate?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"spaces-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/spaces.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-spaces\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Spaces Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-spaces\\\">Hugging Face Spaces</span> <em>(<a href=\\\"https://huggingface.co/docs/hub/spaces\\\" target=\\\"\\\\_blank\\\">What is Spaces?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"txyz-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/txyz.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-txyz\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Spaces Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-txyz\\\">TXYZ.AI</span> <em>(<a href=\\\"https://txyz.ai\\\" target=\\\"\\\\_blank\\\">What is TXYZ.AI?</a>)</em> </div> </div> </div> <div id=\\\"replicate-output\\\"></div> <div id=\\\"spaces-output\\\"></div> <div id=\\\"txyz-output\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfour\\\"> <label for=\\\"tabfour\\\">Related Papers</label> <div class=\\\"tab\\\"> <h1>Recommenders and Search Tools</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"influenceflower-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/influenceflower.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-influenceflower\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Link to Influence Flower</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-influenceflower\\\">Influence Flower</span> <em>(<a href=\\\"https://influencemap.cmlab.dev/\\\" target=\\\"\\\\_blank\\\">What are Influence Flowers?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"core-recommender-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-core\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Core recommender toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-core\\\">CORE Recommender</span> <em>(<a href=\\\"https://core.ac.uk/services/recommender\\\">What is CORE?</a>)</em> </div> </div></div> <div id=\\\"influenceflower-output\\\"></div> <div id=\\\"influenceflower-output-graph\\\" style=\\\"display:none\\\"> <ul class=\\\"flower-tabs\\\"> <li class=\\\"active\\\"><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-author')\\\">Author</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-venue')\\\">Venue</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-inst')\\\">Institution</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-topic')\\\">Topic</a></li> </ul> <div class=\\\"flower-tab-content\\\"> <div class=\\\"tab-flower active\\\" id=\\\"tab-author\\\"><svg id=\\\"flower-graph-author\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-venue\\\"><svg id=\\\"flower-graph-venue\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-inst\\\"><svg id=\\\"flower-graph-inst\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-topic\\\"><svg id=\\\"flower-graph-topic\\\"></svg></div> </div> </div> <div id=\\\"coreRecommenderOutput\\\"></div> <div id=\\\"iarxivOutput\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfive\\\"> <label for=\\\"tabfive\\\"> About arXivLabs </label> <div class=\\\"tab\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <h1>arXivLabs: experimental projects with community collaborators</h1> <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p> <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p> <p>Have an idea for a project that will add value for arXiv's community? <a href=\\\"https://info.arxiv.org/labs/index.html\\\"><strong>Learn more about arXivLabs</strong></a>.</p> </div> <div class=\\\"column is-narrow is-full-mobile\\\"> <p class=\\\"icon-labs\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" role=\\\"presentation\\\" viewBox=\\\"0 0 635.572 811\\\"><path d=\\\"M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z\\\"/><path d=\\\"M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z\\\" fill=\\\"#666\\\"/><path d=\\\"M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z\\\" fill=\\\"#999\\\"/><path d=\\\"M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z\\\" fill=\\\"#ccc\\\"/><path d=\\\"M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z\\\" fill=\\\"#fc0\\\"/><path d=\\\"M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81\\\" fill=\\\"none\\\" stroke=\\\"#000\\\" stroke-miterlimit=\\\"10\\\" stroke-width=\\\"20\\\"/><path d=\\\"M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z\\\"/></svg></p> </div> </div> </div> </div> </div> <!-- END LABS AREA --> <div class=\\\"endorsers\\\"> <a href=\\\"/auth/show-endorsers/2502.19187\\\" class=\\\"endorser-who\\\" rel=\\\"nofollow\\\">Which authors of this paper are endorsers?</a> | <a id=\\\"mathjax\\\\_toggle\\\" href=\\\"javascript:setMathjaxCookie()\\\">Disable MathJax</a> (<a href=\\\"https://info.arxiv.org/help/mathjax.html\\\">What is MathJax?</a>) <span class=\\\"help\\\" style=\\\"font-style: normal; float: right; margin-top: 0; margin-right: 1em;\\\"></span> </div> <script type=\\\"text/javascript\\\" language=\\\"javascript\\\">mathjaxToggle();</script> </div> </div> </main> <footer style=\\\"clear: both;\\\"> <div class=\\\"columns is-desktop\\\" role=\\\"navigation\\\" aria-label=\\\"Secondary\\\" style=\\\"margin: -0.75em -0.75em 0.75em -0.75em\\\"> <!-- Macro-Column 1 --> <div class=\\\"column\\\" style=\\\"padding: 0;\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li> <li><a href=\\\"https://info.arxiv.org/help\\\">Help</a></li> </ul> </div> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li> <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg> <a href=\\\"https://info.arxiv.org/help/contact.html\\\"> Contact</a> </li> <li> <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d=\\\"M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z\\\"/></svg> <a href=\\\"https://info.arxiv.org/help/subscribe\\\"> Subscribe</a> </li> </ul> </div> </div> </div> <!-- End Macro-Column 1 --> <!-- Macro-Column 2 --> <div class=\\\"column\\\" style=\\\"padding: 0;\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/help/license/index.html\\\">Copyright</a></li> <li><a href=\\\"https://info.arxiv.org/help/policies/privacy\\\\_policy.html\\\">Privacy Policy</a></li> </ul> </div> <div class=\\\"column sorry-app-links\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/help/web\\\\_accessibility.html\\\">Web Accessibility Assistance</a></li> <li> <p class=\\\"help\\\"> <a class=\\\"a11y-main-link\\\" href=\\\"https://status.arxiv.org\\\" target=\\\"\\\\_blank\\\">arXiv Operational Status <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 256 512\\\" class=\\\"icon filter-dark\\\\_grey\\\" role=\\\"presentation\\\"><path d=\\\"M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z\\\"/></svg></a><br> Get status notifications via <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/email/new\\\" target=\\\"\\\\_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg>email</a> or <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/slack/new\\\" target=\\\"\\\\_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z\\\"/></svg>slack</a> </p> </li> </ul> </div> </div> </div> <!-- end MetaColumn 2 --> <!-- End Macro-Column 2 --> </div> </footer> </div> <script src=\\\"/static/base/1.0.1/js/member\\\\_acknowledgement.js\\\"></script> </body> </html></x-turndown>\",\"annotations\":null}", "tool_call_id": "toolu_01GvC66RE56KzrbDh3YBE6aQ"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\\\[2502.01100\\\\] ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning            </script> <link rel=\\\"canonical\\\" href=\\\"https://arxiv.org/abs/2502.01100\\\"/> <meta name=\\\"description\\\" content=\\\"Abstract page for arXiv paper 2502.01100: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\"><meta property=\\\"og:type\\\" content=\\\"website\\\" /> <meta property=\\\"og:site\\\\_name\\\" content=\\\"arXiv.org\\\" /> <meta property=\\\"og:title\\\" content=\\\"ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\" /> <meta property=\\\"og:url\\\" content=\\\"https://arxiv.org/abs/2502.01100v2\\\" /> <meta property=\\\"og:image\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" /> <meta property=\\\"og:image:secure\\\\_url\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" /> <meta property=\\\"og:image:width\\\" content=\\\"1200\\\" /> <meta property=\\\"og:image:height\\\" content=\\\"700\\\" /> <meta property=\\\"og:image:alt\\\" content=\\\"arXiv logo\\\"/> <meta property=\\\"og:description\\\" content=\\\"We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.\\\"/> <meta name=\\\"twitter:site\\\" content=\\\"@arxiv\\\"/> <meta name=\\\"twitter:card\\\" content=\\\"summary\\\"/> <meta name=\\\"twitter:title\\\" content=\\\"ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\"/> <meta name=\\\"twitter:description\\\" content=\\\"We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive...\\\"/> <meta name=\\\"twitter:image\\\" content=\\\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\\\"/> <meta name=\\\"twitter:image:alt\\\" content=\\\"arXiv logo\\\"/> <link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"/static/browse/0.3.4/css/tooltip.css\\\"/><link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\\\"/> <script src=\\\"/static/browse/0.3.4/js/mathjaxToggle.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//code.jquery.com/jquery-latest.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\\\"></script> <script src=\\\"/static/browse/0.3.4/js/toggle-labs.js?20241022\\\" type=\\\"text/javascript\\\"></script> <script src=\\\"/static/browse/0.3.4/js/cite.js\\\" type=\\\"text/javascript\\\"></script><meta name=\\\"citation\\\\_title\\\" content=\\\"ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Lin, Bill Yuchen\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Bras, Ronan Le\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Richardson, Kyle\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Sabharwal, Ashish\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Poovendran, Radha\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Clark, Peter\\\" /><meta name=\\\"citation\\\\_author\\\" content=\\\"Choi, Yejin\\\" /><meta name=\\\"citation\\\\_date\\\" content=\\\"2025/02/03\\\" /><meta name=\\\"citation\\\\_online\\\\_date\\\" content=\\\"2025/07/15\\\" /><meta name=\\\"citation\\\\_pdf\\\\_url\\\" content=\\\"https://arxiv.org/pdf/2502.01100\\\" /><meta name=\\\"citation\\\\_arxiv\\\\_id\\\" content=\\\"2502.01100\\\" /><meta name=\\\"citation\\\\_abstract\\\" content=\\\"We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.\\\" /> </head> <body class=\\\"with-cu-identity\\\"> <div class=\\\"flex-wrap-footer\\\"> <header> <a href=\\\"#content\\\" class=\\\"is-sr-only\\\">Skip to main content</a> <!-- start desktop header --> <div class=\\\"columns is-vcentered is-hidden-mobile\\\" id=\\\"cu-identity\\\"> <div class=\\\"column\\\" id=\\\"cu-logo\\\"> <a href=\\\"https://www.cornell.edu/\\\"><img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University\\\" /></a> </div> <!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\\\"column\\\" id=\\\"support-ack\\\"> <span id=\\\"support-ack-url\\\">We gratefully acknowledge support from the Simons Foundation, <a href=\\\"https://info.arxiv.org/about/ourmembers.html\\\">member institutions</a>, and all contributors.</span> <a href=\\\"https://info.arxiv.org/about/donate.html\\\" class=\\\"btn-header-donate\\\">Donate</a> </div> </div> <div id=\\\"header\\\" class=\\\"is-hidden-mobile\\\"> <a aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"/IgnoreMe\\\"></a> <div class=\\\"header-breadcrumbs is-hidden-mobile\\\"> <a href=\\\"/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\\\" alt=\\\"arxiv logo\\\" style=\\\"height:40px;\\\"/></a> <span>&gt;</span> <a href=\\\"/list/cs/recent\\\">cs</a> <span>&gt;</span> arXiv:2502.01100 </div> <div class=\\\"search-block level-right\\\"> <form class=\\\"level-item mini-search\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\"> <div class=\\\"field has-addons\\\"> <div class=\\\"control\\\"> <input class=\\\"input is-small\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" /> <p class=\\\"help\\\"><a href=\\\"https://info.arxiv.org/help\\\">Help</a> | <a href=\\\"https://arxiv.org/search/advanced\\\">Advanced Search</a></p> </div> <div class=\\\"control\\\"> <div class=\\\"select is-small\\\"> <select name=\\\"searchtype\\\" aria-label=\\\"Field to search\\\"> <option value=\\\"all\\\" selected=\\\"selected\\\">All fields</option> <option value=\\\"title\\\">Title</option> <option value=\\\"author\\\">Author</option> <option value=\\\"abstract\\\">Abstract</option> <option value=\\\"comments\\\">Comments</option> <option value=\\\"journal\\\\_ref\\\">Journal reference</option> <option value=\\\"acm\\\\_class\\\">ACM classification</option> <option value=\\\"msc\\\\_class\\\">MSC classification</option> <option value=\\\"report\\\\_num\\\">Report number</option> <option value=\\\"paper\\\\_id\\\">arXiv identifier</option> <option value=\\\"doi\\\">DOI</option> <option value=\\\"orcid\\\">ORCID</option> <option value=\\\"author\\\\_id\\\">arXiv author ID</option> <option value=\\\"help\\\">Help pages</option> <option value=\\\"full\\\\_text\\\">Full text</option> </select> </div> </div> <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\"> <button class=\\\"button is-small is-cul-darker\\\">Search</button> </div> </form> </div> </div><!-- /end desktop header --> <div class=\\\"mobile-header\\\"> <div class=\\\"columns is-mobile\\\"> <div class=\\\"column logo-arxiv\\\"><a href=\\\"https://arxiv.org/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\\\" alt=\\\"arXiv logo\\\" style=\\\"height:60px;\\\" /></a></div> <div class=\\\"column logo-cornell\\\"><a href=\\\"https://www.cornell.edu/\\\"> <picture> <source media=\\\"(min-width: 501px)\\\" srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg 400w\\\" sizes=\\\"400w\\\" /> <source srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell\\\\_seal\\\\_simple\\\\_black.svg 2x\\\" /> <img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University Logo\\\" /> </picture> </a></div> <div class=\\\"column nav\\\" id=\\\"toggle-container\\\" role=\\\"menubar\\\"> <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-white\\\"><title>open search</title><path d=\\\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\\\"/></svg></button> <div class=\\\"mobile-toggle-block toggle-target\\\"> <form class=\\\"mobile-search-form\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\"> <div class=\\\"field has-addons\\\"> <input class=\\\"input\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" /> <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\"> <input type=\\\"hidden\\\" name=\\\"searchtype\\\" value=\\\"all\\\"> <button class=\\\"button\\\">GO</button> </div> </form> </div> <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-white\\\" role=\\\"menu\\\"><title>open navigation menu</title><path d=\\\"M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z\\\"/ ></svg></button> <div class=\\\"mobile-toggle-block toggle-target\\\"> <nav class=\\\"mobile-menu\\\" aria-labelledby=\\\"mobilemenulabel\\\"> <h2 id=\\\"mobilemenulabel\\\">quick links</h2> <ul> <li><a href=\\\"https://arxiv.org/login\\\">Login</a></li> <li><a href=\\\"https://info.arxiv.org/help\\\">Help Pages</a></li> <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li> </ul> </nav> </div> </div> </div> </div><!-- /end mobile-header --> </header> <main> <div id=\\\"content\\\"> <!-- rdf:RDF xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\" xmlns:dc=\\\"http://purl.org/dc/elements/1.1/\\\" xmlns:trackback=\\\"http://madskills.com/public/xml/rss/module/trackback/\\\"> <rdf:Description rdf:about=\\\"/abs/2502.01100\\\" dc:identifier=\\\"/abs/2502.01100\\\" dc:title=\\\"ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\" trackback:ping=\\\"/trackback/2502.01100\\\" /> </rdf:RDF> --><div id=\\\"abs-outer\\\"> <div class=\\\"leftcolumn\\\"> <div class=\\\"subheader\\\"> <h1>Computer Science > Artificial Intelligence</h1> </div> <div class=\\\"header-breadcrumbs-mobile\\\"> <strong>arXiv:2502.01100</strong> (cs) </div> <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"/static/base/1.0.1/css/abs.css\\\"> <div id=\\\"content-inner\\\"> <div id=\\\"abs\\\"> <div class=\\\"dateline\\\"> \\\\[Submitted on 3 Feb 2025 (<a href=\\\"https://arxiv.org/abs/2502.01100v1\\\">v1</a>), last revised 15 Jul 2025 (this version, v2)\\\\]</div> <h1 class=\\\"title mathjax\\\"><span class=\\\"descriptor\\\">Title:</span>ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning</h1> <div class=\\\"authors\\\"><span class=\\\"descriptor\\\">Authors:</span><a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+B+Y\\\" rel=\\\"nofollow\\\">Bill Yuchen Lin</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bras,+R+L\\\" rel=\\\"nofollow\\\">Ronan Le Bras</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Richardson,+K\\\" rel=\\\"nofollow\\\">Kyle Richardson</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Sabharwal,+A\\\" rel=\\\"nofollow\\\">Ashish Sabharwal</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Poovendran,+R\\\" rel=\\\"nofollow\\\">Radha Poovendran</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+P\\\" rel=\\\"nofollow\\\">Peter Clark</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Choi,+Y\\\" rel=\\\"nofollow\\\">Yejin Choi</a></div> <div id=\\\"download-button-info\\\" hidden>View a PDF of the paper titled ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning, by Bill Yuchen Lin and 6 other authors</div> <a class=\\\"mobile-submission-download\\\" href=\\\"/pdf/2502.01100\\\">View PDF</a> <a class=\\\"mobile-submission-download\\\" href=\\\"https://arxiv.org/html/2502.01100v2\\\">HTML (experimental)</a> <blockquote class=\\\"abstract mathjax\\\"> <span class=\\\"descriptor\\\">Abstract:</span>We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. <br>Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement. </blockquote> <!--CONTEXT--> <div class=\\\"metatable\\\"> <table summary=\\\"Additional metadata\\\"> <tr> <td class=\\\"tablecell label\\\">Comments:</td> <td class=\\\"tablecell comments mathjax\\\">Accepted to ICML 2025</td> </tr> <tr> <td class=\\\"tablecell label\\\">Subjects:</td> <td class=\\\"tablecell subjects\\\"> <span class=\\\"primary-subject\\\">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)</td> </tr><tr> <td class=\\\"tablecell label\\\">Cite as:</td> <td class=\\\"tablecell arxivid\\\"><span class=\\\"arxivid\\\"><a href=\\\"https://arxiv.org/abs/2502.01100\\\">arXiv:2502.01100</a> \\\\[cs.AI\\\\]</span></td> </tr> <tr> <td class=\\\"tablecell label\\\">&nbsp;</td> <td class=\\\"tablecell arxividv\\\">(or <span class=\\\"arxivid\\\"> <a href=\\\"https://arxiv.org/abs/2502.01100v2\\\">arXiv:2502.01100v2</a> \\\\[cs.AI\\\\]</span> for this version) </td> </tr> <tr> <td class=\\\"tablecell label\\\">&nbsp;</td> <td class=\\\"tablecell arxivdoi\\\"> <a href=\\\"https://doi.org/10.48550/arXiv.2502.01100\\\" id=\\\"arxiv-doi-link\\\">https://doi.org/10.48550/arXiv.2502.01100</a><div class=\\\"button-and-tooltip\\\"> <button class=\\\"more-info\\\" aria-describedby=\\\"more-info-desc-1\\\"> <svg height=\\\"15\\\" role=\\\"presentation\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><path fill=\\\"currentColor\\\" d=\\\"M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z\\\" class=\\\"\\\"></path></svg> <span class=\\\"visually-hidden\\\">Focus to learn more</span> </button> <!-- tooltip description --> <div role=\\\"tooltip\\\" id=\\\"more-info-desc-1\\\"> <span class=\\\"left-corner\\\"></span> arXiv-issued DOI via DataCite</div> </div> </td> </tr></table> </div> </div> </div> <div class=\\\"submission-history\\\"> <h2>Submission history</h2> From: Bill Yuchen Lin \\\\[<a href=\\\"/show-email/82d97e69/2502.01100\\\" rel=\\\"nofollow\\\">view email</a>\\\\] <br/> <strong><a href=\\\"/abs/2502.01100v1\\\" rel=\\\"nofollow\\\">\\\\[v1\\\\]</a></strong> Mon, 3 Feb 2025 06:44:49 UTC (18,441 KB)<br/> <strong>\\\\[v2\\\\]</strong> Tue, 15 Jul 2025 01:14:25 UTC (2,932 KB)<br/> </div> </div> <!--end leftcolumn--> <div class=\\\"extra-services\\\"> <div class=\\\"full-text\\\"> <a name=\\\"other\\\"></a> <span class=\\\"descriptor\\\">Full-text links:</span> <h2>Access Paper:</h2> <ul> <div id=\\\"download-button-info\\\" hidden> View a PDF of the paper titled ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning, by Bill Yuchen Lin and 6 other authors</div><li><a href=\\\"/pdf/2502.01100\\\" aria-describedby=\\\"download-button-info\\\" accesskey=\\\"f\\\" class=\\\"abs-button download-pdf\\\">View PDF</a></li><li><a href=\\\"https://arxiv.org/html/2502.01100v2\\\" class=\\\"abs-button\\\" id=\\\"latexml-download-link\\\">HTML (experimental)</a></li><li><a href=\\\"/src/2502.01100\\\" class=\\\"abs-button download-eprint\\\">TeX Source</a></li><li><a href=\\\"/format/2502.01100\\\" class=\\\"abs-button download-format\\\">Other Formats</a></li></ul> <div class=\\\"abs-license\\\"><a href=\\\"http://creativecommons.org/licenses/by/4.0/\\\" title=\\\"Rights to this article\\\" class=\\\"has\\\\_license\\\"> <img alt=\\\"license icon\\\" role=\\\"presentation\\\" src=\\\"https://arxiv.org/icons/licenses/by-4.0.png\\\"/> <span>view license</span> </a></div> </div> <!--end full-text--> <div class=\\\"browse\\\"> Current browse context: <div class=\\\"current\\\">cs.AI</div> <div class=\\\"prevnext\\\"> <span class=\\\"arrow\\\"> <a class=\\\"abs-button prev-url\\\" href=\\\"/prevnext?id=2502.01100&amp;function=prev&amp;context=cs.AI\\\" accesskey=\\\"p\\\" title=\\\"previous in cs.AI (accesskey p)\\\" rel=\\\"nofollow\\\">&lt;&nbsp;prev</a> </span> <span class=\\\"is-hidden-mobile\\\">&nbsp; | &nbsp;</span> <span class=\\\"arrow\\\"> <a class=\\\"abs-button next-url\\\" href=\\\"/prevnext?id=2502.01100&amp;function=next&amp;context=cs.AI\\\" accesskey=\\\"n\\\" title=\\\"next in cs.AI (accesskey n)\\\" rel=\\\"nofollow\\\">next&nbsp;&gt;</a> </span><br/> </div><div class=\\\"list\\\"> <a class=\\\"abs-button abs-button-grey abs-button-small context-new\\\" href=\\\"/list/cs.AI/new\\\" rel=\\\"nofollow\\\">new</a> <span class=\\\"is-hidden-mobile\\\"> | </span> <a class=\\\"abs-button abs-button-grey abs-button-small context-recent\\\" href=\\\"/list/cs.AI/recent\\\" rel=\\\"nofollow\\\">recent</a> <span class=\\\"is-hidden-mobile\\\"> | </span><a class=\\\"abs-button abs-button-grey abs-button-small context-id\\\" href=\\\"/list/cs.AI/2025-02\\\" rel=\\\"nofollow\\\">2025-02</a> </div><div class=\\\"abs-switch-cat\\\"> Change to browse by: <div class=\\\"switch context-change\\\"> <a href=\\\"/abs/2502.01100?context=cs\\\" rel=\\\"nofollow\\\">cs</a><br class=\\\"is-hidden-mobile\\\"> <a class=\\\"subclass\\\" href=\\\"/abs/2502.01100?context=cs.CL\\\" rel=\\\"nofollow\\\">cs.CL</a><br class=\\\"is-hidden-mobile\\\"> <a class=\\\"subclass\\\" href=\\\"/abs/2502.01100?context=cs.LG\\\" rel=\\\"nofollow\\\">cs.LG</a><br class=\\\"is-hidden-mobile\\\"> </div> </div> </div> <div class=\\\"extra-ref-cite\\\"> <h3>References &amp; Citations</h3> <ul> <li><a class=\\\"abs-button abs-button-small cite-ads\\\" href=\\\"https://ui.adsabs.harvard.edu/abs/arXiv:2502.01100\\\">NASA ADS</a></li><li><a class=\\\"abs-button abs-button-small cite-google-scholar\\\" href=\\\"https://scholar.google.com/scholar\\\\_lookup?arxiv\\\\_id=2502.01100\\\" target=\\\"\\\\_blank\\\" rel=\\\"noopener\\\">Google Scholar</a></li> <li><a class=\\\"abs-button abs-button-small cite-semantic-scholar\\\" href=\\\"https://api.semanticscholar.org/arXiv:2502.01100\\\" target=\\\"\\\\_blank\\\" rel=\\\"noopener\\\">Semantic Scholar</a></li> </ul> <div style=\\\"clear:both;\\\"></div> </div> <div class='extra-ref-cite'> <span id='bib-cite-trigger' class=\\\"bib-cite-button abs-button\\\">export BibTeX citation</span> <span id='bib-cite-loading' hidden='true'>Loading...</span> </div> <div id='bib-cite-modal' class='bib-modal' hidden='true'> <div class='bib-modal-content'> <div class='bib-modal-title'> <h2>BibTeX formatted citation</h2> <span class='bib-modal-close' >&times;</span> </div> <div> <textarea id='bib-cite-target' class=\\\"bib-citation-content\\\" aria-label=\\\"loading the citation\\\">loading...</textarea> </div> <div> <span>Data provided by: </span> <a id='bib-cite-source-api'></a> </div> </div> </div><div class=\\\"bookmarks\\\"> <div><h3>Bookmark</h3></div><a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2502.01100&amp;description=ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\" title=\\\"Bookmark on BibSonomy\\\"> <img src=\\\"/static/browse/0.3.4/images/icons/social/bibsonomy.png\\\" alt=\\\"BibSonomy logo\\\"/> </a> <a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"https://reddit.com/submit?url=https://arxiv.org/abs/2502.01100&amp;title=ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\\" title=\\\"Bookmark on Reddit\\\"> <img src=\\\"/static/browse/0.3.4/images/icons/social/reddit.png\\\" alt=\\\"Reddit logo\\\"/> </a> </div> </div> <!--end extra-services--> <!-- LABS AREA --> <div id=\\\"labstabs\\\"> <div class=\\\"labstabs\\\"><input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabone\\\"checked=\\\"checked\\\"> <label for=\\\"tabone\\\">Bibliographic Tools</label> <div class=\\\"tab labs-display-bib\\\"> <h1>Bibliographic and Citation Tools</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"bibex-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/bibex/bibex.js?20241202\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Bibliographic Explorer Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-bibex\\\">Bibliographic Explorer</span> <em>(<a href=\\\"https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer\\\">What is the Explorer?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"connectedpapers-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/connectedpapers.js\\\" aria-labelledby=\\\"label-for-connected-papers\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Connected Papers Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-connected-papers\\\">Connected Papers</span> <em>(<a href=\\\"https://www.connectedpapers.com/about\\\" target=\\\"\\\\_blank\\\">What is Connected Papers?</a>)</em> </div> </div><div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"litmaps-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/litmaps.js?20210617\\\" aria-labelledby=\\\"label-for-litmaps\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Litmaps Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-litmaps\\\">Litmaps</span> <em>(<a href=\\\"https://www.litmaps.co/\\\" target=\\\"\\\\_blank\\\">What is Litmaps?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"scite-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/scite.js?20210617\\\" aria-labelledby=\\\"label-for-scite\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">scite.ai Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-scite\\\">scite Smart Citations</span> <em>(<a href=\\\"https://www.scite.ai/\\\" target=\\\"\\\\_blank\\\">What are Smart Citations?</a>)</em> </div> </div> </div> <div class=\\\"labs-content-placeholder labs-display\\\" style=\\\"display: none;\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"connectedpapers-output\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"litmaps-open-in\\\"></div> <div style=\\\"min-height: 15px\\\" id=\\\"scite-open-in\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabtwo\\\"> <label for=\\\"tabtwo\\\">Code, Data, Media</label> <div class=\\\"tab\\\"> <h1>Code, Data and Media Associated with this Article</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"alphaxiv-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/alphaxiv.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-alphaxiv\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">alphaXiv Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-alphaxiv\\\">alphaXiv</span> <em>(<a href=\\\"https://alphaxiv.org/\\\" target=\\\"\\\\_blank\\\">What is alphaXiv?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"catalyzex-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/catalyzex.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-cx\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Links to Code Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-cx\\\">CatalyzeX Code Finder for Papers</span> <em>(<a href=\\\"https://www.catalyzex.com\\\" target=\\\"\\\\_blank\\\">What is CatalyzeX?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"dagshub-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/dagshub.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-dagshub\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">DagsHub Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-dagshub\\\">DagsHub</span> <em>(<a href=\\\"https://dagshub.com/\\\" target=\\\"\\\\_blank\\\">What is DagsHub?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"gotitpub-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/gotitpub.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-gotitpub\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">GotitPub Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-gotitpub\\\">Gotit.pub</span> <em>(<a href=\\\"http://gotit.pub/faq\\\" target=\\\"\\\\_blank\\\">What is GotitPub?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"huggingface-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/huggingface.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-huggingface\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Huggingface Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-huggingface\\\">Hugging Face</span> <em>(<a href=\\\"https://huggingface.co/huggingface\\\" target=\\\"\\\\_blank\\\">What is Huggingface?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"paperwithcode-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/paperswithcode.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-pwc\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Links to Code Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-pwc\\\">Papers with Code</span> <em>(<a href=\\\"https://paperswithcode.com/\\\" target=\\\"\\\\_blank\\\">What is Papers with Code?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"sciencecast-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/sciencecast.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-sciencecast\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">ScienceCast Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-sciencecast\\\">ScienceCast</span> <em>(<a href=\\\"https://sciencecast.org/welcome\\\" target=\\\"\\\\_blank\\\">What is ScienceCast?</a>)</em> </div> </div> </div> <div id=\\\"alphaxiv-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"catalyzex-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"dagshub-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"gotitpub-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"pwc-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"pwc-data-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"sciencecast-output\\\" style=\\\"display:none\\\"></div> <div id=\\\"huggingface-output\\\" style=\\\"display:none\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"labstabs-demos-input\\\"> <label for=\\\"labstabs-demos-input\\\" id=\\\"labstabs-demos-label\\\">Demos</label> <div class=\\\"tab\\\"> <h1>Demos</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"replicate-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/replicate.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-replicate\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Replicate Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-replicate\\\">Replicate</span> <em>(<a href=\\\"https://replicate.com/docs/arxiv/about\\\" target=\\\"\\\\_blank\\\">What is Replicate?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"spaces-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/spaces.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-spaces\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Spaces Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-spaces\\\">Hugging Face Spaces</span> <em>(<a href=\\\"https://huggingface.co/docs/hub/spaces\\\" target=\\\"\\\\_blank\\\">What is Spaces?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"txyz-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/txyz.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-txyz\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Spaces Toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-txyz\\\">TXYZ.AI</span> <em>(<a href=\\\"https://txyz.ai\\\" target=\\\"\\\\_blank\\\">What is TXYZ.AI?</a>)</em> </div> </div> </div> <div id=\\\"replicate-output\\\"></div> <div id=\\\"spaces-output\\\"></div> <div id=\\\"txyz-output\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfour\\\"> <label for=\\\"tabfour\\\">Related Papers</label> <div class=\\\"tab\\\"> <h1>Recommenders and Search Tools</h1> <div class=\\\"toggle\\\"> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"influenceflower-toggle\\\" data-script-url=\\\"/static/browse/0.3.4/js/influenceflower.js\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-influenceflower\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Link to Influence Flower</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-influenceflower\\\">Influence Flower</span> <em>(<a href=\\\"https://influencemap.cmlab.dev/\\\" target=\\\"\\\\_blank\\\">What are Influence Flowers?</a>)</em> </div> </div> <div class=\\\"columns is-mobile lab-row\\\"> <div class=\\\"column lab-switch\\\"> <label class=\\\"switch\\\"> <input id=\\\"core-recommender-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-core\\\"> <span class=\\\"slider\\\"></span> <span class=\\\"is-sr-only\\\">Core recommender toggle</span> </label> </div> <div class=\\\"column lab-name\\\"> <span id=\\\"label-for-core\\\">CORE Recommender</span> <em>(<a href=\\\"https://core.ac.uk/services/recommender\\\">What is CORE?</a>)</em> </div> </div></div> <div id=\\\"influenceflower-output\\\"></div> <div id=\\\"influenceflower-output-graph\\\" style=\\\"display:none\\\"> <ul class=\\\"flower-tabs\\\"> <li class=\\\"active\\\"><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-author')\\\">Author</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-venue')\\\">Venue</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-inst')\\\">Institution</a></li> <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-topic')\\\">Topic</a></li> </ul> <div class=\\\"flower-tab-content\\\"> <div class=\\\"tab-flower active\\\" id=\\\"tab-author\\\"><svg id=\\\"flower-graph-author\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-venue\\\"><svg id=\\\"flower-graph-venue\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-inst\\\"><svg id=\\\"flower-graph-inst\\\"></svg></div> <div class=\\\"tab-flower\\\" id=\\\"tab-topic\\\"><svg id=\\\"flower-graph-topic\\\"></svg></div> </div> </div> <div id=\\\"coreRecommenderOutput\\\"></div> <div id=\\\"iarxivOutput\\\"></div> </div> <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfive\\\"> <label for=\\\"tabfive\\\"> About arXivLabs </label> <div class=\\\"tab\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <h1>arXivLabs: experimental projects with community collaborators</h1> <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p> <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p> <p>Have an idea for a project that will add value for arXiv's community? <a href=\\\"https://info.arxiv.org/labs/index.html\\\"><strong>Learn more about arXivLabs</strong></a>.</p> </div> <div class=\\\"column is-narrow is-full-mobile\\\"> <p class=\\\"icon-labs\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" role=\\\"presentation\\\" viewBox=\\\"0 0 635.572 811\\\"><path d=\\\"M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z\\\"/><path d=\\\"M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z\\\" fill=\\\"#666\\\"/><path d=\\\"M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z\\\" fill=\\\"#999\\\"/><path d=\\\"M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z\\\" fill=\\\"#ccc\\\"/><path d=\\\"M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z\\\" fill=\\\"#fc0\\\"/><path d=\\\"M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81\\\" fill=\\\"none\\\" stroke=\\\"#000\\\" stroke-miterlimit=\\\"10\\\" stroke-width=\\\"20\\\"/><path d=\\\"M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z\\\"/></svg></p> </div> </div> </div> </div> </div> <!-- END LABS AREA --> <div class=\\\"endorsers\\\"> <a href=\\\"/auth/show-endorsers/2502.01100\\\" class=\\\"endorser-who\\\" rel=\\\"nofollow\\\">Which authors of this paper are endorsers?</a> | <a id=\\\"mathjax\\\\_toggle\\\" href=\\\"javascript:setMathjaxCookie()\\\">Disable MathJax</a> (<a href=\\\"https://info.arxiv.org/help/mathjax.html\\\">What is MathJax?</a>) <span class=\\\"help\\\" style=\\\"font-style: normal; float: right; margin-top: 0; margin-right: 1em;\\\"></span> </div> <script type=\\\"text/javascript\\\" language=\\\"javascript\\\">mathjaxToggle();</script> </div> </div> </main> <footer style=\\\"clear: both;\\\"> <div class=\\\"columns is-desktop\\\" role=\\\"navigation\\\" aria-label=\\\"Secondary\\\" style=\\\"margin: -0.75em -0.75em 0.75em -0.75em\\\"> <!-- Macro-Column 1 --> <div class=\\\"column\\\" style=\\\"padding: 0;\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li> <li><a href=\\\"https://info.arxiv.org/help\\\">Help</a></li> </ul> </div> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li> <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg> <a href=\\\"https://info.arxiv.org/help/contact.html\\\"> Contact</a> </li> <li> <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d=\\\"M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z\\\"/></svg> <a href=\\\"https://info.arxiv.org/help/subscribe\\\"> Subscribe</a> </li> </ul> </div> </div> </div> <!-- End Macro-Column 1 --> <!-- Macro-Column 2 --> <div class=\\\"column\\\" style=\\\"padding: 0;\\\"> <div class=\\\"columns\\\"> <div class=\\\"column\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/help/license/index.html\\\">Copyright</a></li> <li><a href=\\\"https://info.arxiv.org/help/policies/privacy\\\\_policy.html\\\">Privacy Policy</a></li> </ul> </div> <div class=\\\"column sorry-app-links\\\"> <ul style=\\\"list-style: none; line-height: 2;\\\"> <li><a href=\\\"https://info.arxiv.org/help/web\\\\_accessibility.html\\\">Web Accessibility Assistance</a></li> <li> <p class=\\\"help\\\"> <a class=\\\"a11y-main-link\\\" href=\\\"https://status.arxiv.org\\\" target=\\\"\\\\_blank\\\">arXiv Operational Status <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 256 512\\\" class=\\\"icon filter-dark\\\\_grey\\\" role=\\\"presentation\\\"><path d=\\\"M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z\\\"/></svg></a><br> Get status notifications via <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/email/new\\\" target=\\\"\\\\_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg>email</a> or <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/slack/new\\\" target=\\\"\\\\_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z\\\"/></svg>slack</a> </p> </li> </ul> </div> </div> </div> <!-- end MetaColumn 2 --> <!-- End Macro-Column 2 --> </div> </footer> </div> <script src=\\\"/static/base/1.0.1/js/member\\\\_acknowledgement.js\\\"></script> </body> </html></x-turndown>\",\"annotations\":null}", "tool_call_id": "toolu_01JF6V9W65BQBaxA9jijdp3y"}, {"role": "assistant", "content": "Now let me get the PDFs to extract more detailed information:", "tool_calls": [{"id": "toolu_01ApgXzFYc7UNtQqwpKk4ZBe", "type": "function", "function": {"name": "pdf-tools-read_pdf_pages", "arguments": "{\"pdf_file_path\": \"https://arxiv.org/pdf/2410.06526\", \"start_page\": 1, \"end_page\": 10}"}}, {"id": "toolu_01Wf8K1cjQsZfXDLjzqTTTNW", "type": "function", "function": {"name": "pdf-tools-read_pdf_pages", "arguments": "{\"pdf_file_path\": \"https://arxiv.org/pdf/2502.19187\", \"start_page\": 1, \"end_page\": 5}"}}, {"id": "toolu_015giWSdzybMDBoSLWfg5WRB", "type": "function", "function": {"name": "pdf-tools-read_pdf_pages", "arguments": "{\"pdf_file_path\": \"https://arxiv.org/pdf/2502.01100\", \"start_page\": 1, \"end_page\": 10}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 100\\nExtracted page range: 1-10\\n\\n=== Page 1 ===\\nPublished as a conference paper at ICLR 2025\\nKOR-B ENCH : BENCHMARKING LANGUAGE MODELS\\nONKNOWLEDGE -ORTHOGONAL REASONING TASKS\\nKaijing Ma1,5\u2217, Xinrun Du1,3\u2217, Yunran Wang6\u2217, Haoran Zhang1,7, Zhoufutu Wen1, Xingwei Qu1,8,\\nJian Yang1,Jiaheng Liu1,9,Minghao Liu1,4,Xiang Yue1,10,Wenhao Huang1 2 3\u2020,Ge Zhang1 2 3\u2020\\n1Multimodal Art Projection Research Community,2ByteDance.Inc,301.AI,42077.AI,\\n5Tongji University,6\u00c9cole Polytechnique,7University of Illinois at Urbana-Champaign,\\n8University of Manchester,9Nanjing University,10Carnegie Mellon University\\nmkj3085003@gmail.com, duxinrun2000@gmail.com, gezhang@umich.edu\\nhttps://kor-bench.github.io/\\nABSTRACT\\nIn this paper, we introduce K NOWLEDGE -ORTHOGONAL REASONING (KOR),\\na concept aimed at minimizing reliance on domain-specific knowledge, enabling\\nmore accurate evaluation of models\u2019 reasoning abilities in out-of-distribution set-\\ntings. Based on this concept, we propose the K NOWLEDGE -ORTHOGONAL REA-\\nSONING BENCHMARK (KOR-B ENCH ), encompassing five task categories: Op-\\neration, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes mod-\\nels\u2019 effectiveness in applying new rule descriptions to solve novel rule-driven\\nquestions. O1-Preview and O1-Mini achieve accuracies of 72.88% and 70.16%,\\nsurpassing Claude-3.5-Sonnet and GPT-4o (58.96% and 58.00%), highlighting\\nthe effectiveness of KOR-Bench. We perform detailed analyses, identifying bot-\\ntlenecks in the Cipher task with Stepwise Prompting, where two rounds of Self-\\nCorrection yield optimal results. We evaluate performance across three integrated\\ntasks, explore the impact of Tricks on the Puzzle task, and visualize rule-focused\\nattention. Additionally, we conduct an ablation study on dataset size, benchmark\\ncorrelations, and zero-shot and three-shot \\\"only questions\\\" experiments. KOR-\\nBench aims to enhance reasoning evaluation and support further research in this\\narea.\\n1 I NTRODUCTION\\nCipher\\nFigure 1: Overview of KOR-Bench.Reasoning is a fundamental aspect of human\\nintelligence, and research indicates that when\\nmodels reach a sufficient scale, they exhibit\\nemergent behaviors\u2014including advanced rea-\\nsoning capabilities such as understanding com-\\nplex scenarios, strategic planning, and multi-\\nstep execution, making this capability a crucial\\nindicator of an intelligent system\u2019s ability to\\nhandle complex tasks (Huang & Chang, 2022;\\nGui et al., 2024).\\nWhen learning new tasks and solving new prob-\\nlems, humans are never \u201cstarting from scratch\u201d;\\nrather, they are \u201cnearly starting from scratch\u201d.\\nThis phenomenon is evident in various scenar-\\nios: by understanding game rules, humans can\\nquickly master the gameplay (Nam & McClel-\\nland, 2024); by learning the basic rules of ad-\\ndition, humans can easily solve the problem of\\nadding two numbers of any length (Hu et al.,\\n2024); by giving restrictions and constraints,\\nhumans can apply thoughtful methods such as\\n* Equal Technical Contributions.\\n\u2020Corresponding Authors.1arXiv:2410.06526v3  [cs.DB]  1 Mar 2025\\n\\n=== Page 2 ===\\nPublished as a conference paper at ICLR 2025\\nReductio ad absurdum andElimination to solve puzzles (Bill Yuchen Lin, 2024). Human society\\nis abundant with OOD (Out-of-Distribution) tasks (Liu et al., 2021b)\u2014those that are novel and un-\\ndefined\u2014requiring continuous adaptation and the ability to navigate new paradigms. Humans have\\nabilities like abstract, rule-based, and explanatory reasoning, enabling them to learn rules efficiently\\nand adapt quickly to specific areas.\\nSimilarly, we expect models to develop similar capabilities so that they can still effectively handle\\nOOD tasks when encountering unfamiliar rules and frameworks, and generate results that conform\\nto specific rules or settings in real-world applications (Sun et al., 2024). Despite the models\u2019 re-\\nmarkable achievements on certain reasoning tasks, the study Mondorf & Plank (2024) points out\\nthat they are still challenged by conceptual errors and limitations when dealing with scenarios be-\\nyond the training data. While the incorporation of large amounts of code and data during model\\ntraining improves the performance of a given task, this improvement is based more on the model\u2019s\\nmemory of the patterns of the training data than on its increased ability to follow rules or reason.\\nThis reliance on in-domain knowledge limits the effectiveness of existing evaluation benchmarks in\\naccurately measuring a model\u2019s reasoning ability (Wu et al., 2023; Zhang et al., 2023; Dziri et al.,\\n2023). Therefore, there is an urgent need to establish more comprehensive and effective evaluation\\nbenchmark to measure the ability of models to understand, follow new rules and solve problems\\nefficiently, while reducing the reliance on pre-trained knowledge.\\nInspired by a deeper understanding of the human learning process, we propose the concept of\\n\u201cKnowledge-Orthogonal Reasoning\\\" (KOR) to explore a model\u2019s capabilities in reading compre-\\nhension, immediate learning, knowledge transfer, logical reasoning, and problem-solving, while\\nreducing the reliance on the existing knowledge base. Knowledge Orthogonality, formally defined\\nin Appendix A, refers to the independence between background/domain-specific knowledge (e.g.,\\ngeneral knowledge or skills acquired during pre-training) and the rules explicitly defined to solve\\na particular task. It ensures that task-solving relies on understanding and reasoning about the task\\nrules, while background knowledge only aids the reasoning process. \u201cKnowledge-Orthogonal Rea-\\nsoning Benchmark\u201d (KOR-Bench) focuses on evaluating how models apply newly-defined rules to\\nsolve new rule-driven questions, rather than relying on data retrieval or information memorization.\\nSpecifically, we design a series of tasks to challenge and demonstrate the model\u2019s reasoning ability\\nby introducing new elements and rules. These tasks are divided into five categories, each based on\\none of the following new elements: new symbols, new concepts, new execution rules, new problem-\\nsolving frameworks, and new story-context settings. The specific categories are as follows:\\n\u2022Operation Reasoning Task : Understand new definitions of mathematical symbols and\\napply this knowledge to perform calculations in mathematical reasoning tasks.\\n\u2022Logic Reasoning Task : Reason and solve problems based on new logical rules and newly\\ncategorized logical concepts in logical reasoning tasks.\\n\u2022Cipher Reasoning Task : Perform encryption and decryption operations according to new\\nexecution rules in cryptography reasoning tasks.\\n\u2022Puzzle Reasoning Task : Solve puzzles and intellectual games based on newly defined\\nproblem-solving frameworks in conditional constraint and combinatorial reasoning tasks.\\n\u2022Counterfactual Reasoning Task : Engage in hypothetical thinking and reasoning within\\nnew story contexts in conjectural scenario reasoning tasks.\\nThese tasks push models beyond traditional reasoning frameworks by customizing rules and prob-\\nlems, demonstrating their innovation and adaptability in the face of non-standard problems. We plan\\nto increase the size of the dataset in the future, explore parameterized rules, deepen the inference\\nhierarchy, refine the evaluation of the reasoning process, and expand the multimodal version.\\n2 R ELATED WORK\\nTo comprehensively assess the reasoning capabilities of large language models, researchers have\\nevaluated them through various benchmark tests, including aspects such as commonsense reasoning\\n(Bang et al., 2023; Bian et al., 2023; Clark et al., 2018), logical reasoning (Tian et al., 2021; Liu\\net al., 2021a; 2023), multi-hop reasoning (Yang et al., 2018; Chen et al., 2020; Khashabi et al.,\\n2018), and mathematical reasoning (Hendrycks et al., 2021; Arora et al., 2023; Wei et al., 2023).\\n2\\n\\n=== Page 3 ===\\nPublished as a conference paper at ICLR 2025\\nAccording to Chen et al. (2024), the realization of reasoning ability hinges on two core components:\\n(1) possessing extensive general knowledge of the world, and (2) effectively integrating new infor-\\nmation into an existing knowledge base. This framework provides a crucial lens through which we\\ncan evaluate the reasoning capabilities of LLMs.\\nKnowledge-Dependent Based Evaluation. Most knowledge-dependent benchmarks, such as\\nMMLU (Hendrycks et al., 2020), MMLU-Pro (Wang et al., 2024), GPQA (Rein et al., 2023), Com-\\nmonsenseQA (Talmor et al., 2018), and SciQ (Pedersen et al., 2020), assess a model\u2019s ability to\\naccumulate and recall data, often struggling to distinguish between true reasoning and simple recall.\\nDesigning reasoning benchmarks is challenging because domain-specific knowledge can obscure\\nreasoning performance. This raises the question: Is the model reasoning or recalling learned pat-\\nterns? Benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) target\\nmathematical reasoning, while FOLIO (Han et al., 2022) and Multi-LogiEval (Patel et al., 2024)\\nfocus on logical reasoning. However, these still rely heavily on domain knowledge, potentially\\nmasking genuine reasoning capabilities.\\nInformation Integration Based Evaluation. Moreover, there is relatively little research on the\\nability of (2) models to integrate new information. This imbalance in evaluation hinders a compre-\\nhensive understanding of the model\u2019s adaptability and creativity in unfamiliar environments. Some\\nstudies have begun addressing this by testing models on classic puzzles within specific tasks, such\\nas ZebraLogic (Bill Yuchen Lin, 2024; Berman et al., 2024), Math word problems (Xu et al., 2024),\\nMathador-LM Benchmark (Kurtic et al., 2024), BeyondX Benchmark (Kao et al., 2024), Connec-\\ntions Game (Todd et al., 2024), Cryptic Crosswords (Sadallah et al., 2024), GridPuzzle (Tyagi et al.,\\n2024), and Crossword Puzzles (Saha et al., 2024). These challenges assess the model\u2019s logical rea-\\nsoning, spatial cognition, and creative thinking by testing its ability to recognize patterns, apply\\nlogic, and derive insights from given information, highlighting divergent and lateral thinking. Addi-\\ntionally, Natural Plan (Zheng et al., 2024) and TravelPlanner (Xie et al., 2024), evaluate the models\u2019\\ninformation integration and decision-making skills in complex planning scenarios.\\nRule-Following Based Evaluation. Recent evaluations are expanding from instruction-following to\\nfocusing on rule-following capabilities. This trend is exemplified by benchmarks such as RuleBench\\n(Sun et al., 2024) for general rule following, LOGICGAME (Gui et al., 2024) for execution and\\nplanning reasoning, SearchBench (Borazjanizadeh et al., 2024) for search and problem-solving, and\\nPuzzleBench (Mittal et al., 2024) for combinatorial reasoning. This shift reflects a growing interest\\nin assessing models\u2019 reasoning and problem-solving abilities in complex, dynamic environments.\\nKnowledge Orthogonality Based Evaluation. Building on these research trends, we introduce\\nthe concept of \u201cknowledge orthogonality\\\" to address the limitations of current assessment meth-\\nods. Our approach aims to reduce the impact of domain-specific knowledge on reasoning ability\\nassessment, thoroughly examine rule-following capabilities in OOD scenarios, and provide a more\\ncomprehensive and fair evaluation framework.\\n3 K NOWLEDGE -ORTHOGONAL REASONING BENCHMARK\\n3.1 O VERVIEW\\nKOR-Bench contains five categories, each containing 25 manually defined rules that are suitably\\nmodified to ensure that they do not appear in common pre-training data, maintaining a setting that\\nis orthogonal to domain-specific knowledge. Each rule is accompanied by 10 problem instances de-\\nsigned to evaluate reasoning based on the rule. For a detailed classification of the five task categories\\nin KOR-Bench, including the number of corresponding rules and the distribution of answer formats,\\nplease refer to Tables 4 and 6 in Appendix C.\\n3.2 D ATA CONSTRUCTION PROCESS\\nData construction for KOR-Bench follows three main phases: (1) Rule Design, (2) Rule-Driven\\nQ&A Design, and (3) Quality Validation, as shown in Figure 2. The entire data creation process is\\ncarried out primarily through manual annotation, with large language models (LLMs) used only for\\nquality validation and difficulty filtering. Details of each phase are in Appendix B.\\n3\\n\\n=== Page 4 ===\\nPublished as a conference paper at ICLR 2025\\nRule \\nRedefinition\\nHumanRule \\nExtraction\\nQ&A \\nAdaptation\\nFormat \\nSpecification\\nHumanHuman\\nHuman \\nVerification\\nLLM \\nVerificationQuality Quality  \\nValidation ValidationPuzzle \\nWebsites\\nPrior \\nKnowledgeDataData\\nBooks \\n& TextsRule-Driven Q&A Design Rule-Driven Q&A Design\\nHuman\\nHuman&CodeQ&A \\nGenerationRule Design Rule Design\\nFigure 2: Overview of the KOR-Bench Data Construction Process.\\n3.3 D ATASET CATEGORIES\\n3.3.1 O PERATION REASONING\\nIn operation reasoning task, new symbolic operators and corresponding rules are defined, typically\\ninvolving an operator and its associated equations. These rules are derived from classical mathe-\\nmatical operations but have been combined or adjusted to align with the concepts and framework of\\nKOR. These rules cover various levels of difficulty and knowledge domains, ranging from elemen-\\ntary arithmetic to advanced mathematics. This section not only assesses the model\u2019s comprehension\\nof the novel rules but also evaluates its reasoning capabilities in mathematical operations. The model\\nmust be acquainted with classical mathematical operations and apply its understanding of mathemat-\\nical knowledge in accordance with the newly defined rules to solve these rule-driven questions. For\\nspecific descriptions of each rule, please refer to Table 8.\\n3.3.2 L OGIC REASONING\\nRules in the logic section are based on traditional logic textbooks and refined with symbolic ad-\\njustments and innovative definitions to address the specific challenges of KOR-Bench. These rules\\nassess the model\u2019s understanding of classical logic and its ability to apply new rules to unconven-\\ntional problems, demonstrating flexibility and innovation. Ten problems of varying difficulty have\\nbeen designed for each rule. A detailed description of each rule is provided in Table 9.\\n3.3.3 C IPHER REASONING\\nCipher section consists of traditional and modern cryptographic methods, which have been modified\\nto address the specific challenges of KOR-Bench. These methods are based on uncommon encryp-\\ntion and decryption techniques found on the Braingle and dCode websites. They have been adapted\\nby altering substitution tables and adjusting certain steps in the encryption process. We verify their\\naccuracy with encryption and decryption programs and generate examples based on these rules.\\nThis section tests the model\u2019s ability to understand new rules and reason step-by-step according to\\nthem. Encryption and decryption involve techniques like transposition and rotation, further testing\\nthe model\u2019s spatial understanding. Table 10 lists the details of each cipher rule.\\n3.3.4 P UZZLE REASONING\\nRules for the puzzle section are divided into three categories: classic paper puzzles (e.g., star bat-\\ntle), number games (e.g., sudoku and 24-point), and word games (e.g., anagram). The puzzles are\\n4\\n\\n=== Page 5 ===\\nPublished as a conference paper at ICLR 2025\\nDefine an operation  such that when a is \\na multiple of b, \\na\u203bb = a/b + 2;\\nwhen b is a multiple of a, \\na\u203bb = b/a + 2; \\nif a is not a multiple of b and b is not a \\nmultiple of a, a\u203bb = 24. \\nBoth a and b are integers.\\nRule-Driven Question Rule-Driven QuestionRule\\nPropositional  Symbolization  Rules:\\n- Equivalence  is represented  by `::=::` - \\nNegation  is represented  by `!`\\n-Implication  is represented  by `>`\\nBasic Equivalence :\\n(10) A > B ::=:: !A | B\\n...\\nRule-Driven QuestionOperation Cipher Logic\\nCompute  25\u203b5\u203b14.\\nX\u203b14=5 Find X.Using Basic Equivalence  (10), \\nwhat  equivalent  expression  is obtained  \\nby removing  all occurrences  of > in (p > \\nq) > r? Plaintext : \\\"IVWANCXRTWU\\\"\\nPlease provide the encrypted  answer in \\nthe format [[...]].Encryption  \\n- Convert the message  to Morse code, \\nwith Morse characters  separated  by a \\nslash \\\"/\\\" and words separated  by \\ndouble slashes \\\"//\\\". \\n- If there is a single character  remaining  \\nat the end, it is added directly to the \\nend of the ciphertext .  \\n...Rule\\nProfessor  Oak is renowned  in the \\nPok\u00e9mon  world for his extensive  \\nresearch on Pok\u00e9mon  and their \\nrelationships  with humans. His work, \\nparticularly  in the field of Pok\u00e9mon  \\nbehavior  and genetics,  is considered  \\ngroundbreaking  and has paved the way \\nfor future studies. \\n...\\n1. The game is played on an n*n grid, \\nunder each of which a mine may be \\nhidden or empty. \\n2. Some squares show a number \\nindicating  the number of mines around \\nthem (8 squares including  the diagonal) . \\n3. You need to find all the squares \\nwhere mines are located. \\nRule-Driven Question\\nPuzzle CounterfactualX 2 X 3 X \\nX X 3 X X \\n1 2 3 3 2 \\nX X X X 2 \\n1 X 2 X XWho is considered  a pioneer in the study \\nof genetics?  \\nA. Gregor Mendel    B. Charles Darwin\\nC. Professor  Oak    D. Bill the Pok\u00e9maniacRule RuleRule\\nRule-Driven Question\\nFigure 3: Illustration and Examples of the Five Task Categories in KOR-Bench.\\nsourced from Braingle and Puzzle Prime, two sites that offer classic and original puzzles, as well as\\nchallenging and entertaining brain games. A detailed description of each puzzle rule is provided in\\nTable 11. These rules examine not only mathematical, verbal, and spatial reasoning skills but also\\nthe model\u2019s understanding of the rules and their use in complex, integrated problems. In most cases,\\nthe model has to use a combination of abilities to find the answer. Under each rule, ten problems of\\nvarying difficulty were designed based on that rule.\\n3.3.5 C OUNTERFACTUAL REASONING\\nCounterfactual reasoning aims to test the model\u2019s ability to navigate hypothetical scenarios and adapt\\nto new rules and environments. This section leverages 25 selected works from anime, television,\\nfilm, and game as foundational world settings. Within these settings, the model must derive answers\\nbased on the established worldviews and story rules from the given text information under these\\nnew conditions. In each case, the questions are crafted to deviate from real-life answers, requiring\\nthe model to engage in counterfactual thinking. This tests the model\u2019s ability to adapt to new rules,\\ninterpret fictional contexts, and engage in complex reasoning beyond conventional real-world logic.\\nThe rule setting for counterfactual reasoning is shown in Table 12.\\n3.4 S TATISTICS\\nTable 1 details the KOR-Bench statistics, covering the number and length of rules and questions. Ap-\\npendix C provides further details on KOR-Bench. In particular, Table 4 gives a statistical overview\\nof the number of rules in different categories, illustrating the distribution of rules in each category. In\\naddition, Table 8, 9, 10, 11, 12 provide detailed summary summaries of the rules for each category\\nof tasks. Table 7 shows the mean and standard deviation of the input and output tokens for each task\\ntype in KOR-Bench, using GPT-4o as an example. These statistics not only reveal the characteris-\\n5\\n\\n=== Page 6 ===\\nPublished as a conference paper at ICLR 2025\\ntics of different task types but also help us assess the differences in their specific demands on the\\ncomputational resources of the model.\\nCategory Total Rs Avg. R Len Max. R Len Total Qs Avg. Q Len Ans. Fmt\\nOperation 25 51.32 208 250 170.81 NR, ME, SD\\nLogic 25 1549.12 3338 250 411.54 NR, TR, MC\\nCipher 25 2436.64 6454 250 157.2 TR\\nPuzzle 25 473.16 767 250 394.9 NR, ME, TR, SD\\nCounterfactual 25 4572.56 9472 250 388.66 MC\\nTable 1: Overview of KOR-Bench Statistics. *Note: This table presents the total number of\\nrules, average rule length, maximum rule length, total number of questions, and average question\\nlength for five types of reasoning tasks, along with the involved answer formats. The lengths all\\nrefer to the number of characters. We define five answer formats: NR (Numerical Response), ME\\n(Mathematical Expression), TR (Textual Response), MC (Multiple Choice), and SD (Structured\\nData). Appendix C.2 provides a detailed explanation of the answer formats and the proportions of\\neach format across the different tasks.*\\n4 E XPERIMENT SETUP\\nPrompting Strategy. Zero-shot prompting strategy for chat model generates responses based on\\nnewly defined rules and questions, as outlined in prompt template in Appendix D. Base model uses\\nthree-shot strategy, providing three generic Q&A pairs for each rule to support in-context learning.\\nEvaluation Methodology. We parse the output by regular expression1to try to match the contents\\nof the double square brackets, and if not found, try to match the single square brackets and clean\\nthe extraction results. To further improve the accuracy of the analysis, we customise the design\\nof the evaluation script by observing the model\u2019s output and processing the problems under some\\nspecific rules. After completing the output extraction and special rule processing, it is compared\\nwith the answer. Specifically, for mathematical expressions, SymPy (Meurer et al., 2017) is used for\\nparsing in LaTeX format and simplifying the expressions for comparison. The accuracy of the model\\non each type of task and the overall accuracy on the entire test set are calculated. Comprehensive\\ndetails regarding the extraction and evaluation can be found in Appendix C.4.\\n5 R ESULT ANALYSIS\\nTable 2 presents the performance of the frontier models on KOR-Bench, revealing several key in-\\nsights. Overall, accuracy varies significantly across models and task types.\\nChat Model Performance. Within the landscape of chat models, O1-Preview ( 72.88% ) and O1-\\nMini ( 70.16% ) currently demonstrate the best overall performance, especially excelling in the Ci-\\npher and Puzzle reasoning tasks. In the Cipher category, their accuracies reach 82.80% and79.60% ,\\nsignificantly outperforming GPT-4o\u2019s 42.80% . On the Puzzle category, they also achieve superior\\naccuracies of 36.80% and35.60% , far surpassing GPT-4o\u2019s 16.80% , further highlighting the advan-\\ntages of the O1 series models in creative reasoning tasks. Meanwhile, Claude-3.5-Sonnet ( 58.96% )\\nand GPT-4o ( 58.00% ) follow as the next best-performing models. Claude-3.5-Sonnet shows better\\nresults on Operation and Logic reasoning tasks, especially in Logic reasoning. On the other hand,\\nGPT-4o performs better on Cipher and Puzzle reasoning tasks, particularly in Cipher reasoning, a\\ndominance that may be related to its native multimodal nature. This suggests that Claude-3.5-Sonnet\\nis more accurate in understanding and applying rules, while GPT-4o is better at handling tasks that\\nrequire in-depth analysis and creative thinking. Additionally, Qwen2.5-32B-Instruct outperforms\\nQwen2.5-72B-Instruct, suggesting that model size alone doesn\u2019t ensure better performance (McKen-\\nzie et al., 2023).\\nBase Model Performance. For base models, Meta-Llama-3.1-405B achieves the highest overall\\naccuracy at 39.68% . Additionally, the performance of the base model and its associated chat model\\n1The specific regular expression used is r\u2019\\\\[\\\\[\\\\s *(.*?)\\\\s *\\\\]\\\\]\u2019\\n6\\n\\n=== Page 7 ===\\nPublished as a conference paper at ICLR 2025\\nModel Size Open Overall Operation Logic Cipher Puzzle Counterfactual\\nChat Model\\nO1-preview-2024-09-12 (OpenAI, 2024b) * \u2717 72.88 88.80 63.20 82.80 36.80 92.80 ( 5.20)\\nO1-mini-2024-09-12 (OpenAI, 2024b) * \u2717 70.16 82.80 61.20 79.60 35.60 91.60 (5.60 )\\nClaude-3.5-sonnet-20240620 (Anthropic, 2024) * \u2717 58.96 88.40 67.20 33.20 14.80 91.20(6.00)\\nGPT-4o-2024-05-13 (OpenAI, 2024a) * \u2717 58.00 86.00 52.40 42.80 16.80 92.00 (4.80 )\\nMeta-Llama-3.1-405B-Instruct (Dubey et al., 2024) 405B \u2714 55.36 87.82 56.80 31.20 13.93 87.60(9.20)\\nQwen2.5-32B-Instruct (Team, 2024) 32B \u2714 54.72 93.20 56.80 26.80 8.00 88.80(7.60)\\nGPT-4-Turbo-2024-04-09 (OpenAI, 2023) * \u2717 53.52 90.40 54.00 23.20 12.80 87.20(9.60)\\nMistral-Large-Instruct-2407 (team, 2024) 123B \u2714 53.12 86.80 51.20 22.80 15.60 89.20(6.80)\\nQwen2.5-72B-Instruct (Team, 2024) 72.7B \u2714 52.16 83.60 53.20 26.40 10.40 87.20(8.40)\\nMeta-Llama-3.1-70B-Instruct (Dubey et al., 2024) 70B \u2714 50.00 84.80 49.20 20.40 7.60 88.00(8.40)\\nYi-Large * \u2717 50.00 84.00 47.60 20.80 11.20 86.40(11.20)\\nQwen2.5-14B-Instruct (Team, 2024) 14.7B \u2714 49.36 84.40 50.00 14.40 9.20 88.80(7.60)\\nMeta-Llama-3-70B-Instruct (AI@Meta, 2024) 70B \u2714 49.20 82.40 46.40 20.40 7.20 89.60( 5.20)\\nDoubao-Pro-128k * \u2717 48.08 85.20 46.40 11.20 7.60 90.00(5.60 )\\nDeepSeek-V2.5 (DeepSeek-AI, 2024) 236B \u2714 47.76 74.80 48.00 18.00 11.20 86.80(10.00)\\nQwen2-72B-Instruct (Yang et al., 2024) 72.71B \u2714 47.04 78.00 45.60 12.80 9.20 89.60(7.20)\\nGemma-2-27b-It (Team, 2024) 27B \u2714 44.48 73.60 49.20 7.20 5.20 87.20(9.20)\\nPhi-3.5-MoE-Instruct (Abdin et al., 2024) 16x3.8B \u2714 43.92 76.40 39.60 10.80 4.80 88.00(6.40)\\nGemini-1.5-Pro (Team et al., 2024) * \u2717 43.36 81.60 46.40 6.80 10.80 71.20(8.40)\\nGemma-2-9b-It (Team, 2024) 9B \u2714 41.60 70.00 39.60 6.40 6.40 85.60(9.20)\\nYi-1.5-34B-Chat (AI et al., 2024) 34B \u2714 39.76 79.60 24.40 8.00 3.20 83.60(6.80)\\nPhi-3.5-mini-Instruct (Abdin et al., 2024) 3.8B \u2714 39.04 69.20 31.20 8.80 3.60 82.40(9.60)\\nQwen2.5-7B-Instruct (Team, 2024) 7.61B \u2714 38.56 55.60 39.20 6.40 6.00 85.60(8.80)\\nMeta-Llama-3.1-8B-Instruct (Dubey et al., 2024) 8B \u2714 37.20 60.40 28.80 8.40 2.00 86.40(8.00)\\nYi-1.5-9B-Chat (AI et al., 2024) 9B \u2714 35.20 60.40 23.60 7.60 3.60 80.80(10.00)\\nMeta-Llama-3-8B-Instruct (AI@Meta, 2024) 8B \u2714 32.80 46.00 20.00 7.60 4.00 86.40(6.40)\\nC4ai-Command-R-Plus-08-2024 104B \u2714 32.72 30.00 34.40 6.80 2.00 90.40(5.60 )\\nYi-1.5-6B-Chat (AI et al., 2024) 6B \u2714 32.48 67.20 10.80 4.40 2.80 77.20(12.80)\\nC4ai-Command-R-08-2024 32B \u2714 31.12 29.60 28.80 5.20 3.60 88.40(8.00)\\nQwen2-7B-Instruct (Yang et al., 2024) 7.07B \u2714 30.72 28.80 28.00 3.20 4.80 88.80(7.20)\\nGemma-2-2b-It (Team, 2024) 2B \u2714 24.32 19.20 15.20 3.60 0.40 83.20(6.80)\\nMistral-7B-Instruct-v0.3 (Jiang et al., 2023) 7B \u2714 24.16 13.20 19.20 4.80 2.40 81.20(11.20)\\nQwen2.5-1.5B-Instruct (Team, 2024) 1.54B \u2714 20.40 14.80 10.00 0.80 0.80 75.60(9.60)\\nOLMo-7B-0724-Instruct-hf (Groeneveld et al., 2024) 7B \u2714 18.48 13.20 6.40 1.20 1.20 70.40(8.80)\\nMAP-Neo-7B-Instruct-v0.1 (Zhang et al., 2024) 7B \u2714 18.16 38.40 10.40 2.00 1.60 38.40(9.20)\\nQwen2-1.5B-Instruct (Yang et al., 2024) 1.54B \u2714 14.32 6.80 6.80 0.40 0.80 56.80(14.40)\\nQwen2.5-0.5B-Instruct (Team, 2024) 0.49B \u2714 9.04 4.40 3.20 0.00 0.80 36.80(14.00)\\nQwen2-0.5B-Instruct (Yang et al., 2024) 0.49B \u2714 3.52 0.80 2.00 1.60 0.40 12.80(14.40)\\nBase Model\\nMeta-Llama-3.1-405B (Dubey et al., 2024) 405B \u2714 39.68 39.20 51.20 11.20 8.40 88.40 ( 6.00 )\\nQwen2.5-32B (Team, 2024) 32.5B \u2714 37.28 38.40 50.00 9.20 6.80 82.00(11.60)\\nQwen2.5-72B (Team, 2024) 72.7B \u2714 37.28 38.80 49.20 10.80 5.20 82.40(10.80)\\nMeta-Llama-3-70B (AI@Meta, 2024) 70B \u2714 35.20 30.00 44.40 7.60 8.00 86.00 (6.00 )\\nQwen2-72B (Yang et al., 2024) 72.71B \u2714 34.32 34.00 45.60 7.60 4.80 79.60(12.40)\\nMeta-Llama-3.1-70B (Dubey et al., 2024) 70B \u2714 33.84 24.80 46.40 7.20 7.60 83.20(10.00 )\\nGemma-2-27b (Team, 2024) 27B \u2714 33.36 26.40 42.40 7.60 5.60 84.80 (7.60)\\nQwen2.5-14B (Team, 2024) 14.7B \u2714 33.28 30.80 44.80 6.40 5.20 79.20(14.00)\\nYi-1.5-34B (AI et al., 2024) 34B \u2714 30.08 24.80 39.20 7.20 3.20 76.00(14.40)\\nYi-1.5-9B (AI et al., 2024) 9B \u2714 29.20 22.00 39.20 8.00 2.80 74.00(11.20)\\nQwen2.5-7B (Team, 2024) 7.61B \u2714 28.80 24.40 34.00 8.00 2.00 75.60(13.60)\\nQwen2-7B (Yang et al., 2024) 7.07B \u2714 27.44 20.40 30.00 6.40 4.00 76.40(14.80)\\nMeta-Llama-3.1-8B (Dubey et al., 2024) 8B \u2714 26.00 14.00 32.00 5.20 3.20 75.60(12.40)\\nGemma-2-9b (Team, 2024) 9B \u2714 25.52 16.80 35.20 6.00 2.80 66.80(14.80)\\nMeta-Llama-3-8B (AI@Meta, 2024) 8B \u2714 24.96 14.40 28.00 6.00 2.00 74.40(12.80)\\nMistral-7B-v0.1 (Jiang et al., 2023) 7B \u2714 21.60 11.20 28.80 2.80 2.40 62.80(18.80)\\nYi-1.5-6B (AI et al., 2024) 6B \u2714 20.88 11.60 27.20 3.20 2.80 59.60(22.40)\\nMAP-Neo-7B (Zhang et al., 2024) 7B \u2714 15.60 7.20 22.00 4.00 0.80 44.00(31.60)\\nQwen2.5-1.5B (Team, 2024) 1.54B \u2714 15.12 12.00 16.00 1.60 1.60 44.40(34.00)\\nOLMo-7B-0724-hf (Groeneveld et al., 2024) 7B \u2714 14.80 4.80 22.00 1.20 0.80 45.20(19.60)\\nGemma-2-2b (Team, 2024) 2B \u2714 13.20 7.20 15.60 1.60 0.40 41.20(22.80)\\nQwen2-1.5B (Yang et al., 2024) 1.54B \u2714 12.32 8.80 15.20 0.80 1.20 35.60(36.80)\\nQwen2-0.5B (Yang et al., 2024) 0.49B \u2714 9.92 5.20 12.40 0.80 0.40 30.80(22.80)\\nQwen2.5-0.5B (Team, 2024) 0.49B \u2714 9.12 6.00 10.80 0.40 1.20 27.20(26.40)\\nTable 2: Models Performance on KOR-Bench. *Note: Values in parentheses represent the pro-\\nportion of real-life answers in the counterfactual setting, where lower proportions are better; for all\\nother values, higher proportions are better. For Chat models, the best result is in blue , for Base\\nmodels, it\u2019s in green . The second-best is bold , and the third-best is underlined .*\\nshows less decline in the Logic category, compared to a significant drop in other inference tasks.\\nThis difference is likely due to the shallower depth of inference required in the Logic category.\\nReasoning Process Performance. When evaluating reasoning abilities, larger models often trigger\\nChain-of-Thought (CoT) reasoning automatically, applying rules step-by-step and demonstrating\\na clear reasoning process in their responses. While they occasionally make execution errors on\\ncomplex tasks, their overall rule application remains strong. In contrast, smaller models often fail to\\n7\\n\\n=== Page 8 ===\\nPublished as a conference paper at ICLR 2025\\nactivate CoT reasoning.Especially in the Cipher task, smaller models often output \\\"Hello World\\\" as\\nthe answer without any reasoning.\\nReasoning Tasks Performance. Across the five types of reasoning tasks, models generally perform\\nbest on the Counterfactual reasoning task, indicating an apparent strength in literal reasoning com-\\npared to tasks involving mathematical, logical, or theoretical reasoning. Following that, they also\\nperform well on Operation and Logic reasoning tasks, which typically involve one or two levels of\\nreasoning. However, aside from the O1 series models, the models struggle with Cipher and Puzzle\\nreasoning tasks, with a maximum accuracy of 42.80% on the Cipher task and just 16.80% on the\\nPuzzle task, revealing significant weaknesses in handling deeper reasoning challenges.\\nSingle Task Analysis. Models struggle with algebraic problems involving unknowns but perform\\nbetter in forward symbolic computation in Operation reasoning. In Logic reasoning, construct-\\ning correct logical expressions remains difficult due to symbolic complexity. In Cipher reasoning,\\nerrors are most frequent in Position Mapping, Transpose Writing, and Mathematical Calculation,\\nalong with Split Connection and Multi-Step Execution. Puzzle reasoning reveals strengths in single-\\nsolution tasks but challenges in multi-step and spatial reasoning. In Counterfactual reasoning, as\\noverall model accuracy increases, the ratio of real-life answers decreases, suggesting an error from\\nthe models\u2019 fixed knowledge. Chat models\u2019 real-life answer ratios stay below 15% , while base mod-\\nels improve to 36.8% as accuracy drops (see Figures 6 and 7 in Appendix E). Appendix G provides\\nerror case studies for each task.\\n6 F URTHER ANALYSIS\\nWe select 16 models for a detailed analysis of their reasoning behaviors, including Claude-3.5-\\nSonnet, GPT-4o, DeepSeek-V2.5, and six model series: Meta-Llama-3.1, Qwen2.5, Qwen2, Yi,\\nCommand-R, and Mistral. For each series, we include one large model and one small model. The\\nexperiments aim to examine their characteristics, with further details in Appendix F.\\n6.1 S TEPWISE PROMPTING ANALYSIS OF CIPHER TASK BOTTLENECKS\\nClaude-3.5-SonnetGpt-4o\\nQwen2.5-32B-Instruct\\nQwen2.5-72B-InstructDeepSeek-V2.5\\nMeta-Llama-3.1-70B-InstructMistral-Large-Instruct-2407\\nQwen2-72B-InstructYi-Large\\nC4ai-Command-R-Plus\\nMeta-Llama-3.1-8B-InstructQwen2.5-7B-Instruct C4ai-Command-R\\nMistral-7B-Instruct-v0.3\\nYi-1.5-9B-Chat\\nQwen2-7B-Instruct\\nModel040\\n30\\n20\\n10Sub-S tep Accuracy(%)\\n(a) Model Accuracy in Cipher Sub-Steps.\\n (b) Average Error Rates in Sub-Steps.\\nFigure 4: Model Performance in Cipher Stepwise Prompting: (a) Accuracy and (b) Error Rates.\\nIn the Cipher Reasoning task, we select five highly erroneous rules and, with human expertise, break\\ndown the solution process into sequential sub-steps to guide the LLM in solving the problem step by\\nstep. This allows us to perform stepwise prompting analysis, pinpointing challenges and bottlenecks\\nin the reasoning process. There are 9 types of these sub-steps, as detailed in Table 13. Figure 4 shows\\nthe accuracy of models on cipher sub-steps and the error rates across nine types of sub-steps. An\\nexample of dividing a problem into sub-steps is provided in the Appendix F.1.2.\\n8\\n\\n=== Page 9 ===\\nPublished as a conference paper at ICLR 2025\\nResults show that error rates for Encoding andPartition are relatively low, indicating these are not\\nmajor factors in Cipher reasoning. Error rates for Shift ,Mapping , and Substitution are higher,\\nsuggesting these sub-steps are more challenging. High error rates for Calculation indicate com-\\nplex calculations affect reasoning. Error rates for Rotation ,Conditional Filling , and Conditional\\nReading are nearly 100%, suggesting spatial operations are a bottleneck. Model error rates across\\nall sub-steps are detailed in Appendix F.1.3.\\n6.2 A NALYSIS ON SELF-CORRECTION\\nFigure 5: Self-Correction\u2019s Impact on Overall Accuracy.\\nWe conduct the Self-Correction experiment to guide the model in identifying errors, reflecting on\\ntheir causes, and improving reasoning accuracy. Figure 5 illustrates the results of model self-\\ncorrection in KOR-Bench. With a maximum of 5 rounds, the history may exceed the model\u2019s context\\nwindow, requiring the extraction of the previous round\u2019s response for re-input. This process involves\\nidentifying the relevant response for inclusion in the next input sequence. Appendix F.4.1 provides\\nthe self-correction prompt template used for this purpose.\\nAll models show a significant performance improvement after self-correction, with an average in-\\ncrease of 10.36% . Detailed results are in Appendix F.4.2. Figure 11 shows the correction rate from\\nthe model\u2019s perspective, with the most significant improvement in the first two rounds, and limited\\ngains in later rounds. Figure 12 presents the correction rate by task category, with the Counterfactual\\ncategory achieving the highest rate of 44.05% in the first round, and strong corrections in the first\\ntwo rounds for the other categories, diminishing in the last two rounds.\\n6.3 A NALYSIS ON COMPLEX TASK PROCESSING\\nThe Complex Task Processing experiment evaluates the model\u2019s ability to apply rules to solve mul-\\ntiple problems, manage longer reasoning chains, and test reasoning robustness. It includes three\\nsettings: (1) Multi-Q: 1 rule, 1-10 questions ;(2) Multi-R: 2-3 rules, 1 question ;(3) Multi-RQ:\\n2-3 rules, 1-3 questions . See Appendix F.5.1 and Appendix F.5.2 for evaluation details. Each set-\\nting contains random combinations of five reasoning task types, with 1000 examples per type. The\\nmodel\u2019s task is to extract relevant information, reason deeply, and solve problems efficiently.\\nTable 3 displays the model performance. Claude-3.5-Sonnet consistently performs the best across\\nall settings, demonstrating a robust overall capability and resilience against interference. Yi-Large\\nand GPT-4o show similar performance. Mistral-7B-Instruct-v0.3 performs significantly worse in\\n9\\n\\n=== Page 10 ===\\nPublished as a conference paper at ICLR 2025\\nModel Size Overall Multi-Q Multi-R Multi-RQ\\nClose Model\\nClaude-3.5-sonnet-20240620 * 31.37 ( 43.24 ) 23.40 ( 42.25 ) 45.20 25.50 ( 42.28 )\\nGPT-4o-2024-05-13 * 21.80 (29.40) 15.00 (25.39) 31.20 19.20 (31.62)\\nYi-Large * 22.73 (31.11 ) 14.90 (29.09) 33.40 19.90 (30.85 )\\nOpen Model\\nDeepseek-V2.5 236B 21.23 (31.12) 16.50 (31.88 ) 28.70 18.50 (32.77)\\nMistral-Large-Instruct-2407 123B 18.27 (26.31) 14.80 (27.91) 25.10 14.90 (25.92)\\nC4ai-Command-R-Plus-08-2024 104B 9.53 (17.37) 11.00 (22.94) 9.60 8.00 (19.58)\\nQwen2-72B-Instruct 72.71B 17.73 (27.03) 14.70 (28.46) 24.60 13.90 (28.03)\\nQwen2.5-72B-Instruct 72.7B 13.53 (21.26) 13.30 (25.58) 16.00 11.30 (22.20)\\nMeta-Llama-3.1-70B-Instruct 70B 17.60 (24.71) 14.70 (24.59) 23.90 14.20 (25.63)\\nQwen2.5-32B-Instruct 32B 23.97 (33.96 ) 20.00 (35.13 ) 33.40 19.90 (33.33 )\\nC4ai-Command-R-08-2024 32B 16.13 (23.64) 10.40 (21.79) 26.10 11.90 (23.03)\\nYi-1.5-9B-Chat 9B 4.10 (9.47) 5.30 (16.16) 4.90 2.10 (7.33)\\nMeta-Llama-3.1-8B-Instruct 8B 7.00 (9.06) 7.60 (11.32) 8.10 5.30 (7.77)\\nQwen2.5-7B-Instruct 7.61B 6.77 (12.34) 5.40 (13.79) 9.80 5.10 (13.42)\\nQwen2-7B-Instruct 7.07B 7.47 (14.03) 7.50 (17.87) 8.90 6.00 (15.33)\\nMistral-7B-Instruct-v0.3 7B 9.57 (15.52) 4.20 (13.36) 17.70 6.80 (15.50)\\nTable 3: Evaluation of Model Performance Across Complex Task Processing Settings. *Note:\\nThe overall accuracy is shown outside the parentheses, while the pass rate for individual sub-\\nproblems is inside. The Multi-R Setting has multiple rules but only one question, so it has a single\\nvalue. The best accuracy is in blue , the best pass rate is in green , the second-best results are\\nbolded , and the third-best are underlined .*\\nthe Multi-Q setting compared to Multi-R and Multi-RQ, suggesting limitations in handling mul-\\ntiple problems simultaneously. C4ai-Command-R-Plus performs poorly in Multi-R and Multi-RQ\\nsettings, indicating weaknesses in multi-task switching.\\n6.4 M ORE EXPERIMENTS AND ANALYSES\\nAppendix F.2 provides an analysis of model performance after the introduction of the Trick field\\nin the puzzle task. Appendix F.3 gives the experimental setup and analysis of the Rule-Focused\\nAttention Visualization based on Retrieval Head (Wu et al., 2024), which can be an effective tool for\\nimproving interpretability. The generated file is a PDF highlighting the attention distribution, which\\ncan also be utilized for future expansions of the vision version. Appendix H includes some generated\\nexamples for reference. Appendix I demonstrates the robustness of KOR-Bench to size variations\\nthrough an ablation study. Correlations with other benchmarks show a stronger alignment with\\nreasoning-focused benchmarks, particularly MMLU-Pro(refer to Appendix J for details). Finally,\\nwe evaluate the model\u2019s ability to recognize patterns and extract reasoning rules through zero-shot\\nand three-shot \\\"only questions\\\" experiments. In the zero-shot setting, models rely solely on prior\\nknowledge, often struggling with accuracy due to insufficient information. In the three-shot setting,\\nmodels infer rules from three examples, improving performance, as detailed in Appendix K.\\n7 C ONCLUSION\\nBy maintaining orthogonality with domain-specific knowledge, we introduce KOR-Bench to evalu-\\nate models\u2019 reasoning abilities in reading comprehension, immediate learning, knowledge transfer,\\nlogical reasoning, and problem-solving, while minimizing the influence of pre-existing knowledge.\\nKOR-Bench provides substantial differentiation and poses a significant challenge, as evidenced by\\nO1-Preview and O1-Mini achieving 72.88% and 70.16%, respectively, while advanced models like\\nClaude-3.5-Sonnet and GPT-4o score only 58.96% and 58.00%. We aim for KOR-Bench to be a\\ncomprehensive and challenging benchmark that evaluates models\u2019 reasoning abilities while decou-\\npling them from intrinsic knowledge, ultimately advancing research in reasoning and planning.\\n10\",\"annotations\":null}", "tool_call_id": "toolu_01ApgXzFYc7UNtQqwpKk4ZBe"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 36\\nExtracted page range: 1-5\\n\\n=== Page 1 ===\\n2025-5-7\\nBIG-Bench Extra Hard\\nMehran Kazemi1, Bahare Fatemi2, Hritik Bansal1,3, John Palowitch1, Chrysovalantis Anastasiou1, Sanket\\nVaibhav Mehta1, Lalit K. Jain2, Virginia Aglietti1, Disha Jindal1, Peter Chen1, Nishanth Dikkala2, Gladys Tyen1,\\nXin Liu2, Uri Shalit1, Silvia Chiappa1, Kate Olszewska1, Yi Tay1, Vinh Q. Tran1, Quoc V. Le1and Orhan Firat1\\n1Google DeepMind,2Google Research,3UCLA\\nLarge language models (LLMs) are increasingly deployed in everyday applications, demanding robust\\ngeneral reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning bench-\\nmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader\\nreasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a\\ncrucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of\\nchallenging tasks that allowed for a comprehensive assessment of general reasoning across various skills\\nwithin a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and\\nits harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many\\ntasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard\\n(BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces\\neach task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly\\nincreased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of\\n9.8% for the best general-purpose model and 44.8% for the best reasoning-specialized model, indicating\\nsubstantial room for improvement and highlighting the ongoing challenge of achieving robust general\\nreasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh .\\n1. Introduction\\n05101520253035404550\\nHarmonic Mean Accuracy (%)Random\\nLlama-3.1-8B-Instruct\\nGemma2-27B-IT\\nGemma3 27B\\nGemini-2.0-Flash-Lite\\nGemini-2.0-Flash\\nGPT-4o\\nDistill-R1-Qwen-32B\\nDeepseek R1\\no3-mini(high)general-purpose models\\nreasoning modelsPerformance on BBEH\\nFigure 1|Model performances on BBEH (harmonic mean\\nover individual task performances).Recent research has made significant\\nstrides in evaluating the reasoning capa-\\nbilities of large language models (LLMs),\\nbut the focus has been disproportion-\\nately skewed towards math/science and\\ncoding. This emphasis is likely driven\\nby the availability of challenging bench-\\nmarks in these domains and the relative\\nease of evaluating quantitative solutions.\\nHowever, reasoning encompasses a far\\nbroader spectrum of cognitive skills, in-\\ncluding logical deduction, temporal and\\nspatial understanding, commonsense reasoning, and even the ability to comprehend humor.\\nTo assess these diverse facets of reasoning, the community has relied on the BIG-Bench benchmark\\n(Srivastava et al., 2022), specifically its more challenging subset, BIG-Bench Hard (BBH) (Suzgun\\net al., 2022). BBH has served as the de facto standard for evaluating general reasoning in LLMs due\\nto its versatility and the wide array of reasoning skills it probes. However, the rapid advancements in\\nLLM development has led to a saturation of BBH, with state-of-the-art models achieving over 90%\\naccuracy. This performance ceiling renders BBH less effective in discriminating between the reasoning\\nabilities of the latest generation of LLMs.\\nLlama and Qwen experiments in this paper were conducted only by parties outside of Google.\\nCorresponding author(s): mehrankazemi@google.com and baharef@google.com\\n\u00a92025 Google DeepMind. All rights reservedarXiv:2502.19187v2  [cs.CL]  6 May 2025\\n\\n=== Page 2 ===\\nBIG-Bench Extra Hard\\nTo address this challenge and push the boundaries of LLM evaluation, we introduce BIG-Bench\\nExtra Hard (BBEH), a new benchmark designed to assess advanced reasoning capabilities. BBEH\\nbuilds upon BBH by replacing each of the 23 tasks from BBH with a novel counterpart that probes\\nsimilar reasoning capabilities, but exhibits significantly increased difficulty. Solving the tasks in BBEH\\nrequires even further reasoning skills than the problems in BBH. These skills include, but are not\\nlimited to, many-hop reasoning, learning on the fly, finding errors in reasoning traces, processing\\nlong-context inputs and finding (multi-)needles in a haystack, going against strong prior, dealing\\nwith long-range dependencies, dealing with distractors and inducing patterns from examples. By\\npresenting LLMs with problems that demand a wider array of reasoning capabilities, BBEH aims to\\nprovide a more accurate measure of their general reasoning abilities.\\nWe provide a comprehensive evaluation of several state-of-the-art LLMs on BBEH. Our results\\ndemonstratethatBBEHpresentsasignificantchallengeevenforthemostadvancedmodels. Specifically,\\nwe observe a ceiling accuracy of 23.9% for the best general-purpose model and 54.2% for the model\\nreasoning-specializedmodel,highlightingthesubstantialheadroomforimprovementinrobustgeneral\\nreasoning for LLM. Furthermore, we conduct a detailed failure analysis, revealing intriguing failure\\nmodes for both general-purpose LLMs and models specifically designed for reasoning and thinking.\\nThese findings offer valuable insights into the current limitations of LLMs and provide guidance for\\nfuture research aimed at enhancing their reasoning capabilities.\\n2. Related Work\\nThere has been significant emphasis on using LLMs for mathematical and scientific reasoning. This has\\nled to the popularity and proliferation of math- and STEM-based evaluations, such as (Cobbe et al.,\\n2021; Hendrycks et al., 2020), and more recently, (Glazer et al., 2024; Phan et al., 2025). However,\\nthe generalizability of mathematical reasoning skills to broader domains remains unclear. Indeed,\\nattempts to make existing benchmarks more robust\u2014for example, Mirzadeh et al. (2024)\u2014have\\nhighlighted an overall lack of robustness and logical reasoning capabilities. Several benchmarks have\\nalso been developed to address specific areas of reasoning, including temporal reasoning (Beniwal\\net al., 2024; Dhingra et al., 2022; Xiong et al., 2024), spatial understanding (Bohnet et al., 2024;\\nMirzaee et al., 2021; Shi et al., 2022; Yamada et al., 2023), commonsense reasoning (Sakaguchi\\net al., 2021; Talmor et al., 2018; Zellers et al., 2019), and logical reasoning (Parmar et al., 2024;\\nSaparov and He, 2022; Saparov et al., 2023; Tafjord et al., 2020). However, these benchmarks tend\\nto focus narrowly on specific domains, leading to potential evaluation biases if a more holistic view\\nof model capabilities is not considered. To address this limitation, several benchmarks have been\\ndeveloped to integrate multiple tasks into a single evaluation framework, including (Hendrycks et al.,\\n2020; Kazemi et al., 2024; Lu et al., 2023; Parmar et al., 2024; Srivastava et al., 2022; Wang, 2018;\\nWang et al., 2019, 2024; Weston et al., 2015). Our work builds on this line of research, introducing\\na new set of challenging tasks for future model evaluation and performance improvement. The\\nmulti-task nature of our benchmark with fine-grained tasks each focused on some reasoning skills\\nenables model developers to discover and analyze failure modes in further depth. Note that while\\nprivate initiatives such as ChatBot Arena (lmarena) and the SEAL leaderboard (ScaleAI) conduct\\nmodel evaluations across various aspects, they may suffer from several potential issues as pointed\\nout in Bansal and Maini (2024). Our benchmark provides an open evaluation framework with an\\nautomatic and deterministic scoring mechanism, ensuring full transparency and reproducibility for\\nthe broader research community.\\n2\\n\\n=== Page 3 ===\\nBIG-Bench Extra Hard\\n3. Going Beyond BIG-Bench Hard\\nBIG-Bench, and its later subset BIG-Bench Hard (BBH), have served the research community for\\nseveral years as de facto benchmarks for measuring general reasoning capabilities. Here, we mainly\\nfocus on BBH which has been favored in the recent years. We believe three key factors in the success\\nof BBH were the following: 1- it was challenging even for the frontier models at the time, thus helping\\nreveal model shortcomings and areas for improvement, 2- despite being composed of various tasks, all\\nthe questions in BBH were in a way that allowed for simple correctness verification of model responses\\nwith a few simple rules, and 3- perhaps most importantly, it tested for a vast array of reasoning skills\\nmaking it a good proxy for measuring general reasoning. The latter factor is in contrast to many\\nexisting reasoning benchmarks that test only for a few of these skills as explained in the related work\\nsection.\\nDespite the great success of BBH and its widespread use, it also exhibited some key limitations.\\nFirstly, a random chance baseline already has a high performance on BBH due to the high number\\nof tasks with limited options (8/23 of the tasks have binary labels, and another 5/23 have at most\\n5 options to choose from). Secondly, some of the problems in BBH can be solved using shortcuts\\nwithout solving the problem (for example, in the geometric shapes task, whenever there are three L\\ncommands the answer is triangle). Thirdly, while real-world reasoning problems typically require\\nprocessing large amounts of input texts, the input lengths of the problems in BBH are often quite\\nshort with a macro average of 700 characters across tasks. Fourthly, since the problems were made to\\nbe challenging for the LLMs of the time, they typically require only few hops of reasoning. And finally,\\nwhile BBH tests for a quite large and diverse set of skills, the set can be greatly expanded to include\\neven more reasoning skills. The fast pace of improvement in the reasoning ability of the latest LLMs\\ncoupled with the limitations of the BBH dataset outlined above has led to BBH getting saturated with\\nlatest models achieving accuracies in the range of 90+ percent on it, thus causing BBH to disappear\\nfrom the latest model evaluation reports.\\nWe create a challenging benchmark for measuring the general reasoning capability of LLMs by\\nusing BBH as a guide: we preserve the positive aspects of BBH and lift the limitations as much as\\npossible. Specifically, wecreateabenchmarkthatischallengingforthefrontiermodels,thecorrectness\\nof the model responses can be verified automatically despite being composed of several sub-tasks,\\nthe random chance baseline has a much lower success rate and the shortcuts are removed from the\\nproblems as much as possible, problems require processing longer inputs and require significantly\\nmore hops of reasoning to be solved, and, perhaps most importantly, covers a wide array of reasoning\\nskills including those in the original BBH dataset and beyond. Through studying the 23 tasks in\\nBIG-Bench Hard (BBH), we identify that for a reasoner to perform well on BBH it needs to have\\nat least the following broad reasoning skills: 1- temporal understanding, 2- spatial and geometric\\nunderstanding, 3- commonsense understanding, 4- humour understanding, 5- causal understanding,\\n6- reasoning about world entities and events, 7- deductive logical reasoning, 8- reasoning through\\nlinguisticknowledge, 9-countingandfiltering, 10-datastructuresandalgorithms, and11-performing\\narithmetic operations.\\nFor our benchmark, we wish to preserve the high diversity of the reasoning skills and capabilities\\nfrom BBH and also expand upon it by including more of the following skills. 1- Many-hop reasoning:\\nThe ability to solve a problem that require many steps/hops, 2- Very long-range dependency: the\\nability to remember and use information that have been provided or concluded much earlier in the\\ncontext, 3- Going against strong prior: the ability to reason through a problem even when it does\\nnot match the prior beliefs (McKenzie et al. (2023) report inverse scaling behaviours in such cases),\\n4-Learning on the fly: the able to learn something new on the fly (i.e. from the information in the\\ncontext) and apply it to solve a problem, 5- Dealing with distractors: The ability to identify the\\n3\\n\\n=== Page 4 ===\\nBIG-Bench Extra Hard\\nSpatial Reasoning\\nYou have been given a diamond tile map consisting of N rows\\n[...] There is a unique object placed at each vertex. [...] You\\nare initially at the top corner where you see a football. Then\\nyou move down-right for one step and see a shampoo. Then\\nyou move down-left for one step and you see a cat. [...] Then,\\nyou jump to a random vertex V where you see a bear. Then you\\nmove [...] Then you move up-left and you see a shampoo. Then\\nyou jump back to the random vertex V and do the following\\nmoves: down-left, down-left, down-right, up-left, down-left, up-\\nleft. What will you find?Buggy Tables\\nI have a table with 30 rows (including the header) and 18\\ncolumns. The table was converted to Markdown format as fol-\\nlows:<TABLE IN MARKDOWN FORMAT> . However, the\\ncode used to convert the table into Markdown format was\\nbuggy and mistakenly replaced some values with \\\"ERROR\\\" .\\nThe correct values for those cells in row-order are respec-\\ntively as follows: [9,10,null,12,..., 29]. Compute the abso-\\nlute difference between the mean of coding_minutes and\\nexercise_minutes , considering only the days where: 1- The\\nnumber of meetings was greater than 2.\\nCausal Understanding\\nReagent X is being added to a tank. At each time interval, a\\ndrop of Reagent X is being added. At one point the tank over-\\nflows. Is adding the last drop of Reagent X a necessary cause\\nfor the tank to overflow?Word Sorting\\nConsider a new alphabet whose letters have the same order as\\nthe English alphabet, except that r and p are swapped. Sort the\\nfollowing words with the new alphabet and separate them with\\ncomma: syndrome, therefrom, [...], specifications.\\nMultistep Arithmetic\\nConsider the following new operations:\\n\ud835\udc4e[]\ud835\udc4f=[...]\\n\ud835\udc4e@\ud835\udc4f=(\\n(\ud835\udc4e\u2212\ud835\udc4f)\u2217\ud835\udc4f,if\ud835\udc4e[]\ud835\udc4f <2\\n(\ud835\udc4f\u2212\ud835\udc4e)\u2217\ud835\udc4e,otherwise\\nFor brevity, we use \ud835\udc4e\u27e8\ud835\udc5c\ud835\udc5d1\u27e9\u27e8\ud835\udc5c\ud835\udc5d2\u27e9\ud835\udc4fto denote(\ud835\udc4e\ud835\udc5c\ud835\udc5d1\ud835\udc4f)\ud835\udc5c\ud835\udc5d2\ud835\udc4f.\\nLet A = ((((1 @*+ 4) <>+[] (-4 *<>* -1)) @; ((-1 <> six)\\n; (2 ;* one))) @@@ (((five ;- five) []@@ (-8 - one)) ; ((two +;\\n-5) +[]- (three - -8)))). Let B = [...]. Let C = [...]. Compute:\\nA + B - C.BoardgameQA\\nA few players are playing a boardgame. The current state of\\nthe game is as follows. The bee has a football with a radius of\\n15 inches. The chihuahua has a smoothie. [...]\\nRule1: Anyone who [...] has to pay $$$to the beaver.\\nRule2: If the stork [...], then it does not pay $$$to beaver.\\nRule 2 is preferred over Rule 1. If a rule is preferred over the\\nother, it means whenever both of them can be applied to derive\\nnew conclusions and those conclusions contradict with each\\nother, we should go with the conclusion from the rule with\\nhigher preference.\\nWhat is the truth value of the statement: \\\"Does the finch shout\\nat the mermaid?\\\"\\nDyck Language\\nYou are given an initial Dyck language sequence and the steps,\\nprovided as thoughts, that were used to arrive at the closing\\nbracket sequence in the Dyck language. Your job is to identify\\nthe first step that was a mistake.\\nTask: Complete the rest of the sequence, making sure that the\\nparentheses are closed properly.\\nInput: ( < < > [ (\\nThoughts: Thought 1: We should process each input one by\\none and keep track of the stack configuration. Thought 2:\\nstack: empty Thought 3:(; stack:([...]Time Arithmetic\\nLet the answer to Q1 be \ud835\udc4band the answer to Q2 be Y.\\nQ1: Clara and William were born on 2015-Aug-24 and\\n2016-May-20 respectively. When William was 326 days old,\\nhow old was Clara in days?\\nQ2: <TEXT OF THE QUESTION>\\nDefine: X\u2019 = X + 3, Y\u2019 = Y - 568\\nQ3: Alan and Mary tried a new restaurant on Aug Y\u2019, 1997\\nand really liked it. They decided to go to the same restaurant\\nevery\ud835\udc4b\u2032days. If today is Oct 11, 1997 , when is the next time\\nthey will go to that restaurant?\\nShuffled Objects\\nAlice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at\\na square dance. At the start of a song, they each have a part-\\nner: Alice is dancing with Rodrigo, Bob is dancing with Jamie,\\nClaire is dancing with Patrick, Dave is dancing with Lola, Eve is\\ndancing with Izzi, Fred is dancing with Melissa, and Gertrude\\nis dancing with Karl. Throughout the song, the dancers often\\ntrade partners. First, Fred and Bob switch partners. Then, Fred\\nand Claire switch partners (let\u2019s call it Action 1). Then, Claire\\nand Bob switch partners (let\u2019s call it Action 2). [...] Then, Ac-\\ntion 1 repeats. Then, Fred and Claire switch partners. [...] At\\nthe end, who is Fred dancing with?Geometric Shapes\\nSuppose we draw this SVG path element: M -18.79152,1.82096\\nL -8.48528,-8.48528 M -32.29803,24.74825 [...] . Out of the\\nfollowing shapes:\\n1. rectangle that is not a square and with no diagonals\\ndrawn\\n2. square with no diagonals drawn\\n...\\nWhich one(s) can be viewed when the lines in the SVG are\\nvisualized? Options: (A) 1 and 3 (B) only 5 [...].\\nFigure 2|Sample Questions from 10 tasks. The text has been shortened due to lack of space.\\n4\\n\\n=== Page 5 ===\\nBIG-Bench Extra Hard\\nparts of information provided that are necessary for solving the problem, and not getting distracted by\\nthe redundant information, 6- Long-context: the ability to reason through a long input context and\\nstitching different pieces of input together, 7- Needle in a haystack: the ability to find small pieces\\nof relevant information from a large input (e.g., finding one value from a large table), 8- Finding\\nerrors in reasoning traces: the ability to identify the errors in a chain of reasoning not produced by\\nthe model itself, 9- Inductive reasoning: the ability to induce patterns from a number of examples\\nand being able to apply the same pattern to a new instance of a problem, 10- Contraint satisfaction:\\nthe ability to understand constraints and find a solution that satisfies them, 11- Compositional\\nunderstanding: the ability to solve multiple independent problems that are fused into one problem\\n(Hosseini et al. (2024) and Miner et al. (2024) report higher failure rates than expected in the\\ncompositional case), and 12- Knowledge-intense reasoning: the ability to reason in domains where\\na great amount of domain knowledge is needed.\\n4. BIG-Bench Extra Hard\\nWe create BIG-Bench Extra Hard (BBEH), a dataset that tests the general reasoning capability of\\nmodels on a wide array of reasoning skills. To this end, we build on the success of BBH and replace\\neach of the 23 tasks in BBH with another task that is in a similar reasoning domain and tests for\\nsimilar (or more) skills, but is more challenging compared to the original one. Replacing each task\\nwith another one in the same domain that tests for similar capabilities ensures that we preserve the\\nhigh diversity of original BBH dataset. In Table 1, we outline a high-level description of the new tasks\\nin BBEH, how they have been constructed and which task from BBH the replace, and what reasoning\\nskills they target. The benchmark contains 200questions per task, except for the Disambiguation QA\\ntask where we have 120questions. For more details about the tasks and some intuitions from the\\nexperimental results and model failure modes, see Appendix A. Samples from a few of our tasks are\\nprovided in Figure 2.\\nA key challenge in creating benchmarks is ensuring they remain difficult for frontier models. This\\nis particularly true for reasoning benchmarks, given the rapid progress in the field over the past\\nyear, and especially for BBEH, which comprises 23 distinct tasks, each requiring careful design. To\\nensure our tasks challenge frontier models, we adopted a semi-adversarial approach. We selected\\ntwo strong reference models: one general-purpose and one specialized in reasoning. We iteratively\\nincreased task difficulty while keeping in mind the extra skills that we wanted our benchmark to test\\nfor, evaluating the reference models on each new iteration. If a task proved insufficiently challenging,\\nwe either replaced it with another task or added extra types of difficulty and re-evaluated until the\\ndifficulty level was satisfactory. We used Gemini 1.5 Flash (Team et al., 2024a) as our general-purpose\\nreference model and the Gemini Thinking Experimental model as our reasoning-specialized reference\\nmodel (initially the December 2024 version but later changed to the January 2025 version, known as\\nGemini-2.0-Flash-Thinking-Exp-01-21). These models were chosen for their performance and the\\nspeed of generating outputs, which facilitated rapid iteration during task construction. We iterated\\non each task until both reference models achieved an accuracy below 70%.\\nIn most cases, we tried to use the reference models only as a black box that provided feedback\\non the difficulty of our tasks. In some cases, however, making tasks more difficult required looking\\ninto the approach adopted by the model. As an example, the original \\\"Boolean Expression\\\" task in\\nBBH required models to evaluate the truth value of expressions such as (not True) or False . Our initial\\nattempt to increase difficulty involved creating longer expressions with significantly more clauses.\\nHowever, our reference model achieved high accuracy regardless of the number of clauses. While\\ninitially this seemed surprising, upon investigating the model\u2019s approach, we discovered it cleverly\\nused Python to solve the problem by directly evaluating the expression: result = <expression>;\\n5\",\"annotations\":null}", "tool_call_id": "toolu_01Wf8K1cjQsZfXDLjzqTTTNW"}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 17\\nExtracted page range: 1-10\\n\\n=== Page 1 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\nBill Yuchen Lin1Ronan Le Bras2\\nKyle Richardson2Ashish Sabharwal2Radha Poovendran1Peter Clark2Yejin Choi3\\n1University of Washington2Allen Institute for AI3Stanford University\\nbyuchen@uw.edu ronanlb@allenai.org yejinc@stanford.edu\\nhttps://hf.co/spaces/allenai/ZebraLogic\\nAbstract\\nWe investigate the logical reasoning capabilities\\nof large language models (LLMs) and their scal-\\nability in complex non-monotonic reasoning. To\\nthis end, we introduce ZebraLogic, a comprehen-\\nsive evaluation framework for assessing LLM rea-\\nsoning performance on logic grid puzzles derived\\nfrom constraint satisfaction problems (CSPs). Ze-\\nbraLogic enables the generation of puzzles with\\ncontrollable and quantifiable complexity, facili-\\ntating a systematic study of the scaling limits of\\nmodels such as Llama, o1 models, and DeepSeek-\\nR1. By encompassing a broad range of search\\nspace complexities and diverse logical constraints,\\nZebraLogic provides a structured environment\\nto evaluate reasoning under increasing difficulty.\\nOur results reveal a significant decline in accuracy\\nas problem complexity grows\u2014a phenomenon we\\nterm the \u201ccurse of complexity.\u201d This limitation\\npersists even with larger models and increased\\ninference-time computation, suggesting inherent\\nconstraints in current LLM reasoning capabilities.\\nAdditionally, we explore strategies to enhance\\nlogical reasoning, including Best-of-N sampling,\\nbacktracking mechanisms, and self-verification\\nprompts. Our findings offer critical insights into\\nthe scalability of LLM reasoning, highlight fun-\\ndamental limitations, and outline potential direc-\\ntions for improvement.\\n1University of Washington2Allen Institute for AI\\n3Stanford University. Correspondence to: Bill Yuchen Lin\\n<byuchen@uw.edu >.\\nProceedings of the 42ndInternational Conference on Machine\\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\\nby the author(s).1. Introduction\\nLogical reasoning stands as a cornerstone of human intel-\\nligence and remains a central challenge in AI. While re-\\ncent advances have demonstrated promise in tasks requiring\\ncommon sense and general knowledge (Brown et al., 2020;\\nChowdhery et al., 2022; Bubeck et al., 2023), the capabili-\\nties of Large Language Models (LLMs) in handling complex\\ndeductive problems remain uncertain. This limitation in our\\nunderstanding is especially critical as systematic reasoning\\nunderpins many real-world applications. To systematically\\nstudy LLMs\u2019 logical reasoning capabilities and their scal-\\ning limits, an ideal evaluation framework must: (1) isolate\\npure logical reasoning from domain knowledge; (2) enable\\nprecise control over problem complexity; (3) minimize data\\nleakage to prevent training data memorization; (4) provide\\nobjective metrics for assessing an LLM\u2019s reasoning results.\\nConstraint satisfaction problems (CSPs) offer such a con-\\ntrolled framework (Dechter, 2003): they are mathematically\\nwell-defined, scalable in both complexity and search space,\\nand have solutions that can be automatically verified. By for-\\nmulating logical tasks as CSPs, we can rigorously evaluate\\nhow well LLMs adhere to logical constraints, independent\\nof domain-specific data or heavy numerical computation.\\nAs a representative class of CSPs, logic grid puzzles (specif-\\nically Zebra Puzzles or Einstein\u2019s Riddle, (Prosser, 1993))\\nare particularly suitable as they require pure formal rea-\\nsoning, remain accessible enough to serve as an effective\\ntestbed, and embody core skills relevant to real-world ap-\\nplications such as task planning, scheduling, and resource\\nallocation. Hence, we introduce ZebraLogic , an evaluation\\nframework for creating logic puzzles with controllable, and\\nquantifiable complexity, thus improving our understanding\\non the scaling limits of LLMs including Llama (AI@Meta,\\n2024), o1 (OpenAI, 2024) and R1 (DeepSeek-AI, 2025).1\\nThrough extensive evaluation of various LLMs across di-\\n1The closest related effort is by Tyagi et al. (2024), who focused\\non a detailed analysis of the kinds of errors LLMs make when\\nsolving grid puzzles using reasoning chains; more details in \u00a77.\\n1arXiv:2502.01100v2  [cs.AI]  15 Jul 2025\\n\\n=== Page 2 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\n# Z3 conflicts \u2b0a405B70B8B3B\\n4oO1-miniO1\\n40\\n4040O1-miniScaling Model SizeScaling CoT TokensScaling Sample Size\\nR1O1-previewThe curse of complexity!\\nFigure 1: Accuracy vs number of Z3 conflicts for Llama-3 (left), showing the size scaling effect on the reasoning performance.\\nThe middle figure shows the curves for gpt-4o(-mini) vs o1 and R1, showing the scaling effect of model size and test-time\\ncompute. The right figure shows the scaling effect of repeated sampling by pass@k metric with different sample sizes.\\nverse architectures and sizes, we observe a dramatic de-\\ncline in performance as puzzle complexity increases\u2014a\\nphenomenon we term the \u201c curse of complexity for reason-\\ning.\u201d Most models struggle once the puzzle\u2019s search space\\nexceeds 107possibilities (e.g., for puzzles with 4x5 grid\\nsize) or when the number of logical conflicts in a widely\\nused SMT solver named Z3 (de Moura & Bj\u00f8rner, 2008)\\nsurpasses 20. These findings suggest that limited reason-\\ning in current LLMs are not solely a matter of model- or\\nsample-size scaling, but also arise from insufficient test-time\\ncompute. This shortfall underscores the need to train LLMs\\nto reason step by step (Wei et al., 2022) explicitly (e.g., via\\nreinforcement learning (Lambert et al., 2024a)), as exem-\\nplified by emerging reasoning models such as o1 and R1.\\nSpecifically, we conduct a systematic investigation into the\\nscaling behavior of LLMs in logical reasoning, focusing on\\nthree key dimensions: model size ( \u00a74), sampling ( \u00a75), and\\ntest-time compute ( \u00a76). Understanding scaling behavior of\\nLLMs in reasoning is critical to identify the most promising\\ndirections for advancing LLMs\u2019 reasoning capabilities and\\nto guide future research efforts more effectively.\\nOur work makes the following key contributions:\\n\u2022We create the ZebraLogic dataset, a benchmark of 1,000\\nlogic grid puzzles spanning multiple complexity levels,\\ndesigned to evaluate LLMs\u2019 logical reasoning capabilities\\nsystematically with two complexity metrics: search space\\nsize and Z3 conflict count (\u00a72).\\n\u2022We report \u201cthe curse of complexity\u201d in logical reason-\\ning with LLMs: the performance dramatically declines\\nas the problem complexity increases and after a certain\\nthreshold, most models struggle to solve any logical puz-\\nzle. This limitation persists even when scaling to signifi-\\ncantly larger models (such as Llama-3.1-405B) or using\\nenhanced training data, indicating a deeper challenge thatcannot be resolved by model scaling alone (\u00a73 and \u00a74).\\n\u2022We scale the test-time compute of LLMs by increasing the\\nnumber of generation samples, revealing that it has both\\npromise and challenges. While Best-of-N sampling can\\nimprove potential performance, practical selection meth-\\nods like majority voting or reward models show limited\\nimprovement. Additionally, even pass@128 cannot break\\nthe curse of complexity (\u00a75).\\n\u2022We find that it\u2019s much more promising to scale up the\\nreasoning tokens (i.e., chain-of-thoughts; CoTs) generated\\nduring inference with a backtracking mechanism. We\\ntake OpenAI\u2019s o1 models as a typical example and show\\nthat they generate significantly more, nearly 10x (hidden)\\nreasoning tokens than others, which scale properly with\\nproblem complexity. Based on our empirical results, we\\nalso find that there exists an optimal ratio of reasoning\\ntokens to Z3 conflicts, but O1-like models cannot always\\nreach this optimal ratio when the complexity is extremely\\nhigh, thus not achieving perfect reasoning (\u00a76).\\n\u2022Moreover, we explore the potential of using self-\\nverification prompting to improve LLMs ( \u00a76.2). We find\\nthat such methods can help LLMs improve their perfor-\\nmance, but the improvement is very marginal. We fur-\\nther analyze the reasoning process of o1 and discuss its\\nstrengths and weakness in logical reasoning (\u00a7D).\\n2. Problem Formulation of Logical Reasoning\\nConstraint Satisfaction Problems (CSPs) provide a power-\\nful framework for modeling and solving logical reasoning\\ntasks. In CSPs, solutions must satisfy a set of constraints\\nover variables and their possible values. This framework\\nis particularly valuable for evaluating systematic reason-\\ning capabilities, as it requires explicit handling of logical\\nrelationships and dependencies. We leverage this frame-\\n2\\n\\n=== Page 3 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street. Each house is \\noccupied by a different person. Each house has a unique value for each of the following attributes:\\n- Each person has a unique name :Eric ,Peter ,Arnold\\n- Each person has a unique favorite drink : milk, water, tea\\n- Each person has a unique hobby : photography, cooking, gardening\\n1. Arnold  is not in the first  house.\\n2. The person who likes milk  is Eric .\\n3. The photography  enthusiast is not in the first  house.\\n4. The person who loves cooking  is directly left of the \\nperson who likes milk .\\n5. The one who only drinks water  is Arnold .\\n6. The person who likes milk  is not in the second  house.\\nSolutionBackground\\nCluesClue 2  + Clue 5 \\nClue 1 2 Arnold\\nClue 3 photography 3proof by contradictionmilk ? ?\\nArnold water ? ?Eric\\nEric 3 milk2 cooking ? ?\\n?Clue 6 \\nClue 4 Uniqueness\\ntea ? Peter\\nmilk 1,3 Eric\\nSolutionUniqueness\\nFigure 2: This example of ZebraLogic features 3 houses (N=3) and 3 attributes (M=3), with 6 clues (K=6). The Background\\noutlines the attributes, their possible values, and the uniqueness constraints. The Clues provide additional constraints\\nregarding the attributes . The task for the model is to determine the correct assignment of attributes to each house based on\\nthese clues, as illustrated in the Solution grid.\\nwork through logic grid puzzles in our ZebraLogic dataset\\nto assess LLMs\u2019 deductive reasoning abilities.\\n2.1. Logic Grid Puzzles\\nEach puzzle in ZebraLogic consists of Nhouses (numbered\\n1 toNfrom left to right) and Mdifferent attributes for\\neach house. There are Ndistinct values for each attribute,\\nand each house must have a unique value for each attribute.\\nGiven a list of Kclues, one must use logical deduction to\\ndetermine the unique correct assignment of values. Figure 2\\nillustrates an example of such a puzzle, as well as a reason-\\ning chain for solving it. Importantly, while some ZebraLogic\\npuzzles can be solved through straightforward linear deduc-\\ntion, many require more complex non-monotonic reasoning\\nstrategies, such as counterfactual reasoning that involves\\nbacktracking and revising assumptions. This is particularly\\ntrue as the search space grows larger and the clues become\\nmore intricate \u2013 a key aspect of our study on the scaling\\nbehavior of LLMs.\\n2.2. Problem Formulation\\nWe provide a detailed mathematical formulation of logic\\ngrid puzzles as a CSP. This formulation not only clarifies\\nthe underlying structure of the puzzles in ZebraLogic but\\nalso highlights how our study can be generalized to various\\nreasoning problems. The example shown in Fig. 2 illustrates\\nthis formulation.\\nBackground. Consider Nhouses numbered 1 to N. Each\\nhouse has a different occupant with a set AofMunique\\nattributes such as name, favorite drink, hobby, etc. Each\\nattribute a\u2208 A represents a category of characteristics\\nand takes values in a set VaofNpossible values. Forexample, for the attribute Name , we might have VName =\\n{Eric,Peter,Arnold }in a puzzle with N= 3 houses. As\\nillustrated in Fig. 2, other attributes might include Drink\\nwith values like milk, water, and tea, or Hobby with val-\\nues like photography, cooking, and gardening. To model\\nthe puzzle as a Constraint Satisfaction Problem, we define\\nvariables representing the assignment of values to attributes\\nfor each house.\\n\u2022 Let H={1,2,3,\u00b7\u00b7\u00b7} be the set of houses, |H|=N.\\n\u2022LetA={Name ,Drink ,\u00b7\u00b7\u00b7} be the set of attributes, |A|=M.\\n\u2022 Define xa,k\u2208 Vafor each attribute a\u2208 A and house k\u2208H.\\nUniqueness Constraints: The constraints ensure that each\\nvalue is assigned exactly once, as described in the Back-\\nground part in Figure 2. For each attribute, the set of as-\\nsigned values across all houses must exactly match the set\\nof possible values. That is: {xa,k|k\u2208H}=Va.\\nClue-Based Constraints: Each clue in the puzzle intro-\\nduces additional constraints that must be satisfied by any\\nvalid assignment. Note that there are also several implicit\\npositional constraints that must be considered. For example,\\nthe leftmost house cannot be on the right of any other house,\\nand the rightmost house cannot be on the left of any other\\nhouse (as relevant in Clue 4). These spatial constraints,\\ncombined with the explicit clues, translate the verbal de-\\nscriptions into precise logical conditions to be satisfied..\\nUnder the hood, these clues are translated into formal logic\\nformulas that constrain the relationships between variables.\\nFor our example puzzle in Figure 2, the constraints can be\\nformulated as follows:\\nTask. The task is to find an assignment of attributes to\\nhouses via assigning values to variables xa,kthat is consis-\\ntent with all constraints. These constraints, defined above,\\n3\\n\\n=== Page 4 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\ninclude both the uniqueness requirements for attribute val-\\nues and the logical conditions derived from the specific clues\\nprovided. The result is guaranteed to be unique, and can be\\nusually presented as a table as shown in Fig. 2.\\nClue-based Constraints (Example in Figure 2.\\nClue 1. \u201cArnold is not in the first house\u201d :xName,1\u0338=Arnold\\nClue 2. \u201cThe person who likes milk is Eric\u201d :\u2200k\u2208\\nH,(xName,k=Eric)\u21d0\u21d2 (xDrink,k=milk)\\nClue 3. \u201cThe photography enthusiast is not in the first\\nhouse\u201d :xHobby,1\u0338=photography\\nClue 4. \u201cThe person who loves cooking is directly left of the\\nperson who likes milk\u201d :\u2200k\u2208H<N,(xHobby,k=\\ncooking ) =\u21d2(xDrink,k+1=milk)\\nClue 5. \u201cThe one who only drinks water is Arnold\u201d :\u2200k\u2208\\nH,(xName,k=Arnold )\u21d0\u21d2 (xDrink,k=water )\\nClue 6. \u201cThe person who likes milk is not in the second\\nhouse\u201d :xDrink,2\u0338=milk\\n2.3. ZebraLogic Dataset Creation\\nTo create puzzles, we first define a set of attributes and\\ntheir corresponding value sets. We also establish some\\nclue types, each with its own language templates containing\\nplaceholders for values.\\nAttributes and Values. We construct the attribute set A,\\nwhich includes the many elements (see Appendix B). Each\\nattribute is associated with a minimum of 6 possible values,\\nensuring a rich and diverse set of puzzles. Importantly,\\nwe always include the Name attribute in our samples, as it\\nserves as a crucial element in the puzzle-solving process.\\nClue Types. The possible clue types are categorized into\\nseveral types, including FOUND AT,SAME HOUSE ,NOTAT,\\nDIRECT LEFT/RIGHT ,SIDEBYSIDE,LEFT/RIGHT OF, and\\nONE/TWOBETWEEN . Each clue type captures a specific\\nrelationship between variables, providing a diverse set of\\nconstraints for the puzzles. More details are in Appendix B.\\nClue Types and Illustrative Examples.\\n\u2022FOUND AT: The tea drinker lives in House 3.\\n\u2022SAME HOUSE : The musician drinks tea.\\n\u2022NOTAT: The musician does not drink tea (not at the same\\nhouse).\\n\u2022DIRECT LEFT/RIGHT : The greenhouse is directly to the\\nleft/right of the white house.\\n\u2022SIDEBYSIDE: The coffee drinker and the tea drinker are\\nnext to each other.\\n\u2022LEFT/RIGHT OF: A is somewhere to the left/right of B.\\n\u2022ONE/TWOBETWEEN : 1/2 houses are between A & B.\\nTask Generation Algorithm. Algo. 1 outlines our approach\\nfor generating ZebraLogic puzzles. The process starts byAlgorithm 1 ZebraLogic Puzzle Generation.\\nRequire: A set of possible attributes Aalland their value sets Va\\nfor each a\u2208 A all\\nRequire: Clue types C={c1, . . . , c L}with templates T(c)for\\neachc\u2208 C\\nRequire: Number of houses N, number of attributes M\\n1: Sample Mattributes from Aallto form A={a1, . . . , a M}\\n2: Initialize solution S:H\u00d7 A \u2192S\\na\u2208AVarandomly\\n3:C\u2190ClueGeneration (S) // Initialize clue set\\n4:while C\u0338=\u2205do\\n5: p\u2190SampleClue (C) // Sample a clue to remove\\n6: C\u2032\u2190C\\\\ {p}\\n7: if|Solutions (C\u2032)|= 1then\\n8: C\u2190C\u2032// Remove until Sis the unique solution\\n9: break\\n10: end if\\n11:end while\\n12:return (S, C ) // Return solution and minimal clue set\\nsampling Mattributes from the full attribute set and creating\\nan initial solution grid Sthrough random value assignments.\\nFrom this solution, we generate a comprehensive set of clues\\nCthat capture all valid relationships between values in the\\ngrid. The algorithm then employs an iterative minimization\\nprocedure - at each step, it randomly samples a clue p\u2208 C\\nand attempts to remove it. Using a SAT solver, it verifies\\nwhether the reduced clue set C\u2032=C \\\\ {p}still uniquely\\ndetermines the original solution S. If uniqueness is pre-\\nserved, pis permanently removed and the process continues.\\nThis iteration terminates when no any additional clue can\\nbe removed without augmenting the solution space.\\nWe employ weighted sampling during clue selection, assign-\\ning higher probabilities to simpler clue types (e.g., FOUN -\\nDAT-type clues are more likely to be sampled than NOTAT)\\nto balance puzzle complexity, such that we can efficiently\\nreduce the clue set while maintaining the difficulty of the\\npuzzles. The result is a minimal set of clues that, when\\ncombined with the background information about the at-\\ntributes and their possible values, forms a logically sound\\npuzzle with a single, unique solution. This approach en-\\nsures that each generated puzzle is both solvable and chal-\\nlenging, requiring a combination of logical deduction and\\nnon-monotonic reasoning strategies to solve. Finally, we\\nuse predefined one-shot prompting templates to format the\\npuzzle and instruct the LLMs to generate their reasoning\\nsteps and final results in a JSON format (see Appendix D.1).\\nDataset Statistics. The dataset consists of 1,000 puzzles\\nwhere the size of the search space varies significantly. The\\npuzzles are based on N\u00d7Mgrids where N, M\u2208 {2, ...,6}\\n(i.e., 25 sizes in total, with 40 puzzles per size), covering a\\nwide range of complexity. The average and median number\\nof clues per instance is 10.4and9, respectively.\\n4\\n\\n=== Page 5 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\n2.4. Theoretical Problem Complexity\\nBy reduction from the Quasigroup (or Latin square) Com-\\npletion Problem (QCP) (Colbourn, 1984; Gomes & Shmoys,\\n2002), the ZebraLogic problem is proven to be NP-complete\\n(Sempolinski, 2009). While the problem definition includes\\na rich set of clue types that can be further expanded, a suffi-\\ncient condition for the NP-completeness result is to at least\\ninclude the FOUND ATandNOTATclue types. As a result,\\nwhile a solution to a ZebraLogic puzzle can be easily ver-\\nified, solving ZebraLogic puzzles for large instances may\\nbecome intractable within reasonable time frames using cur-\\nrent computational methods. This implies that, for a fixed\\nLLM size, the required number of reasoning tokens may\\nincrease exponentially with the size of the puzzle.\\n2.5. Measuring Effective Instance Complexity\\nSearch space size. We define the solution space of a Ze-\\nbraLogic puzzle as the total number of possible configu-\\nrations that can satisfy the uniqueness constraints of the\\npuzzle. That is, a N\u00d7Mgrid has a solution space of\\n(N!)M, where Nis the number of houses and Mis the\\nnumber of attributes. The complexity of the search space\\nincreases factorially with the size of the grid, leading to\\na combinatorial explosion in the number of possible con-\\nfigurations.2To better group the puzzles based on their\\ncomplexity, we categorize them into four groups based on\\nthe size of the search space |S|:\\nSmall (|S|<103):2\u00d72, 2\u00d73, 2\u00d74, 2\u00d75, 2\u00d76, 3\u00d72, 3\u00d73, 4\u00d72\\nMedium (103\u2264 |S| <106):3\u00d74, 3\u00d75, 3\u00d76, 4\u00d73, 4\u00d74, 5\u00d72, 6\u00d72\\nLarge (106\u2264 |S| <1010):4\u00d75, 5\u00d73, 4\u00d76, 5\u00d74, 6\u00d73\\nX-Large (|S| \u2265 1010):5\u00d75, 6\u00d74, 5\u00d76, 6\u00d75, 6\u00d76\\nZ3 conflicts. While search space size provides a useful mea-\\nsure of puzzle scale, it is not the only indicator of complexity.\\nTo complement it, we also use the Z3 SMT solver\u2019s conflict\\nmetric. Z3 (de Moura & Bj\u00f8rner, 2008) uses the Conflict\\nDriven Clause Learning (CDCL) algorithm, a backtracking\\napproach based on the DPLL (Davis-Putnam-Logemann-\\nLoveland) algorithm. When solving a puzzle, Z3 records\\nthe number of conflicts encountered - situations where the\\nsolver must backtrack due to contradictions in its current\\nassignment. We run Z3 on each puzzle for 32 times and\\ntake the average number of conflicts as a measure of com-\\nplexity. Puzzles with zero conflicts can typically be solved\\nthrough simple forward chaining, whereas puzzles with\\nmore conflicts require extensive backtracking, indicating\\nhigher logical complexity.\\nWhile search space size captures the number of candidate\\nassignments (given uniqueness constraints), Z3 conflicts\\nquantify the solver\u2019s difficulty in reaching a valid solution.\\n2For example, a 3x4 grid has a solution space of (3!)4= 1296 ,\\nwhile a 4x3 grid has a solution space of (4!)3= 13824 .Together, these metrics offer a complementary view of how\\nthe difficulty of the puzzles scales with the problem size.\\nAppendix B provides additional details on how these two\\nmetrics vary as a function of the puzzle parameters ( N,M).\\n3. Evaluation\\nSetup and Metrics. Our evaluation is done in a one-shot\\nin-context learning setting, where we provide the models\\nwith a single example of how to solve a ZebraLogic puzzle\\nand present the solution in JSON format, and we instruct\\nthe LLMs to output their reasoning and solution in the same\\nformat, thus making it easier to parse and evaluate their an-\\nswers. We mainly look at the puzzle-level accuracy, mean-\\ning that only when all cells in the grid are filled correctly, the\\nmodel is considered to have solved the puzzle. In addition\\nto that, we also report the cell-level accuracy.\\nEvaluated models. We evaluate both open-weight LLMs\\n(e.g., Llama and Qwen) and proprietary LLM APIs includ-\\ning GPT-4o, O1 and Claude models. All evaluated models\\nare prompted in the same way (see Appendix D.1), and\\nwe use the same greedy decoding and prompts and parsing\\nscript across all models to ensure a fair comparison, except\\nfor O1, which does not only greedy decoding so we run it\\nthree times and take the best result.\\n3.1. Main results\\nTable 1 shows the performance of various models. o1 out-\\nperforms all other models, achieving an overall accuracy\\nof 81.0%, and DeepSeek-R1, an open-weight reasoning\\nLLM achieves 78.7%, with a slightly better performance on\\nSmall and Medium-size puzzles than o1-full. However, R1\u2019s\\nperformance on Large and X-Large puzzles is worse than o1-\\nfull. o1-preview and o1-mini achieve 71.4% and 59.7% ac-\\ncuracy, respectively. In contrast, the best-performing open-\\nweight non-reasoning LLM, Sonnet-3.5-1022, only reaches\\n36.2%. The performance gap is even more pronounced in\\nlarger search spaces, where O1-Preview maintains a 17.0%\\naccuracy in the X-Large category, while other models strug-\\ngle to achieve any correct solutions.\\nWe find that our ranking and scoring of these models\\nare aligned with other reasoning benchmarks such as\\nMATH (Hendrycks et al., 2021) for mathematical reasoning\\nand LiveCodeBench (Jain et al., 2024) for competitive pro-\\ngramming. This suggests that the logical reasoning ability\\nof LLMs is highly correlated with their performance on\\nother types of reasoning tasks.\\n3.2. Curse of Complexity in Reasoning with LLMs\\nWe observe that the performance of LLMs drops signifi-\\ncantly as the search space size increases, as shown in Fig. 1\\nand Fig. 3 (in Appendix). We find that for models that are\\n5\\n\\n=== Page 6 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\nModel NamesOverall\\nGrid-level acc.Small\\n<103Medium\\n103\u223c106Large\\n106\u223c109X-Large\\n>109Cell-level\\nAcc.\\no1-full \u1f512 81.0 97.2 92.1 78.0 42.5 78.7\\nDeepSeek-R1 \u1f511 78.7 98.4 95.7 73.5 28.5 80.5\\no1-preview \u1f512 71.4 98.1 88.2 59.5 17.0 75.1\\no1-mini \u1f512 59.7 87.5 76.8 39.0 12.0 70.3\\nClaude Sonnet 3.5 \u1f512 36.2 84.7 28.9 4.0 1.0 54.3\\nLlama-3.1-405B \u1f511 32.6 81.3 22.5 1.5 0.0 45.8\\nGPT-4o \u1f512 31.7 80.0 19.6 2.5 0.5 50.3\\nGemini-1.5-Pro \u1f512 30.5 75.3 20.7 3.0 0.0 50.8\\nMistral-Large-2 \u1f511 29.0 75.9 15.0 2.5 0.0 47.6\\nQwen2.5-72B \u1f511 26.6 72.5 12.1 0.0 0.0 40.9\\nGemini-1.5-Flash \u1f512 25.0 65.0 13.6 2.0 0.0 43.6\\nLlama-3.1-70B \u1f511 24.9 67.8 10.4 1.5 0.0 28.0\\nDeepSeek-v2.5 \u1f511 22.1 62.2 7.9 0.0 0.0 38.0\\nGPT-4o-mini \u1f512 20.1 58.8 4.6 0.0 0.0 41.3\\nGemma-2-27B \u1f511 16.3 46.6 5.0 0.0 0.0 41.2\\nLlama-3.1-8B \u1f511 12.8 39.4 0.7 0.0 0.0 13.7\\nPhi-3.5-4B \u1f511 6.4 19.4 0.7 0.0 0.0 6.0\\nTable 1: Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved\\ncorrectly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (see\\nSec. 2.3). The cell accuracy indicates the percentage of individual cells filled correctly. See Appx. A for more model results.\\noverall worse than GPT-4o-mini can hardly solve puzzles\\nbeyond the Small category \u2014 less than 5% accuracy in\\nMedium-size puzzles and almost no correct solutions in\\nLarge and X-Large puzzles. We can see that even the largest\\nopen-weight LLM, Llama-3.1-405B, only achieves 32.6%\\noverall accuracy. Although 405B has 22.5% accuracy in\\nMedium-size puzzles, it quickly also drops to 1.5% in the\\nLarge category and 0.0% in the X-Large category.\\nThe best non-reasoning LLM, Sonnet 3.5, has 36.2% ac-\\ncuracy in the overall evaluation, but it also drops to 4.0%\\nin the Large category and 1.0% in the X-Large category.\\nThis indicates that the logical reasoning tasks in ZebraLogic\\nare extremely challenging for LLMs, especially for puzzles\\nwith more complexity \u2013 with larger search spaces or harder\\nclues. We can also see that scaling up the model size does\\nnot necessarily improve the performance of LLMs in logical\\nreasoning tasks with large search spaces.\\n3.3. Scaling Behavior of LLMs in Logical Reasoning\\nIn the following sections, we study the scaling behavior\\nof LLMs in logical reasoning, as illustrated in Fig. 1. Our\\nanalysis focuses on two primary types of scaling: 1) scaling\\nmodel size and 2) scaling test-time compute. For test-time\\ncompute, we further explore three sub-dimensions: 1) the\\nnumber of candidate samples, 2) the number of reasoning\\ntokens (i.e., CoT tokens) generated during inference, and 3)the sample size for repeated sampling.\\n4. Scaling Model Size Can Hardly Break the\\nCurse of Complexity in Reasoning\\nThe Curse of Complexity in Reasoning for non-reasoning\\nLLMs. In addition to the search space size, we also use\\nZ3-conflict as the complexity measure to study the scaling\\nbehavior LLMs. Fig. 1 (left) highlights a key observation\\nregarding the performance of various Llama models with\\ndifferent model sizes across an increasing complexity in\\nterms of how many Z3 conflicts on average are encountered\\nwhen solving the ZebraLogic puzzles. A notable finding is\\nthat all model sizes experience a rapid decline in accuracy\\nas the complexity increases, illustrating the challenge posed\\nby complex reasoning tasks. This trend emphasizes the\\ninherent difficulty models face in maintaining high accuracy\\nbeyond a certain threshold of search complexity, irrespective\\nof their size. The phenomenon termed as the \u201ccurse of\\ncomplexity\u201d becomes evident as even the largest models,\\nsuch as the Llama-3.1-405B, cannot sustain high accuracy\\nonce the search space surpasses a certain scale. As shown\\nin Fig. 3, we see a similar trend in the search space size.\\nScaling model size is only effective for smaller search\\nspaces. However, it is important to note the significant bene-\\nfits of scaling model size when the search space is relatively\\nsmall (e.g., \u2264106). In these cases, larger models like the\\n6\\n\\n=== Page 7 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\n405B70B8B3BScaling Model Sizes The curse of complexity \\nScaling SamplesScalingCoTsO1-mini4o-mini(pass@k)\\nFigure 3: Accuracy vs Search Space Size (log scale) comparing multiple scaling behavior of LLMs on ZebraLogic. Left:\\nScaling model sizes. Right: Scaling test-time compute through two approaches - increasing sample size (via pass@k\\nevaluation) and extending chain-of-thought reasoning length. Both model size and test-time compute show diminishing\\nreturns as search space complexity grows beyond a certain complexity. More results are presented in Sec. 3.\\nLlama-3.1-405B and Llama-3.1-70B demonstrate substan-\\ntial improvements in accuracy compared to smaller models\\nsuch as the 3B and 8B versions. This suggests that scal-\\ning up the model size is an effective strategy for enhancing\\nperformance and tackling reasoning tasks in simpler search\\nspaces. Yet, as the complexity of the search space grows\\nbeyond 106, the advantages of larger model sizes diminish,\\nand scaling up the model size proves to be less impactful.\\nThis finding underscores the limited utility of model scaling\\nwhen dealing with highly complex reasoning tasks, as the\\naccuracy plateaus regardless of model size.\\nModel Size Scaling Limitations. This analysis reveals that\\nscaling up model sizes eventually reaches a point of dimin-\\nishing returns in complex search spaces. Beyond a certain\\ncomplexity threshold, increasing model parameters is insuf-\\nficient to prevent performance decline. This highlights a\\ncritical boundary for current scaling strategies, suggesting\\nthat new approaches are needed to overcome the limitations\\nimposed by high search space complexity and to advance\\nreasoning capabilities further.\\n5. Scaling Test-Time Compute with Repeated\\nSampling: Promises & Challenges\\nWe examine the impact of scaling test-time compute, a cru-\\ncial factor affecting LLM performance on logical reasoning\\ntasks. Specifically, here we investigate how increasing the\\nnumber of candidate samples influences model performance.\\nWe begin by employing Best-of-N (BoN) sampling, where\\nwe repeatedly sample N candidates from the model for each\\npuzzle. From these candidates, we can select the best an-\\nswer using various strategies, including majority voting and\\nexisting reward models. To understand the theoretical upper\\nbound of this approach, we also analyze BoN sampling with\\noracle selection, where we use knowledge of the correctanswer to choose the best candidate from the sample pool -\\nequivalent to the pass@k metric in our evaluation (see the\\nright-most plot in Fig. 1 and Fig.3).\\nGPT-4o with Best-of-N sampling and oracle selections\\ncan achieve nearly o1 performance. To understand the\\npotential improvement of scaling test-time compute for logi-\\ncal reasoning, we sample 128 candidates from GPT-4o-mini\\nand GPT-4o and study the coverage of the correct answer in\\nthe sampled candidates. In Table 2, we refer to this coverage\\nmetric as BoN-Oracle , meaning that the best-of-N (BoN)\\nselection is performed given the oracle knowledge of the\\ncorrect answer, i.e., the pass@k metric.\\nWe observe that the BoN-Oracle selection can significantly\\nimprove the performance of GPT-4o-mini and GPT-4o. For\\nexample, GPT-4o with BoN-Oracle N=128 achieves an over-\\nall accuracy of 69.1%, which is higher than O1-mini\u2019s ac-\\ncuracy of 59.7% and a potential scaling effect that can also\\noutperform O1-preview\u2019s accuracy of 71.4% if we keep en-\\nlarging the sampling size. Note that on the Medium-size\\nexamples, we can already see a higher accuracy of 92.9%\\nfor BoN-Oracle N=128 compared O1-preview\u2019s 88.2%, and\\nthe trend shown in the curves indicates that the performance\\nof GPT-4o can be further improved with more test-time\\ncompute. Fig. 6 in Appendix provides further analysis on\\nhow sampling affects model performance.\\nMajority Voting is simple yet effective. For majority vot-\\ning, we rank the candidates based on the frequency of each\\ncell in their solution grid, and select the candidate with\\nthe highest sum of frequencies. As for the Reward Model\\n(RM), we choose the one that ranks to the top on Ai2\u2019s\\nRewardBench leaderboard (Lambert et al., 2024b), named\\nSkywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024). We\\nfind that using Majority V oting for GPT-4o can improve\\nfrom 31.7 to 38.0 (for the overall accuracy) when the sam-\\n7\\n\\n=== Page 8 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\n# Hidden CoTTokenso1-minio1-full\\no1-preview\\nFigure 4: The o1 models\u2019 hidden CoT tokens vs. the number of Z3 conflicts. Each point is an example with a certain number\\nof Z3 conflicts. Larger number of Z3 conflicts are associated with harder reasoning problems.\\nModel &Methods Overall Small Medium Large X-Large\\n\u25ceGPT-4o \u2198 31.7 80.0 19.6 2.5 0.5\\nBoN-Oracle N=128\u2642lightbulb 69.1 99.7 92.9 49.0 7.0\\nBoN-Oracle N=32\u2642lightbulb 60.3 98.4 81.1 28.0 2.5\\nMajority-V oting N=128 37.6 84.7 32.1 7.5 0.0\\nMajority-V oting N=32 38.0 84.1 34.3 7.0 0.5\\nBoN-RM N=32 33.9 77.8 28.9 4.5 0.0\\nSelf-Verify (Oracle) \u2642lightbulb 34.8 83.8 24.6 5.0 0.5\\nSelf-Verify 33.0 82.2 22.1 2.5 0.0\\nSelf-Verify (x2) 32.1 80.0 21.4 2.5 0.0\\n\u25ceGPT-4o-mini \u2198 20.1 58.8 4.6 0.0 0.0\\nBoN-Oracle N=128\u2642lightbulb 51.2 99.7 61.8 10.0 0.0\\nBoN-Oracle N=32\u2642lightbulb 42.7 97.8 39.3 2.0 0.0\\nMajority-V oting N=128 25.0 69.4 8.9 1.5 0.0\\nMajority-V oting N=32 24.5 69.1 8.2 0.5 0.0\\nBoN-RM N=32 22.5 62.2 9.3 0.0 0.0\\nSelf-Verify (Oracle) \u2642lightbulb 22.3 65.0 5.4 0.0 0.0\\nSelf-Verify 21.1 60.9 5.7 0.0 0.0\\nTable 2: Comparison of various test-time compute scaling\\nmethods applied to GPT-4o and GPT-4o-mini. We evalu-\\nate several approaches: BoN-Oracle (selection using ora-\\ncle knowledge to verify correct answers among samples),\\nBoN-RM (selection using a reward model), Majority-V oting\\n(selecting the most common answer across samples), and\\nSelf-Verify (using multi-turn prompting for self-reflection\\nand correction, with and without oracle knowledge). We\\nuse\u2642lightbulbto denote the use of oracle knowledge.\\nple size N=32, while keep increasing the sample size does\\nnot necessarily improve the performance any more. Also,\\nthe performance of GPT-4o with BoN-RM N=32is 33.9,\\nwhich is worse than majority voting, suggesting that the\\ncurrent reward models that are mainly designed for chat\\nor general instruction following tasks may not be directly\\napplicable to (logical) reasoning tasks.\\n6. Scaling Test-Time Compute with Extensive\\nChain-of-Thoughts Tokens\\nAnother approach of scaling test-time compute is to increase\\nthe number of reasoning tokens (i.e., chain-of-thoughts to-\\nkens) that the model generates during inference.6.1. o1 Generates More Hidden Reasoning Tokens\\no1 generates large-scale hidden reasoning tokens. One of\\nthe key differences between o1 and other LLMs is the way\\nthey use more test-time compute to decode much more hid-\\nden chain-of-thoughts (CoT) tokens during inference time,\\nwhich are not directly visible to users. Our analysis shows\\nthat o1 models scale their hidden CoT tokens with puzzle\\ncomplexity - producing on average 5,144.6 (o1-mini) and\\n5,346.3 (o1-preview) hidden reasoning tokens compared to\\n502.9 and 543.7 for GPT-4o-mini and GPT-4o respectively.\\nThis order of magnitude difference in reasoning steps ap-\\npears to contribute to o1\u2019s superior performance on logical\\nreasoning tasks. For detailed analysis of how hidden CoT\\ntokens vary with puzzle complexity, see Appendix C.3.\\nFigure 4 reveals a positive correlation between the number\\nof hidden reasoning tokens generated by o1-preview and\\nZ3 conflicts, aligning with our earlier observation that o1\\nallocates more reasoning tokens to more complex puzzles.\\nFor puzzles with fewer than 20 Z3 conflicts, we observe\\na consistent ratio of approximately 400 hidden reasoning\\ntokens per conflict. However, this scaling pattern plateaus\\nwhen Z3 conflicts exceed 30, suggesting that o1-preview\\nmay have reached its maximum reasoning capacity at the\\ncurrent model size. This suggests that while o1-preview\\ncan effectively leverage more reasoning tokens for com-\\nplex puzzles, there is a limit to the extent to which it can\\nscale reasoning tokens to address highly complex reasoning\\ntasks. With the recent release of o1-full, we find that our\\nprevious estimation is consistent with the actual number\\nof hidden reasoning tokens generated by o1-full, which is\\naround 5,000 on average. This further confirms the scaling\\nbehavior of o1 models in generating more hidden reasoning\\ntokens for complex puzzles.\\nWe also find that when o1-preview make mistakes, they\\nusually generate more hidden reasoning tokens than when\\nthey solve the puzzles correctly, which is consistent with the\\nobservation that o1 tends to generate more reasoning tokens\\nfor more complex puzzles that are harder to solve.\\n8\\n\\n=== Page 9 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\n6.2. Self-Refinement is Limited but Promising\\nThe other feature of o1\u2019s hidden reasoning process is the\\nability to reflect on its own reasoning process and refine\\nits answer. From our observation on the summary of their\\nhidden reasoning process, we can see that o1 often revisits\\nthe clues and constraints to verify its previous reasoning\\nand fix the errors if there are any, which is similar to the\\nZ3 solver\u2019s conflict-driven clause learning mechanism. In\\norder to elicit such self-refinement behavior from LLMs,\\nwe add follow-up queries to ask the model to review its\\ninitial answer and check the clues and constraints again in a\\nmulti-turn conversation setting. There are two settings for\\nthe self-refinement process: one with the oracle knowledge\\nof the correct answer and the other without the oracle knowl-\\nedge. Results in Table 2 show modest improvements with\\nself-verification, particularly without oracle knowledge (4o\\nimproves from 31.7 to 33.0, then decreases to 32.1).\\nSelf-Verification Prompt\\nSelf-Verify: Your answer may be incorrect! Identify\\nany mistakes in your reasoning and answer, if any.\\nCorrect them to ensure they align with the given\\ninformation. Present your updated response in the\\nsame JSON format mentioned in the initial prompt.\\nSelf-Verify (Oracle \u2642lightbulb):\\n\u2022For incorrect results: Your answer is incorrect!\\nRe-examine the clues, correct the mistakes, and\\nthen provide the revised solution in the original\\nJSON format.\\n\u2022For correct results: Your answer is correct.\\nPlease repeat the json-formatted output again.\\n7. Related Work\\nLogical Reasoning Benchmarks and Dataset Creation\\nLogical reasoning has long been a critical area of AI, but\\nonly recently have LLMs been subjected to rigorous testing\\nin this domain. LogiQA (Liu et al., 2020) emerged early\\non to evaluate complex logical comprehension in question-\\nanswering formats; and subsequent efforts by (Liu et al.,\\n2023) reframed it as a Natural Language Inference (NLI)\\ntask to further stress-test LLMs\u2019 capabilities. Researchers\\nhave also explored generating more dynamic or granular\\ndatasets to push the limits of reasoning systems. For in-\\nstance, Madusanka et al. (2024) investigated satisfiability\\ntasks formulated in natural language, studying how vary-\\ning computational complexity influences LLM inference\\nperformance. Similarly, Richardson & Sabharwal (2022) in-\\ntroduced a systematic methodology for building challenging\\nreasoning datasets, exposing robustness gaps in transformer-\\nbased models when tasked with increased complexity. Prior\\nwork on logic grid puzzles include Mitra & Baral (2015)\\nthat proposed a grid-based puzzle dataset prior to the LLM\\nera and focused on automatic translation from language toa formal specification, Dziri et al. (2023) that investigated\\ncompositionality in LLMs on grid-based puzzles, as well as\\nTyagi et al. (2024) that provided a new error taxonomy to\\nevaluate the correctness of the reasoning chains of LLMs.\\nApproaches to Logical Reasoning in LLMs. Several\\nlines of research propose methods to augment or refine\\nLLMs for stronger logical reasoning. Clark et al. (2020)\\ndemonstrated that transformers can emulate logical reason-\\ning over natural language sentences\u2014serving as \u201csoft the-\\norem provers.\u201d Pan et al. (2024) showed that a decoder-\\nonly Transformer could tackle SAT problems, paralleling\\nthe Davis\u2013Putnam\u2013Logemann\u2013Loveland (DPLL) algorithm,\\nthereby expanding the role of LLMs to more complex\\nproblem-solving domains. Alternatively, neuro-symbolic\\nsystems like CLOVER (Ryu et al., 2024) integrate LLMs\\nwith symbolic solvers to better capture the translation of\\nintricate logical semantics from text.\\nEmpirical Evidence of LLM Limitations . Despite these\\npromising developments, LLMs face persistent hurdles as\\nlogical problem complexity increases. Yan et al. (2024)\\ncontended that models may rely heavily on probabilistic\\ncorrelations rather than genuinely understanding logical\\nrules. Similarly, Xie et al. (2024) highlighted the complex\\ninterplay between training data memorization and genuine\\nreasoning abilities of LLMs. Additionally, Schlegel et al.\\n(2022) conducted an extensive empirical study to investi-\\ngate the detection of formally valid inferences in controlled\\nfragments of natural language, revealing that transformers\\noften overfit to superficial patterns rather than acquiring\\nlogical principles. Lam et al. (2024) showed the impact of\\nthe choice of symbolic solvers on the effectiveness of LLMs\\nin deductive reasoning tasks, calling for more consistent\\ncomparative studies. Further empirical evidence from Dziri\\net al. (2023) and Parmar et al. (2024) demonstrated that\\neven ostensibly simple logical tasks continue to challenge\\nthese models. Finally, Madusanka et al. (2023) investigated\\nthe limits of transformers on solving the problem of model-\\nchecking with natural language and the significant impact of\\nthe language fragment on the performance of transformers.\\n8. Conclusion\\nWe introduce ZebraLogic , a controlled benchmark of logic\\ngrid puzzles that highlights the scaling limits of LLM-based\\nreasoning through carefully adjustable complexity. Our ex-\\nperiments reveal a pronounced drop in performance as com-\\nplexity increases, overshadowing gains from model growth\\nor training data expansions. While increasing the generation\\nsample size yields modest improvements, a backtracking-\\nbased approach with expanded reasoning steps significantly\\nboosts accuracy. These results spotlight the importance of\\nnon-monotonic reasoning and provide a valuable framework\\nfor advancing logical reasoning research.\\n9\\n\\n=== Page 10 ===\\nZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\\nAcknowledgments\\nYejin Choi\u2019s research is supported in part by the National\\nScience Foundation under Grant DMS-2134012.\\nReferences\\nAI@Meta. The llama 3 herd of models. 2024. URL https:\\n//arxiv.org/abs/2407.21783 .\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\\nS., Radford, A., Sutskever, I., and Amodei, D. Language\\nmodels are few-shot learners, 2020.\\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J.,\\nHorvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y ., Lund-\\nberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang,\\nY . Sparks of artificial general intelligence: Early ex-\\nperiments with gpt-4. arXiv preprint arXiv:2303.12712 ,\\n2023.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,\\nN. M., Prabhakaran, V ., Reif, E., Du, N., Hutchinson,\\nB., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,\\nG., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,\\nS., Michalewski, H., Garc \u00b4\u0131a, X., Misra, V ., Robinson,\\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,\\nH., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,\\nAgrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pel-\\nlat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov,\\nO., Lee, K., Zhou, Z., Wang, X., Saeta, B., D \u00b4\u0131az, M., Fi-\\nrat, O., Catasta, M., Wei, J., Meier-Hellstern, K. S., Eck,\\nD., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling lan-\\nguage modeling with pathways. ArXiv , abs/2204.02311,\\n2022. URL https://api.semanticscholar.\\norg/CorpusID:247951931 .\\nClark, P., Tafjord, O., and Richardson, K. Trans-\\nformers as soft reasoners over language. In Inter-\\nnational Joint Conference on Artificial Intelligence ,\\n2020. URL https://api.semanticscholar.\\norg/CorpusID:211126663 .\\nColbourn, C. J. The complexity of completing par-\\ntial latin squares. Discret. Appl. Math. , 8:25\u201330,\\n1984. URL https://api.semanticscholar.\\norg/CorpusID:33813226 .de Moura, L. M. and Bj\u00f8rner, N. S. Z3: An efficient\\nsmt solver. In International Conference on Tools and\\nAlgorithms for Construction and Analysis of Systems ,\\n2008. URL https://api.semanticscholar.\\norg/CorpusID:15912959 .\\nDechter, R. Constraint Processing . Morgan Kaufmann,\\n2003.\\nDeepSeek-AI. DeepSeek-R1: Incentivizing reasoning ca-\\npability in LLMs via reinforcement learning. arXiv ,\\nabs/2501.12948, 2025.\\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin,\\nB. Y ., West, P., Bhagavatula, C., Le Bras, R., Hwang,\\nJ. D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Har-\\nchaoui, Z., and Choi, Y . Faith and fate: Limits of trans-\\nformers on compositionality. ArXiv , abs/2305.18654,\\n2023. URL https://api.semanticscholar.\\norg/CorpusID:258967391 .\\nGomes, C. P. and Shmoys, D. B. Completing quasigroups\\nor latin squares: A structured graph coloring problem.\\n2002. URL https://api.semanticscholar.\\norg/CorpusID:10410543 .\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\\nematical problem solving with the math dataset. NeurIPS ,\\n2021.\\nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T.,\\nWang, S., Solar-Lezama, A., Sen, K., and Stoica, I.\\nLivecodebench: Holistic and contamination free eval-\\nuation of large language models for code. arXiv preprint\\narXiv:2403.07974 , 2024.\\nLam, L. H. M., Thatikonda, R. K., and Shareghi,\\nE. A closer look at logical reasoning with llms:\\nThe choice of tool matters. 2024. URL https:\\n//api.semanticscholar.org/CorpusID:\\n270219176 .\\nLambert, N., Morrison, J. D., Pyatkin, V ., Huang, S.,\\nIvison, H., Brahman, F., Miranda, L. J. V ., Liu, A.,\\nDziri, N., Lyu, X., Gu, Y ., Malik, S., Graf, V ., Hwang,\\nJ. D., Yang, J., Le Bras, R., Tafjord, O., Wilhelm,\\nC., Soldaini, L., Smith, N. A., Wang, Y ., Dasigi, P.,\\nand Hajishirzi, H. T \u00a8ulu 3: Pushing frontiers in open\\nlanguage model post-training. ArXiv , abs/2411.15124,\\n2024a. URL https://api.semanticscholar.\\norg/CorpusID:274192505 .\\nLambert, N., Pyatkin, V ., Morrison, J. D., Miranda,\\nL. J. V ., Lin, B. Y ., Chandu, K. R., Dziri, N., Ku-\\nmar, S., Zick, T., Choi, Y ., Smith, N. A., and Ha-\\njishirzi, H. Rewardbench: Evaluating reward mod-\\nels for language modeling. ArXiv , abs/2403.13787,\\n10\",\"annotations\":null}", "tool_call_id": "toolu_015giWSdzybMDBoSLWfg5WRB"}, {"role": "assistant", "content": "Based on my research, I now have enough information to create the table. Let me compile the information for each dataset:\n\n1. **BBH (Big-Bench Hard)**: 23 tasks, no training set (evaluation only), no adjustable difficulty\n2. **Zebra Logic**: 1 task type (logic grid puzzles), no training set, has adjustable difficulty (multiple complexity levels)\n3. **KOR-Bench**: 5 task categories (Operation, Logic, Cipher, Puzzle, Counterfactual), no training set mentioned, no adjustable difficulty\n4. **K&K (Knights and Knaves)**: Based on the paper, it's dynamically generated logical reasoning puzzles, has training set (for fine-tuning experiments), has adjustable difficulty\n5. **BBEH (BIG-Bench Extra Hard)**: 23 tasks (replaces each BBH task), no training set, no adjustable difficulty\n\nNow let me create the LaTeX table:", "tool_calls": [{"id": "toolu_01EKszrrCvfbjkbc5t8vmrgo", "type": "function", "function": {"name": "filesystem-write_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/datasets.tex\", \"content\": \"\\\\begin{table}[!ht]\\n    \\\\begin{center}\\n    \\\\begin{tabular}{lccc}\\n        \\\\toprule\\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\\\\\\n        \\\\midrule\\n        BBH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        Zebra Logic & 1 & \\\\ding{55} & \\\\ding{51}\\\\\\\\\\n        KOR-Bench & 5 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        K\\\\&K & 1 & \\\\ding{51} & \\\\ding{51}\\\\\\\\\\n        BBEH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n  \\\\end{center}\\n\\\\end{table}\\n\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/datasets.tex\",\"annotations\":null}", "tool_call_id": "toolu_01EKszrrCvfbjkbc5t8vmrgo"}, {"role": "assistant", "content": "Perfect! I've created the LaTeX table and saved it to `/workspace/dumps/workspace/datasets.tex`. Here's a summary of what I found for each dataset:\n\n1. **BBH (Big-Bench Hard)**: 23 challenging tasks selected from BIG-Bench, used only for evaluation (no training set), with fixed difficulty levels.\n\n2. **Zebra Logic**: A single task type (logic grid puzzles/constraint satisfaction problems), evaluation-only dataset with 1,000 puzzles spanning multiple controllable complexity levels based on grid size and logical constraints.\n\n3. **KOR-Bench**: 5 broader task categories (Operation, Logic, Cipher, Puzzle, and Counterfactual reasoning), with 25 rules per category and 10 problems per rule (total 1,250 problems), designed for evaluation without a training set, and without adjustable difficulty levels.\n\n4. **K&K (Knights and Knaves)**: A dynamically generated logical reasoning benchmark based on Knights and Knaves puzzles. It includes a training set (used for fine-tuning experiments in the paper) and supports adjustable difficulty levels through controlled generation.\n\n5. **BBEH (BIG-Bench Extra Hard)**: 23 tasks (each replacing a corresponding BBH task with increased difficulty), designed as an evaluation benchmark with 200 questions per task, no training set, and fixed difficulty.\n\nThe table has been saved with only the table content, no comments, and uses `\\ding{51}` for checkmarks and `\\ding{55}` for cross marks as requested."}]}